{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de09d0f6",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [27]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb384650",
   "metadata": {
    "papermill": {
     "duration": 0.002706,
     "end_time": "2024-01-29T23:14:25.976213",
     "exception": false,
     "start_time": "2024-01-29T23:14:25.973507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is taken directly from https://github.com/tcapelle/llm_recipes/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
   "metadata": {
    "papermill": {
     "duration": 0.002534,
     "end_time": "2024-01-29T23:14:25.980986",
     "exception": false,
     "start_time": "2024-01-29T23:14:25.978452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# From Llama to Alpaca: Finetunning and LLM with Weights & Biases\n",
    "In this notebooks you will learn how to finetune a pretrained LLama model on an Instruction dataset. We will use an updated version of the Alpaca dataset that, instead of davinci-003 (GPT3) generations uses GPT4 to get an even better instruction dataset! More details on the [official repo page](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data)\n",
    "\n",
    "> This notebook requires a A100/A10 GPU with at least 24GB of memory. You could tweak the params down and run on a T4 but it would take very long time\n",
    "\n",
    "This notebooks has a companion project and [report](wandb.me/alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7744abe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:25.985986Z",
     "iopub.status.busy": "2024-01-29T23:14:25.985807Z",
     "iopub.status.idle": "2024-01-29T23:14:25.987973Z",
     "shell.execute_reply": "2024-01-29T23:14:25.987741Z"
    },
    "papermill": {
     "duration": 0.005577,
     "end_time": "2024-01-29T23:14:25.988568",
     "exception": false,
     "start_time": "2024-01-29T23:14:25.982991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install wandb transformers trl datasets \"protobuf==3.20.3\" evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693a36b-5cdc-4582-844b-8dd52fa522c5",
   "metadata": {
    "papermill": {
     "duration": 0.001989,
     "end_time": "2024-01-29T23:14:25.992577",
     "exception": false,
     "start_time": "2024-01-29T23:14:25.990588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## With Huggingface TRL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04c0a5-f481-4364-880d-10c254388987",
   "metadata": {
    "papermill": {
     "duration": 0.001957,
     "end_time": "2024-01-29T23:14:25.996511",
     "exception": false,
     "start_time": "2024-01-29T23:14:25.994554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's grab the Alpaca (GPT-4 curated instructions and outputs) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:26.001078Z",
     "iopub.status.busy": "2024-01-29T23:14:26.000950Z",
     "iopub.status.idle": "2024-01-29T23:14:26.072631Z",
     "shell.execute_reply": "2024-01-29T23:14:26.072336Z"
    },
    "papermill": {
     "duration": 0.074717,
     "end_time": "2024-01-29T23:14:26.073214",
     "exception": false,
     "start_time": "2024-01-29T23:14:25.998497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Benchmark from MedQA-USMLE/US/train.jsonl\n",
      "Benchmark contains 10178 questions, made up of 10178 with 5 options and 0 with non-5 options\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json\n",
    "# from uparse_benchmark import parse_benchmark\n",
    "# from ..utilities.parse_benchmark import parse_benchmark\n",
    "from utilities.parse_benchmark import parse_benchmark\n",
    "\n",
    "benchmark = \"MedQA\"\n",
    "benchmark_questions, benchmark_answers = parse_benchmark(benchmark)\n",
    "# print(benchmark_questions[0])\n",
    "# print(benchmark_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a154f968-da0f-4bdc-bf45-e499d95f0606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:26.077727Z",
     "iopub.status.busy": "2024-01-29T23:14:26.077626Z",
     "iopub.status.idle": "2024-01-29T23:14:30.908518Z",
     "shell.execute_reply": "2024-01-29T23:14:30.908251Z"
    },
    "papermill": {
     "duration": 4.833822,
     "end_time": "2024-01-29T23:14:30.909047",
     "exception": false,
     "start_time": "2024-01-29T23:14:26.075225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnelectric\u001b[0m (\u001b[33mneelectric\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/service/BioLlama/wandb/run-20240129_231427-sdqvj2a0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33munique-microwave-63\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft/runs/sdqvj2a0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/neelectric/biollama_ft/runs/sdqvj2a0?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5b5f757790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"biollama_ft\", # the project I am working on\n",
    "           tags=[\"hf_sft\"]) # the Hyperparameters I want to keep track of\n",
    "# artifact = wandb.use_artifact('Neelectric/MedQA-USMLE', type='dataset')\n",
    "# artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811291cc-b7ce-422a-8971-3cbf9fe10a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:30.920694Z",
     "iopub.status.busy": "2024-01-29T23:14:30.920569Z",
     "iopub.status.idle": "2024-01-29T23:14:31.722217Z",
     "shell.execute_reply": "2024-01-29T23:14:31.721937Z"
    },
    "papermill": {
     "duration": 0.811524,
     "end_time": "2024-01-29T23:14:31.723170",
     "exception": false,
     "start_time": "2024-01-29T23:14:30.911646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# print(artifact_dir)\n",
    "artifact_dir = os.getcwd() + \"/benchmarks/MedQA-USMLE/\"\n",
    "from datasets import load_dataset\n",
    "#dataset = load_dataset(\"Neelectric/MedQA-USMLE\")\n",
    "medqa = load_dataset(\"json\", data_dir=artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24cf6197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:31.736852Z",
     "iopub.status.busy": "2024-01-29T23:14:31.736320Z",
     "iopub.status.idle": "2024-01-29T23:14:36.460933Z",
     "shell.execute_reply": "2024-01-29T23:14:36.460638Z"
    },
    "papermill": {
     "duration": 4.734435,
     "end_time": "2024-01-29T23:14:36.461813",
     "exception": false,
     "start_time": "2024-01-29T23:14:31.727378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trying gsutil for SciFive pretraining corpus\n",
    "# !pip install gsutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "abs_1_16 = pd.read_csv(\"abs_1_16.tsv\", sep='\\t')\n",
    "abs_1_30 = pd.read_csv(\"abs_1_30.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f4c49e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:36.473869Z",
     "iopub.status.busy": "2024-01-29T23:14:36.473744Z",
     "iopub.status.idle": "2024-01-29T23:14:36.475777Z",
     "shell.execute_reply": "2024-01-29T23:14:36.475603Z"
    },
    "papermill": {
     "duration": 0.005722,
     "end_time": "2024-01-29T23:14:36.476301",
     "exception": false,
     "start_time": "2024-01-29T23:14:36.470579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# abs_1_16 = abs_1_16.dropna()\n",
    "count_nans = abs_1_16.iloc[:, 0].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a9e0ed7-375f-431a-9420-0022cf5566f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:36.481621Z",
     "iopub.status.busy": "2024-01-29T23:14:36.481538Z",
     "iopub.status.idle": "2024-01-29T23:14:36.483444Z",
     "shell.execute_reply": "2024-01-29T23:14:36.483281Z"
    },
    "papermill": {
     "duration": 0.0053,
     "end_time": "2024-01-29T23:14:36.483976",
     "exception": false,
     "start_time": "2024-01-29T23:14:36.478676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 10178\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 1272\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 1273\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb96e0-b2f2-4a79-ba65-e1c8d6395d54",
   "metadata": {
    "papermill": {
     "duration": 0.002356,
     "end_time": "2024-01-29T23:14:36.488768",
     "exception": false,
     "start_time": "2024-01-29T23:14:36.486412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's log the dataset also as a table so we can inspect it on the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7dda87-d70a-470b-a0f9-040af434dc42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:36.494113Z",
     "iopub.status.busy": "2024-01-29T23:14:36.493990Z",
     "iopub.status.idle": "2024-01-29T23:14:36.496311Z",
     "shell.execute_reply": "2024-01-29T23:14:36.496142Z"
    },
    "papermill": {
     "duration": 0.00561,
     "end_time": "2024-01-29T23:14:36.496820",
     "exception": false,
     "start_time": "2024-01-29T23:14:36.491210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10178\n",
      "1273\n"
     ]
    }
   ],
   "source": [
    "train_dataset = medqa[\"train\"]\n",
    "eval_dataset = medqa[\"test\"]\n",
    "#print sizes\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "# turn both of these into only half their size\n",
    "# train_dataset = train_dataset.select(range(0, len(train_dataset)//2))\n",
    "# eval_dataset = eval_dataset.select(range(0, len(eval_dataset)//2))\n",
    "\n",
    "# print(len(train_dataset))\n",
    "# print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a038eb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:36.502212Z",
     "iopub.status.busy": "2024-01-29T23:14:36.502095Z",
     "iopub.status.idle": "2024-01-29T23:14:36.504563Z",
     "shell.execute_reply": "2024-01-29T23:14:36.504401Z"
    },
    "papermill": {
     "duration": 0.005827,
     "end_time": "2024-01-29T23:14:36.505075",
     "exception": false,
     "start_time": "2024-01-29T23:14:36.499248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an excellently helpful AI assistant that answers biomedical questions. <QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \\n (A) Ampicillin\\n (B) Ceftriaxone\\n (C) Ciprofloxacin\\n (D) Doxycycline\\n (E) Nitrofurantoin</QUESTION>\\n<ANSWER> (E) Nitrofurantoin</ANSWER>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_prompt(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"You are an excellently helpful AI assistant that answers biomedical questions. \"\n",
    "            \"<QUESTION>{question} {option_string}</QUESTION>\\n<ANSWER> ({answer_idx}) {answer}</ANSWER>\").format_map(row)\n",
    "create_prompt(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f1a9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:36.510535Z",
     "iopub.status.busy": "2024-01-29T23:14:36.510403Z",
     "iopub.status.idle": "2024-01-29T23:14:36.515814Z",
     "shell.execute_reply": "2024-01-29T23:14:36.515650Z"
    },
    "papermill": {
     "duration": 0.008893,
     "end_time": "2024-01-29T23:14:36.516466",
     "exception": false,
     "start_time": "2024-01-29T23:14:36.507573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an excellently helpful AI assistant that answers biomedical questions. <QUESTION>A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take? \n",
      " (A) Disclose the error to the patient but leave it out of the operative report\n",
      " (B) Disclose the error to the patient and put it in the operative report\n",
      " (C) Tell the attending that he cannot fail to disclose this mistake\n",
      " (D) Report the physician to the ethics committee\n",
      " (E) Refuse to dictate the operative report</QUESTION>\n",
      "<ANSWER> \n",
      "You are an excellently helpful AI assistant that answers biomedical questions. <QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \n",
      " (A) Ampicillin\n",
      " (B) Ceftriaxone\n",
      " (C) Ciprofloxacin\n",
      " (D) Doxycycline\n",
      " (E) Nitrofurantoin</QUESTION>\n",
      "<ANSWER> (E) Nitrofurantoin</ANSWER>\n"
     ]
    }
   ],
   "source": [
    "def create_prompt_no_answer(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"You are an excellently helpful AI assistant that answers biomedical questions. \"\n",
    "            \"<QUESTION>{question} {option_string}</QUESTION>\\n<ANSWER> \").format_map(row)\n",
    "\n",
    "def return_prompt_no_answer(row):\n",
    "    return {\"text\": create_prompt_no_answer(row)}\n",
    "\n",
    "def return_prompt(row):\n",
    "    return {\"text\": create_prompt(row)}\n",
    "    \n",
    "test_dataset = eval_dataset.map(return_prompt_no_answer)\n",
    "print(test_dataset[0][\"text\"])\n",
    "train_dataset_with_texts = train_dataset.map(return_prompt)\n",
    "print(train_dataset_with_texts[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "720c707b-3bce-4164-b8c1-3c3122200c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:36.522212Z",
     "iopub.status.busy": "2024-01-29T23:14:36.521977Z",
     "iopub.status.idle": "2024-01-29T23:14:37.305168Z",
     "shell.execute_reply": "2024-01-29T23:14:37.304816Z"
    },
    "papermill": {
     "duration": 0.787166,
     "end_time": "2024-01-29T23:14:37.306213",
     "exception": false,
     "start_time": "2024-01-29T23:14:36.519047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# from cti.transformers.transformers.src.transformers.models.auto import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4162aec8-f2ba-45db-9633-817b416d4e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:37.318214Z",
     "iopub.status.busy": "2024-01-29T23:14:37.318038Z",
     "iopub.status.idle": "2024-01-29T23:14:37.319709Z",
     "shell.execute_reply": "2024-01-29T23:14:37.319480Z"
    },
    "papermill": {
     "duration": 0.011121,
     "end_time": "2024-01-29T23:14:37.320265",
     "exception": false,
     "start_time": "2024-01-29T23:14:37.309144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "# model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51c10f7f-2551-4aa4-aef2-29c888b57a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:37.325957Z",
     "iopub.status.busy": "2024-01-29T23:14:37.325698Z",
     "iopub.status.idle": "2024-01-29T23:14:37.327308Z",
     "shell.execute_reply": "2024-01-29T23:14:37.327092Z"
    },
    "papermill": {
     "duration": 0.00503,
     "end_time": "2024-01-29T23:14:37.327830",
     "exception": false,
     "start_time": "2024-01-29T23:14:37.322800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     # quantization_config=bnb_config,\n",
    "#     # load_in_4bit = True, # this causes \"RuntimeError: only Tensors of floating point and complex dtype can require gradients\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79dbbb0-6dac-4f78-a862-8201088c9d57",
   "metadata": {
    "papermill": {
     "duration": 0.002507,
     "end_time": "2024-01-29T23:14:37.332930",
     "exception": false,
     "start_time": "2024-01-29T23:14:37.330423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Training the full models is expensive, but if you have a GPU that can fit the full model, you can skip this part. Let's just train the last 8 layers of the model (Llama2-7B has 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac5b550",
   "metadata": {
    "papermill": {
     "duration": 0.002535,
     "end_time": "2024-01-29T23:14:37.338001",
     "exception": false,
     "start_time": "2024-01-29T23:14:37.335466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37d2f1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:37.343561Z",
     "iopub.status.busy": "2024-01-29T23:14:37.343454Z",
     "iopub.status.idle": "2024-01-29T23:14:46.893646Z",
     "shell.execute_reply": "2024-01-29T23:14:46.893313Z"
    },
    "papermill": {
     "duration": 9.554139,
     "end_time": "2024-01-29T23:14:46.894669",
     "exception": false,
     "start_time": "2024-01-29T23:14:37.340530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                       | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                       | 1/2 [00:03<00:03,  3.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  1.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping layer 15 with retro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from utilities.biollama import BioLlama\n",
    "\n",
    "# questions = [\"Which is the main calcium pump of the sarcoplasmic reticulum? Answer:\"]\n",
    "amended_questions = [\"The main calcium pump of the sarcoplasmic reticulum is \"]\n",
    "questions = amended_questions\n",
    "# answers = [\"Sarcoplasmic reticulum Ca(2+)-ATPase\"] # or \"SERCA\",\"serca2\"\n",
    "\n",
    "prompt = questions[0]\n",
    "# model_id = \"TheBloke/Llama-2-7b-chat-GPTQ\"\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "chunk_length = 32\n",
    "\n",
    "RETRO_layer_ids = [15]\n",
    "\n",
    "BioLlama = BioLlama(\n",
    "    model_id=model_id,\n",
    "    chunk_length=chunk_length,\n",
    "    RETRO_layer_ids=RETRO_layer_ids,\n",
    "    training=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76e11ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:46.907125Z",
     "iopub.status.busy": "2024-01-29T23:14:46.906866Z",
     "iopub.status.idle": "2024-01-29T23:14:46.908831Z",
     "shell.execute_reply": "2024-01-29T23:14:46.908622Z"
    },
    "papermill": {
     "duration": 0.01178,
     "end_time": "2024-01-29T23:14:46.909652",
     "exception": false,
     "start_time": "2024-01-29T23:14:46.897872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(BioLlama.model)\n",
    "model = BioLlama.model\n",
    "tokenizer = BioLlama.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:46.919173Z",
     "iopub.status.busy": "2024-01-29T23:14:46.919069Z",
     "iopub.status.idle": "2024-01-29T23:14:46.924794Z",
     "shell.execute_reply": "2024-01-29T23:14:46.924607Z"
    },
    "papermill": {
     "duration": 0.011472,
     "end_time": "2024-01-29T23:14:46.925613",
     "exception": false,
     "start_time": "2024-01-29T23:14:46.914141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freezing layers, currently only works for single unfrozen retro layer\n",
      "printing layer 14 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "\n",
      "printing layer 15 params\n",
      "layer.self_attn.q_proj.weight\n",
      "layer.self_attn.k_proj.weight\n",
      "layer.self_attn.v_proj.weight\n",
      "layer.self_attn.o_proj.weight\n",
      "layer.mlp.gate_proj.weight\n",
      "layer.mlp.up_proj.weight\n",
      "layer.mlp.down_proj.weight\n",
      "layer.input_layernorm.weight\n",
      "layer.post_attention_layernorm.weight\n",
      "layer.CCA_attn.q_proj.weight\n",
      "layer.CCA_attn.k_proj.weight\n",
      "layer.CCA_attn.v_proj.weight\n",
      "layer.CCA_attn.o_proj.weight\n",
      "CCA.pre_CCA_layernorm.weight\n"
     ]
    }
   ],
   "source": [
    "print(\"freezing layers, currently only works for single unfrozen retro layer\")\n",
    "n_freeze = BioLlama.RETRO_layer_ids[0]\n",
    "# n_freeze = 15\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): \n",
    "    param.requires_grad = True\n",
    "#for every parameter in retro_layer_params, print where in the model it comes from (ie is it from self attention, layer norm, etc)\n",
    "print(\"printing layer 14 params\")\n",
    "for name, param in model.model.layers[14].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\") \n",
    "print(\"\\nprinting layer 15 params\")\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}\")    \n",
    "\n",
    "for param in model.model.layers[n_freeze].parameters(): \n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:46.935180Z",
     "iopub.status.busy": "2024-01-29T23:14:46.935075Z",
     "iopub.status.idle": "2024-01-29T23:14:46.936683Z",
     "shell.execute_reply": "2024-01-29T23:14:46.936495Z"
    },
    "papermill": {
     "duration": 0.007335,
     "end_time": "2024-01-29T23:14:46.937505",
     "exception": false,
     "start_time": "2024-01-29T23:14:46.930170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "model.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:46.947165Z",
     "iopub.status.busy": "2024-01-29T23:14:46.947058Z",
     "iopub.status.idle": "2024-01-29T23:14:46.950315Z",
     "shell.execute_reply": "2024-01-29T23:14:46.950140Z"
    },
    "papermill": {
     "duration": 0.008864,
     "end_time": "2024-01-29T23:14:46.951004",
     "exception": false,
     "start_time": "2024-01-29T23:14:46.942140",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6805.53M, Trainable: 400.57M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ff31b0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:46.960688Z",
     "iopub.status.busy": "2024-01-29T23:14:46.960594Z",
     "iopub.status.idle": "2024-01-29T23:14:46.962131Z",
     "shell.execute_reply": "2024-01-29T23:14:46.961954Z"
    },
    "papermill": {
     "duration": 0.007306,
     "end_time": "2024-01-29T23:14:46.963064",
     "exception": false,
     "start_time": "2024-01-29T23:14:46.955758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_tokens, text = BioLlama.generate(prompt=prompt, max_new_tokens=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac3525d7-3028-499e-8749-c6fbc21a26d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:46.973436Z",
     "iopub.status.busy": "2024-01-29T23:14:46.973341Z",
     "iopub.status.idle": "2024-01-29T23:14:47.001389Z",
     "shell.execute_reply": "2024-01-29T23:14:47.001157Z"
    },
    "papermill": {
     "duration": 0.033876,
     "end_time": "2024-01-29T23:14:47.002325",
     "exception": false,
     "start_time": "2024-01-29T23:14:46.968449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c57893e-6e42-4cda-83c8-1e51a86d2da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:47.012299Z",
     "iopub.status.busy": "2024-01-29T23:14:47.012204Z",
     "iopub.status.idle": "2024-01-29T23:14:47.014341Z",
     "shell.execute_reply": "2024-01-29T23:14:47.014165Z"
    },
    "papermill": {
     "duration": 0.007864,
     "end_time": "2024-01-29T23:14:47.015042",
     "exception": false,
     "start_time": "2024-01-29T23:14:47.007178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5605\n",
      "changing total num size to 2500\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "total_num_steps = 11_210 // batch_size\n",
    "print(total_num_steps)\n",
    "\n",
    "\n",
    "total_num_steps = 2500\n",
    "print(f\"changing total num size to {total_num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c86339ca-27e5-496b-9559-3a65bb3c26a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:47.024850Z",
     "iopub.status.busy": "2024-01-29T23:14:47.024758Z",
     "iopub.status.idle": "2024-01-29T23:14:47.027451Z",
     "shell.execute_reply": "2024-01-29T23:14:47.027284Z"
    },
    "papermill": {
     "duration": 0.008424,
     "end_time": "2024-01-29T23:14:47.028258",
     "exception": false,
     "start_time": "2024-01-29T23:14:47.019834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size//2,\n",
    "    bf16=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=total_num_steps // 10,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=total_num_steps,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=total_num_steps // 3,\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\", #changed to epoch so we save every epoch i guess?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e35a9b70-f36b-4bfd-857f-c80d3450e111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:47.038237Z",
     "iopub.status.busy": "2024-01-29T23:14:47.038052Z",
     "iopub.status.idle": "2024-01-29T23:14:47.086380Z",
     "shell.execute_reply": "2024-01-29T23:14:47.086133Z"
    },
    "papermill": {
     "duration": 0.054067,
     "end_time": "2024-01-29T23:14:47.087185",
     "exception": false,
     "start_time": "2024-01-29T23:14:47.033118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from utils import LLMSampleCB, token_accuracy\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset_with_texts,\n",
    "    dataset_text_field=\"text\",\n",
    "    eval_dataset=eval_dataset,\n",
    "    packing=True,\n",
    "    max_seq_length=1024,\n",
    "    args=training_args,\n",
    "    formatting_func=create_prompt,\n",
    "    # compute_metrics=token_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5754789-2e15-4bc9-800c-01f8ffc625e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:14:47.097130Z",
     "iopub.status.busy": "2024-01-29T23:14:47.097028Z",
     "iopub.status.idle": "2024-01-29T23:55:46.868731Z",
     "shell.execute_reply": "2024-01-29T23:55:46.868308Z"
    },
    "papermill": {
     "duration": 2459.777898,
     "end_time": "2024-01-29T23:55:46.869949",
     "exception": false,
     "start_time": "2024-01-29T23:14:47.092051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 40:47, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>833</td>\n",
       "      <td>1.209300</td>\n",
       "      <td>1.235515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1666</td>\n",
       "      <td>0.867400</td>\n",
       "      <td>1.119950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2499</td>\n",
       "      <td>0.929200</td>\n",
       "      <td>1.082668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/utils.py:268: UserWarning: The dataset reached end and the iterator is reset to the start.\n",
      "  warnings.warn(\"The dataset reached end and the iterator is reset to the start.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'model.layers.15.CCA.layer.mlp.down_proj.weight', 'model.layers.15.CCA.layer.input_layernorm.weight', 'model.layers.15.CCA.layer.CCA_attn.k_proj.weight', 'model.layers.15.CCA.layer.CCA_attn.o_proj.weight', 'model.layers.15.pre_CCA_layernorm.weight', 'model.layers.15.CCA.layer.self_attn.k_proj.weight', 'model.layers.15.CCA.layer.CCA_attn.v_proj.weight', 'model.layers.15.CCA.layer.mlp.gate_proj.weight', 'model.layers.15.CCA.layer.post_attention_layernorm.weight', 'model.layers.15.CCA.layer.CCA_attn.q_proj.weight', 'model.layers.15.CCA.layer.self_attn.o_proj.weight', 'model.layers.15.CCA.layer.self_attn.q_proj.weight', 'model.layers.15.CCA.layer.mlp.up_proj.weight', 'model.layers.15.CCA.layer.self_attn.v_proj.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.004 MB of 0.004 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.004 MB of 0.016 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.025 MB of 0.036 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.025 MB of 0.036 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.025 MB of 0.036 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.025 MB of 0.036 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.025 MB of 0.036 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.025 MB of 0.036 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.036 MB of 0.036 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss ‚ñà‚ñÉ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ‚ñà‚ñÅ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ‚ñÅ‚ñà‚ñÜ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ‚ñÅ‚ñà‚ñÜ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ‚ñà‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 1.08267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 163.4014\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 7.791\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 7.791\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 0.49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 2500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.8296\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 1.11507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2448.8229\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 2.042\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.021\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33munique-microwave-63\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft/runs/sdqvj2a0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240129_231427-sdqvj2a0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#very hacky but maybe this will work:\n",
    "tokenizer.model_input_names = ['labels', 'input_ids', 'attention_mask']\n",
    "# trainer.args.train_batch_size = 1\n",
    "# self.args.train_batch_size\n",
    "\n",
    "#also hacky, but could work:\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "print(\"Starting training\")\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a81beabc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:55:46.882095Z",
     "iopub.status.busy": "2024-01-29T23:55:46.881978Z",
     "iopub.status.idle": "2024-01-29T23:55:46.884278Z",
     "shell.execute_reply": "2024-01-29T23:55:46.884002Z"
    },
    "papermill": {
     "duration": 0.009238,
     "end_time": "2024-01-29T23:55:46.885098",
     "exception": false,
     "start_time": "2024-01-29T23:55:46.875860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/service/BioLlama/utilities/finetuning/biollama_training_output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "684ca6fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:55:46.896710Z",
     "iopub.status.busy": "2024-01-29T23:55:46.896596Z",
     "iopub.status.idle": "2024-01-29T23:56:20.202510Z",
     "shell.execute_reply": "2024-01-29T23:56:20.202110Z"
    },
    "papermill": {
     "duration": 33.3127,
     "end_time": "2024-01-29T23:56:20.203317",
     "exception": false,
     "start_time": "2024-01-29T23:55:46.890617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 26586516\r\n",
      "drwxrwxr-x 2 service service       4096 Jan 28 22:07 checkpoint-12500\r\n",
      "drwxrwxr-x 2 service service       4096 Jan 29 23:55 checkpoint-2500\r\n",
      "-rw-rw-r-- 1 service service        690 Jan 29 23:55 config.json\r\n",
      "-rw-rw-r-- 1 service service        184 Jan 29 23:55 generation_config.json\r\n",
      "drwxrwxr-x 2 service service       4096 Jan 29 23:14 logs\r\n",
      "-rw-rw-r-- 1 service service 4840396416 Jan 29 23:55 model-00001-of-00006.safetensors\r\n",
      "-rw-rw-r-- 1 service service 4857206856 Jan 29 23:55 model-00002-of-00006.safetensors\r\n",
      "-rw-rw-r-- 1 service service 4991441440 Jan 29 23:56 model-00003-of-00006.safetensors\r\n",
      "-rw-rw-r-- 1 service service 4991424864 Jan 29 23:56 model-00004-of-00006.safetensors\r\n",
      "-rw-rw-r-- 1 service service 4857206904 Jan 29 23:56 model-00005-of-00006.safetensors\r\n",
      "-rw-rw-r-- 1 service service 2684472112 Jan 29 23:56 model-00006-of-00006.safetensors\r\n",
      "-rw-rw-r-- 1 service service      24444 Jan 29 23:56 model.safetensors.index.json\r\n",
      "-rw-rw-r-- 1 service service        438 Jan 29 23:56 special_tokens_map.json\r\n",
      "-rw-rw-r-- 1 service service       1763 Jan 29 23:56 tokenizer_config.json\r\n",
      "-rw-rw-r-- 1 service service    1842767 Jan 29 23:56 tokenizer.json\r\n",
      "-rw-rw-r-- 1 service service     499723 Jan 29 23:56 tokenizer.model\r\n",
      "-rw-rw-r-- 1 service service       4792 Jan 29 23:56 training_args.bin\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(output_dir)\n",
    "#print contents of output_dir\n",
    "!ls -l $output_dir\n",
    "#print full path of output_dir\n",
    "# !pwd $output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea96e24b",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16ff2529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T23:56:20.211634Z",
     "iopub.status.busy": "2024-01-29T23:56:20.211515Z",
     "iopub.status.idle": "2024-01-29T23:56:28.372896Z",
     "shell.execute_reply": "2024-01-29T23:56:28.372351Z"
    },
    "papermill": {
     "duration": 8.166124,
     "end_time": "2024-01-29T23:56:28.373452",
     "exception": true,
     "start_time": "2024-01-29T23:56:20.207328",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/service/BioLlama/utilities/finetuning/biollama_training_output/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                       | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                                                                 | 1/6 [00:00<00:02,  1.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                                                    | 2/6 [00:01<00:02,  1.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                       | 3/6 [00:01<00:01,  1.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                          | 4/6 [00:01<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at /home/service/BioLlama/utilities/finetuning/biollama_training_output/ were not used when initializing LlamaForCausalLM: ['model.layers.15.CCA.pre_CCA_layernorm.weight', 'model.layers.15.layer.CCA_attn.k_proj.weight', 'model.layers.15.layer.CCA_attn.o_proj.weight', 'model.layers.15.layer.CCA_attn.q_proj.weight', 'model.layers.15.layer.CCA_attn.v_proj.weight', 'model.layers.15.layer.input_layernorm.weight', 'model.layers.15.layer.mlp.down_proj.weight', 'model.layers.15.layer.mlp.gate_proj.weight', 'model.layers.15.layer.mlp.up_proj.weight', 'model.layers.15.layer.post_attention_layernorm.weight', 'model.layers.15.layer.self_attn.k_proj.weight', 'model.layers.15.layer.self_attn.o_proj.weight', 'model.layers.15.layer.self_attn.q_proj.weight', 'model.layers.15.layer.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/service/BioLlama/utilities/finetuning/biollama_training_output/ and are newly initialized: ['model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping layer 15 with retro\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 29\u001b[0m\n\u001b[1;32m     18\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou are an excellently helpful AI assistant that answers biomedical questions. <QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[38;5;124mn room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m (A) Ampicillin\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m (B) Ceftriaxone\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m (C) Ciprofloxacin\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m (D) Doxycycline\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m (E) Nitrofurantoin</QUESTION>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m<ANSWER> \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# input_ids = new_tokenizer.encode(prompt, return_tensors=\"pt\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# input_ids = new_tokenizer.encode(prompt, return_tensors=\"pt\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# output = new_model.generate(input_ids, max_new_tokens=35, do_sample=True, top_p=0.95, top_k=60)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(new_tokenizer.decode(output[0], skip_special_tokens=True))\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m num_tokens, text \u001b[38;5;241m=\u001b[39m BioLlama\u001b[38;5;241m.\u001b[39mgenerate(prompt\u001b[38;5;241m=\u001b[39mprompt, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m35\u001b[39m)\n",
      "File \u001b[0;32m~/BioLlama/utilities/biollama.py:333\u001b[0m, in \u001b[0;36mBioLlama.generate\u001b[0;34m(self, prompt, max_new_tokens)\u001b[0m\n\u001b[1;32m    331\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    332\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(input_ids)\n\u001b[0;32m--> 333\u001b[0m generate_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    334\u001b[0m     inputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens, use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    335\u001b[0m )\n\u001b[1;32m    336\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(generate_ids)\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    338\u001b[0m     num_tokens,\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m     )[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    344\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/generation/utils.py:1525\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1524\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m   1526\u001b[0m         input_ids,\n\u001b[1;32m   1527\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   1528\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[1;32m   1529\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   1530\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m   1531\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m   1532\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[1;32m   1533\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1534\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   1535\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   1536\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1537\u001b[0m     )\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1542\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1543\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1548\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1549\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/generation/utils.py:2622\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2619\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2621\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2622\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[1;32m   2623\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2624\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2625\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   2626\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2627\u001b[0m )\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2630\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/BioLlama/utilities/biollama.py:289\u001b[0m, in \u001b[0;36mnew_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids_biollama \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 289\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids_biollama \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1183\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1180\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1184\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1185\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1186\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1187\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1188\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1189\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1190\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1191\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1192\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1193\u001b[0m )\n\u001b[1;32m   1195\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1070\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1061\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1062\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         use_cache,\n\u001b[1;32m   1068\u001b[0m     )\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1070\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m   1071\u001b[0m         hidden_states,\n\u001b[1;32m   1072\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1073\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1074\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1075\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1076\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1077\u001b[0m     )\n\u001b[1;32m   1079\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/BioLlama/utilities/biollama.py:269\u001b[0m, in \u001b[0;36mRETROLayer.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_difference \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    268\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m residual[:,\u001b[38;5;241m0\u001b[39m:size_difference,:]\n\u001b[0;32m--> 269\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((prefix, hidden_states), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    270\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "#there is a finetune of llama 2 7b hf in the foler finetuned_models\n",
    "#load this local model here and use it to generate some text\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\"\n",
    "print(output_dir)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from utilities.biollama import BioLlama\n",
    "#answers = [\"Sarcoplasmic reticulum Ca(2+)-ATPase\"] # or \"SERCA\",\"serca2\"\n",
    "\n",
    "chunk_length = 32\n",
    "\n",
    "BioLlama = BioLlama(model_id=output_dir, chunk_length=chunk_length, training=False)\n",
    "# num_tokens, text = BioLlama.generate(prompt=prompt, max_new_tokens=35)\n",
    "\n",
    "# new_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "# new_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
    "prompt = 'You are an excellently helpful AI assistant that answers biomedical questions. <QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \\n (A) Ampicillin\\n (B) Ceftriaxone\\n (C) Ciprofloxacin\\n (D) Doxycycline\\n (E) Nitrofurantoin</QUESTION>\\n<ANSWER> '\n",
    "\n",
    "# input_ids = new_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "# input_ids = new_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# print(input_ids)\n",
    "# print(input_ids.shape)\n",
    "\n",
    "# output = new_model.generate(input_ids, max_new_tokens=35, do_sample=True, top_p=0.95, top_k=60)\n",
    "# print(new_tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt, max_new_tokens=35)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2525.510606,
   "end_time": "2024-01-29T23:56:30.799734",
   "environment_variables": {},
   "exception": true,
   "input_path": "BioLlama_finetuning_WandB_HF.ipynb",
   "output_path": "output.ipynb",
   "parameters": {},
   "start_time": "2024-01-29T23:14:25.289128",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}