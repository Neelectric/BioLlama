{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a198e8",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [28]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb384650",
   "metadata": {
    "papermill": {
     "duration": 0.002823,
     "end_time": "2024-01-23T00:15:21.612463",
     "exception": false,
     "start_time": "2024-01-23T00:15:21.609640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is taken directly from https://github.com/tcapelle/llm_recipes/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
   "metadata": {
    "papermill": {
     "duration": 0.003083,
     "end_time": "2024-01-23T00:15:21.624114",
     "exception": false,
     "start_time": "2024-01-23T00:15:21.621031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# From Llama to Alpaca: Finetunning and LLM with Weights & Biases\n",
    "In this notebooks you will learn how to finetune a pretrained LLama model on an Instruction dataset. We will use an updated version of the Alpaca dataset that, instead of davinci-003 (GPT3) generations uses GPT4 to get an even better instruction dataset! More details on the [official repo page](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data)\n",
    "\n",
    "> This notebook requires a A100/A10 GPU with at least 24GB of memory. You could tweak the params down and run on a T4 but it would take very long time\n",
    "\n",
    "This notebooks has a companion project and [report](wandb.me/alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7744abe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:21.629455Z",
     "iopub.status.busy": "2024-01-23T00:15:21.629296Z",
     "iopub.status.idle": "2024-01-23T00:15:21.631521Z",
     "shell.execute_reply": "2024-01-23T00:15:21.631249Z"
    },
    "papermill": {
     "duration": 0.006271,
     "end_time": "2024-01-23T00:15:21.632599",
     "exception": false,
     "start_time": "2024-01-23T00:15:21.626328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install wandb transformers trl datasets \"protobuf==3.20.3\" evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693a36b-5cdc-4582-844b-8dd52fa522c5",
   "metadata": {
    "papermill": {
     "duration": 0.003761,
     "end_time": "2024-01-23T00:15:21.640125",
     "exception": false,
     "start_time": "2024-01-23T00:15:21.636364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## With Huggingface TRL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04c0a5-f481-4364-880d-10c254388987",
   "metadata": {
    "papermill": {
     "duration": 0.003715,
     "end_time": "2024-01-23T00:15:21.647585",
     "exception": false,
     "start_time": "2024-01-23T00:15:21.643870",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's grab the Alpaca (GPT-4 curated instructions and outputs) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:21.655673Z",
     "iopub.status.busy": "2024-01-23T00:15:21.655545Z",
     "iopub.status.idle": "2024-01-23T00:15:21.726003Z",
     "shell.execute_reply": "2024-01-23T00:15:21.725681Z"
    },
    "papermill": {
     "duration": 0.075526,
     "end_time": "2024-01-23T00:15:21.726817",
     "exception": false,
     "start_time": "2024-01-23T00:15:21.651291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Benchmark from MedQA-USMLE/US/train.jsonl\n",
      "Benchmark contains 10178 questions, made up of 10178 with 5 options and 0 with non-5 options\n",
      "A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\n",
      " (A) Ampicillin\n",
      " (B) Ceftriaxone\n",
      " (C) Ciprofloxacin\n",
      " (D) Doxycycline\n",
      " (E) Nitrofurantoin\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json\n",
    "# from uparse_benchmark import parse_benchmark\n",
    "# from ..utilities.parse_benchmark import parse_benchmark\n",
    "from utilities.parse_benchmark import parse_benchmark\n",
    "\n",
    "benchmark = \"MedQA\"\n",
    "benchmark_questions, benchmark_answers = parse_benchmark(benchmark)\n",
    "print(benchmark_questions[0])\n",
    "# print(benchmark_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fce67d2-3703-4042-816c-7a13ba9eab3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:21.735125Z",
     "iopub.status.busy": "2024-01-23T00:15:21.735032Z",
     "iopub.status.idle": "2024-01-23T00:15:21.736697Z",
     "shell.execute_reply": "2024-01-23T00:15:21.736486Z"
    },
    "papermill": {
     "duration": 0.006731,
     "end_time": "2024-01-23T00:15:21.737492",
     "exception": false,
     "start_time": "2024-01-23T00:15:21.730761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# dataset_file = \"alpaca_gpt4_data.json\"\n",
    "\n",
    "# with open(dataset_file, \"r\") as f:\n",
    "#     alpaca = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a154f968-da0f-4bdc-bf45-e499d95f0606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:21.745573Z",
     "iopub.status.busy": "2024-01-23T00:15:21.745480Z",
     "iopub.status.idle": "2024-01-23T00:15:26.581409Z",
     "shell.execute_reply": "2024-01-23T00:15:26.581140Z"
    },
    "papermill": {
     "duration": 4.840956,
     "end_time": "2024-01-23T00:15:26.582235",
     "exception": false,
     "start_time": "2024-01-23T00:15:21.741279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnelectric\u001b[0m (\u001b[33mneelectric\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/service/BioLlama/wandb/run-20240123_001522-e1thks20\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mblooming-sun-12\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft/runs/e1thks20\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/neelectric/biollama_ft/runs/e1thks20?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f3c1e053510>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"biollama_ft\", # the project I am working on\n",
    "           tags=[\"hf_sft\"]) # the Hyperparameters I want to keep track of\n",
    "# artifact = wandb.use_artifact('capecape/alpaca_ft/alpaca_gpt4_splitted:latest', type='dataset')\n",
    "# artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "811291cc-b7ce-422a-8971-3cbf9fe10a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:26.596031Z",
     "iopub.status.busy": "2024-01-23T00:15:26.595877Z",
     "iopub.status.idle": "2024-01-23T00:15:27.393905Z",
     "shell.execute_reply": "2024-01-23T00:15:27.393623Z"
    },
    "papermill": {
     "duration": 0.808121,
     "end_time": "2024-01-23T00:15:27.394838",
     "exception": false,
     "start_time": "2024-01-23T00:15:26.586717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# print(artifact_dir)\n",
    "artifact_dir = os.getcwd() + \"/benchmarks/MedQA-USMLE/\"\n",
    "from datasets import load_dataset\n",
    "medqa = load_dataset(\"json\", data_dir=artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9e0ed7-375f-431a-9420-0022cf5566f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:27.408782Z",
     "iopub.status.busy": "2024-01-23T00:15:27.408259Z",
     "iopub.status.idle": "2024-01-23T00:15:27.411557Z",
     "shell.execute_reply": "2024-01-23T00:15:27.411330Z"
    },
    "papermill": {
     "duration": 0.012838,
     "end_time": "2024-01-23T00:15:27.412186",
     "exception": false,
     "start_time": "2024-01-23T00:15:27.399348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 10178\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 1272\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 1273\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb96e0-b2f2-4a79-ba65-e1c8d6395d54",
   "metadata": {
    "papermill": {
     "duration": 0.003857,
     "end_time": "2024-01-23T00:15:27.420096",
     "exception": false,
     "start_time": "2024-01-23T00:15:27.416239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's log the dataset also as a table so we can inspect it on the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c7dda87-d70a-470b-a0f9-040af434dc42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:27.428284Z",
     "iopub.status.busy": "2024-01-23T00:15:27.428186Z",
     "iopub.status.idle": "2024-01-23T00:15:27.430533Z",
     "shell.execute_reply": "2024-01-23T00:15:27.430301Z"
    },
    "papermill": {
     "duration": 0.007279,
     "end_time": "2024-01-23T00:15:27.431264",
     "exception": false,
     "start_time": "2024-01-23T00:15:27.423985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10178\n",
      "1273\n"
     ]
    }
   ],
   "source": [
    "train_dataset = medqa[\"train\"]\n",
    "eval_dataset = medqa[\"test\"]\n",
    "#print sizes\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "# turn both of these into only half their size\n",
    "# train_dataset = train_dataset.select(range(0, len(train_dataset)//2))\n",
    "# eval_dataset = eval_dataset.select(range(0, len(eval_dataset)//2))\n",
    "\n",
    "# print(len(train_dataset))\n",
    "# print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a038eb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:27.437041Z",
     "iopub.status.busy": "2024-01-23T00:15:27.436929Z",
     "iopub.status.idle": "2024-01-23T00:15:27.439365Z",
     "shell.execute_reply": "2024-01-23T00:15:27.439211Z"
    },
    "papermill": {
     "duration": 0.005878,
     "end_time": "2024-01-23T00:15:27.439808",
     "exception": false,
     "start_time": "2024-01-23T00:15:27.433930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an excellently helpful AI assistant that answers biomedical questions. <QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \\n (A) Ampicillin\\n (B) Ceftriaxone\\n (C) Ciprofloxacin\\n (D) Doxycycline\\n (E) Nitrofurantoin</QUESTION>\\n<ANSWER> (E) Nitrofurantoin</ANSWER>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_prompt(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"You are an excellently helpful AI assistant that answers biomedical questions. \"\n",
    "            \"<QUESTION>{question} {option_string}</QUESTION>\\n<ANSWER> ({answer_idx}) {answer}</ANSWER>\").format_map(row)\n",
    "create_prompt(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f1a9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:27.445707Z",
     "iopub.status.busy": "2024-01-23T00:15:27.445541Z",
     "iopub.status.idle": "2024-01-23T00:15:27.449228Z",
     "shell.execute_reply": "2024-01-23T00:15:27.449078Z"
    },
    "papermill": {
     "duration": 0.007186,
     "end_time": "2024-01-23T00:15:27.449659",
     "exception": false,
     "start_time": "2024-01-23T00:15:27.442473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an excellently helpful AI assistant that answers biomedical questions. <QUESTION>A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take? \\n (A) Disclose the error to the patient but leave it out of the operative report\\n (B) Disclose the error to the patient and put it in the operative report\\n (C) Tell the attending that he cannot fail to disclose this mistake\\n (D) Report the physician to the ethics committee\\n (E) Refuse to dictate the operative report</QUESTION>\\n<ANSWER> '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def return_prompt_no_answer(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"You are an excellently helpful AI assistant that answers biomedical questions. \"\n",
    "            \"<QUESTION>{question} {option_string}</QUESTION>\\n<ANSWER> \").format_map(row)\n",
    "\n",
    "def create_prompt_no_answer(row):\n",
    "    return {\"text\": return_prompt_no_answer(row)}\n",
    "    \n",
    "test_dataset = eval_dataset.map(create_prompt_no_answer)\n",
    "test_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe56c16e",
   "metadata": {
    "papermill": {
     "duration": 0.002653,
     "end_time": "2024-01-23T00:15:27.455056",
     "exception": false,
     "start_time": "2024-01-23T00:15:27.452403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "720c707b-3bce-4164-b8c1-3c3122200c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:27.460997Z",
     "iopub.status.busy": "2024-01-23T00:15:27.460883Z",
     "iopub.status.idle": "2024-01-23T00:15:28.251376Z",
     "shell.execute_reply": "2024-01-23T00:15:28.251018Z"
    },
    "papermill": {
     "duration": 0.794556,
     "end_time": "2024-01-23T00:15:28.252326",
     "exception": false,
     "start_time": "2024-01-23T00:15:27.457770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# from cti.transformers.transformers.src.transformers.models.auto import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4162aec8-f2ba-45db-9633-817b416d4e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:28.264390Z",
     "iopub.status.busy": "2024-01-23T00:15:28.264192Z",
     "iopub.status.idle": "2024-01-23T00:15:28.266140Z",
     "shell.execute_reply": "2024-01-23T00:15:28.265884Z"
    },
    "papermill": {
     "duration": 0.01131,
     "end_time": "2024-01-23T00:15:28.266668",
     "exception": false,
     "start_time": "2024-01-23T00:15:28.255358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51c10f7f-2551-4aa4-aef2-29c888b57a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:28.272481Z",
     "iopub.status.busy": "2024-01-23T00:15:28.272308Z",
     "iopub.status.idle": "2024-01-23T00:15:31.777045Z",
     "shell.execute_reply": "2024-01-23T00:15:31.776762Z"
    },
    "papermill": {
     "duration": 3.508333,
     "end_time": "2024-01-23T00:15:31.777686",
     "exception": false,
     "start_time": "2024-01-23T00:15:28.269353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                       | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                       | 1/2 [00:01<00:01,  1.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # load_in_4bit = True, # this causes \"RuntimeError: only Tensors of floating point and complex dtype can require gradients\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79dbbb0-6dac-4f78-a862-8201088c9d57",
   "metadata": {
    "papermill": {
     "duration": 0.00892,
     "end_time": "2024-01-23T00:15:31.789835",
     "exception": false,
     "start_time": "2024-01-23T00:15:31.780915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Training the full models is expensive, but if you have a GPU that can fit the full model, you can skip this part. Let's just train the last 8 layers of the model (Llama2-7B has 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:31.796616Z",
     "iopub.status.busy": "2024-01-23T00:15:31.796480Z",
     "iopub.status.idle": "2024-01-23T00:15:31.799272Z",
     "shell.execute_reply": "2024-01-23T00:15:31.799077Z"
    },
    "papermill": {
     "duration": 0.006788,
     "end_time": "2024-01-23T00:15:31.799799",
     "exception": false,
     "start_time": "2024-01-23T00:15:31.793011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_freeze = 15\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): param.requires_grad = True\n",
    "for param in model.model.layers[n_freeze].parameters(): param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:31.805825Z",
     "iopub.status.busy": "2024-01-23T00:15:31.805723Z",
     "iopub.status.idle": "2024-01-23T00:15:31.807336Z",
     "shell.execute_reply": "2024-01-23T00:15:31.807182Z"
    },
    "papermill": {
     "duration": 0.005222,
     "end_time": "2024-01-23T00:15:31.807845",
     "exception": false,
     "start_time": "2024-01-23T00:15:31.802623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "model.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:31.813918Z",
     "iopub.status.busy": "2024-01-23T00:15:31.813815Z",
     "iopub.status.idle": "2024-01-23T00:15:31.817067Z",
     "shell.execute_reply": "2024-01-23T00:15:31.816841Z"
    },
    "papermill": {
     "duration": 0.006868,
     "end_time": "2024-01-23T00:15:31.817560",
     "exception": false,
     "start_time": "2024-01-23T00:15:31.810692",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 333.46M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f9c99da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:31.823757Z",
     "iopub.status.busy": "2024-01-23T00:15:31.823655Z",
     "iopub.status.idle": "2024-01-23T00:15:31.825717Z",
     "shell.execute_reply": "2024-01-23T00:15:31.825446Z"
    },
    "papermill": {
     "duration": 0.005833,
     "end_time": "2024-01-23T00:15:31.826256",
     "exception": false,
     "start_time": "2024-01-23T00:15:31.820423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f07c7e75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:31.832407Z",
     "iopub.status.busy": "2024-01-23T00:15:31.832294Z",
     "iopub.status.idle": "2024-01-23T00:15:31.833911Z",
     "shell.execute_reply": "2024-01-23T00:15:31.833662Z"
    },
    "papermill": {
     "duration": 0.005312,
     "end_time": "2024-01-23T00:15:31.834454",
     "exception": false,
     "start_time": "2024-01-23T00:15:31.829142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # !pip uninstall transformers -y\n",
    "# !pip install transformers\n",
    "# !pip install -i https://pip.repos.neuron.amazonaws.com transformers-neuronx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac3525d7-3028-499e-8749-c6fbc21a26d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:31.840793Z",
     "iopub.status.busy": "2024-01-23T00:15:31.840681Z",
     "iopub.status.idle": "2024-01-23T00:15:32.018251Z",
     "shell.execute_reply": "2024-01-23T00:15:32.017815Z"
    },
    "papermill": {
     "duration": 0.181844,
     "end_time": "2024-01-23T00:15:32.019180",
     "exception": false,
     "start_time": "2024-01-23T00:15:31.837336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c57893e-6e42-4cda-83c8-1e51a86d2da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:32.032939Z",
     "iopub.status.busy": "2024-01-23T00:15:32.032611Z",
     "iopub.status.idle": "2024-01-23T00:15:32.035668Z",
     "shell.execute_reply": "2024-01-23T00:15:32.035374Z"
    },
    "papermill": {
     "duration": 0.013738,
     "end_time": "2024-01-23T00:15:32.036185",
     "exception": false,
     "start_time": "2024-01-23T00:15:32.022447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "total_num_steps = 11_210 // batch_size\n",
    "print(total_num_steps)\n",
    "# print(\"changing total batch size down to 50 to save time\")\n",
    "# total_num_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "154326d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:32.042509Z",
     "iopub.status.busy": "2024-01-23T00:15:32.042428Z",
     "iopub.status.idle": "2024-01-23T00:15:32.044017Z",
     "shell.execute_reply": "2024-01-23T00:15:32.043757Z"
    },
    "papermill": {
     "duration": 0.00522,
     "end_time": "2024-01-23T00:15:32.044496",
     "exception": false,
     "start_time": "2024-01-23T00:15:32.039276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from cti.transformers.transformers.src.transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "098dea68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:32.066198Z",
     "iopub.status.busy": "2024-01-23T00:15:32.066088Z",
     "iopub.status.idle": "2024-01-23T00:15:32.067774Z",
     "shell.execute_reply": "2024-01-23T00:15:32.067504Z"
    },
    "papermill": {
     "duration": 0.005562,
     "end_time": "2024-01-23T00:15:32.068317",
     "exception": false,
     "start_time": "2024-01-23T00:15:32.062755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip uninstall torch_xla -y\n",
    "# !pip install -i https://pip.repos.neuron.amazonaws.com torch-xla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c86339ca-27e5-496b-9559-3a65bb3c26a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:32.074465Z",
     "iopub.status.busy": "2024-01-23T00:15:32.074364Z",
     "iopub.status.idle": "2024-01-23T00:15:32.077678Z",
     "shell.execute_reply": "2024-01-23T00:15:32.077351Z"
    },
    "papermill": {
     "duration": 0.007078,
     "end_time": "2024-01-23T00:15:32.078265",
     "exception": false,
     "start_time": "2024-01-23T00:15:32.071187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/finetuned_models/\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size//4,\n",
    "    bf16=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=total_num_steps // 10,\n",
    "    # num_train_epochs=1,\n",
    "    max_steps=total_num_steps,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=total_num_steps // 3,\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\", #changed to epoch so we save every epoch i guess?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13c056",
   "metadata": {
    "papermill": {
     "duration": 0.002867,
     "end_time": "2024-01-23T00:15:32.084019",
     "exception": false,
     "start_time": "2024-01-23T00:15:32.081152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e35a9b70-f36b-4bfd-857f-c80d3450e111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:32.090491Z",
     "iopub.status.busy": "2024-01-23T00:15:32.090200Z",
     "iopub.status.idle": "2024-01-23T00:15:32.320415Z",
     "shell.execute_reply": "2024-01-23T00:15:32.320066Z"
    },
    "papermill": {
     "duration": 0.234102,
     "end_time": "2024-01-23T00:15:32.321012",
     "exception": false,
     "start_time": "2024-01-23T00:15:32.086910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from utils import LLMSampleCB, token_accuracy\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    packing=True,\n",
    "    max_seq_length=1024,\n",
    "    args=training_args,\n",
    "    formatting_func=create_prompt,\n",
    "    # compute_metrics=token_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b129b",
   "metadata": {
    "papermill": {
     "duration": 0.008446,
     "end_time": "2024-01-23T00:15:32.332747",
     "exception": false,
     "start_time": "2024-01-23T00:15:32.324301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df6d1457-3c7a-47e6-994b-334dbe9c3d5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:32.339138Z",
     "iopub.status.busy": "2024-01-23T00:15:32.339050Z",
     "iopub.status.idle": "2024-01-23T00:15:32.340776Z",
     "shell.execute_reply": "2024-01-23T00:15:32.340501Z"
    },
    "papermill": {
     "duration": 0.005552,
     "end_time": "2024-01-23T00:15:32.341317",
     "exception": false,
     "start_time": "2024-01-23T00:15:32.335765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.add_callback(wandb_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5754789-2e15-4bc9-800c-01f8ffc625e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:15:32.347538Z",
     "iopub.status.busy": "2024-01-23T00:15:32.347439Z",
     "iopub.status.idle": "2024-01-23T00:39:48.567489Z",
     "shell.execute_reply": "2024-01-23T00:39:48.567054Z"
    },
    "papermill": {
     "duration": 1456.224508,
     "end_time": "2024-01-23T00:39:48.568717",
     "exception": false,
     "start_time": "2024-01-23T00:15:32.344209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [350/350 24:04, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.941800</td>\n",
       "      <td>1.013588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.906100</td>\n",
       "      <td>0.989663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>0.986290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/utils.py:268: UserWarning: The dataset reached end and the iterator is reset to the start.\n",
      "  warnings.warn(\"The dataset reached end and the iterator is reset to the start.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory /home/service/BioLlama/utilities/finetuning/finetuned_models/checkpoint-350 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.004 MB of 0.004 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.004 MB of 0.004 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.004 MB of 0.004 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.004 MB of 0.031 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.004 MB of 0.031 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.031 MB of 0.031 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss ‚ñà‚ñÇ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ‚ñÅ‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ‚ñà‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ‚ñà‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.98629\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 46.2305\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 27.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 3.461\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 1.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 350\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.9028\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 4.546698147790848e+17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.96254\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1449.2455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 7.728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.242\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mblooming-sun-12\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft/runs/e1thks20\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240123_001522-e1thks20/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a81beabc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:39:48.583945Z",
     "iopub.status.busy": "2024-01-23T00:39:48.583815Z",
     "iopub.status.idle": "2024-01-23T00:39:48.586057Z",
     "shell.execute_reply": "2024-01-23T00:39:48.585759Z"
    },
    "papermill": {
     "duration": 0.012113,
     "end_time": "2024-01-23T00:39:48.586883",
     "exception": false,
     "start_time": "2024-01-23T00:39:48.574770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/service/BioLlama/utilities/finetuning/finetuned_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "684ca6fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:39:48.598766Z",
     "iopub.status.busy": "2024-01-23T00:39:48.598640Z",
     "iopub.status.idle": "2024-01-23T00:40:05.629538Z",
     "shell.execute_reply": "2024-01-23T00:40:05.629104Z"
    },
    "papermill": {
     "duration": 17.037696,
     "end_time": "2024-01-23T00:40:05.630153",
     "exception": false,
     "start_time": "2024-01-23T00:39:48.592457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 13163372\r\n",
      "drwxrwxr-x 2 service service       4096 Jan 17 00:52 checkpoint-350\r\n",
      "drwxrwxr-x 2 service service       4096 Jan 22 11:09 checkpoint-50\r\n",
      "-rw-rw-r-- 1 service service        685 Jan 23 00:39 config.json\r\n",
      "-rw-rw-r-- 1 service service        183 Jan 23 00:39 generation_config.json\r\n",
      "drwxrwxr-x 2 service service       4096 Jan 23 00:15 logs\r\n",
      "-rw-rw-r-- 1 service service 4938985352 Jan 23 00:39 model-00001-of-00003.safetensors\r\n",
      "-rw-rw-r-- 1 service service 4947390880 Jan 23 00:39 model-00002-of-00003.safetensors\r\n",
      "-rw-rw-r-- 1 service service 3590488816 Jan 23 00:40 model-00003-of-00003.safetensors\r\n",
      "-rw-rw-r-- 1 service service      23950 Jan 23 00:40 model.safetensors.index.json\r\n",
      "-rw-rw-r-- 1 service service        437 Jan 23 00:40 special_tokens_map.json\r\n",
      "-rw-rw-r-- 1 service service        920 Jan 23 00:40 tokenizer_config.json\r\n",
      "-rw-rw-r-- 1 service service    1842767 Jan 23 00:40 tokenizer.json\r\n",
      "-rw-rw-r-- 1 service service     499723 Jan 23 00:40 tokenizer.model\r\n",
      "-rw-rw-r-- 1 service service       4792 Jan 23 00:40 training_args.bin\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(output_dir)\n",
    "#print contents of output_dir\n",
    "!ls -l $output_dir\n",
    "#print full path of output_dir\n",
    "# !pwd $output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396e9a6",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16ff2529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T00:40:05.638190Z",
     "iopub.status.busy": "2024-01-23T00:40:05.638075Z",
     "iopub.status.idle": "2024-01-23T00:40:05.641101Z",
     "shell.execute_reply": "2024-01-23T00:40:05.640795Z"
    },
    "papermill": {
     "duration": 0.00768,
     "end_time": "2024-01-23T00:40:05.641631",
     "exception": true,
     "start_time": "2024-01-23T00:40:05.633951",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3336475183.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[28], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    output = new_model.generate(input_ids, max_new_tokens=35 do_sample=True, top_p=0.95, top_k=60)\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "#there is a finetune of llama 2 7b hf in the foler finetuned_models\n",
    "#load this local model here and use it to generate some text\n",
    "\n",
    "print(output_dir)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "new_model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
    "prompt = 'You are an excellently helpful AI assistant that answers biomedical questions. <QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?</QUESTION>\\n<ANSWER> '\n",
    "\n",
    "input_ids = new_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "# input_ids = new_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# print(input_ids)\n",
    "# print(input_ids.shape)\n",
    "\n",
    "output = new_model.generate(input_ids, max_new_tokens=35, do_sample=True, top_p=0.95, top_k=60)\n",
    "print(new_tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db817255",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433d006",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1486.136838,
   "end_time": "2024-01-23T00:40:07.064792",
   "environment_variables": {},
   "exception": true,
   "input_path": "BioLlama_finetuning_WandB_HF.ipynb",
   "output_path": "output.ipynb",
   "parameters": {},
   "start_time": "2024-01-23T00:15:20.927954",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
