{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb384650",
   "metadata": {
    "papermill": {
     "duration": 0.002736,
     "end_time": "2024-03-09T18:53:12.288327",
     "exception": false,
     "start_time": "2024-03-09T18:53:12.285591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is taken directly from https://github.com/tcapelle/llm_recipes/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
   "metadata": {
    "papermill": {
     "duration": 0.002936,
     "end_time": "2024-03-09T18:53:12.299960",
     "exception": false,
     "start_time": "2024-03-09T18:53:12.297024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Finetuning Llama-2 to produce BioLlama using HF and WanB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa5b061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:53:12.304992Z",
     "iopub.status.busy": "2024-03-09T18:53:12.304830Z",
     "iopub.status.idle": "2024-03-09T18:53:42.430013Z",
     "shell.execute_reply": "2024-03-09T18:53:42.429615Z"
    },
    "papermill": {
     "duration": 30.130396,
     "end_time": "2024-03-09T18:53:42.432427",
     "exception": false,
     "start_time": "2024-03-09T18:53:12.302031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both GPUs are free\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both GPUs are still free\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    GPUs = GPUtil.getGPUs()\n",
    "    GPU_0_used = GPUs[0].memoryUtil > 0.1\n",
    "    GPU_1_used = GPUs[1].memoryUtil > 0.1\n",
    "    if (not GPU_0_used) and (not GPU_1_used):\n",
    "        print(\"Both GPUs are free\")\n",
    "        # wait for 30 seconds:\n",
    "        time.sleep(30)\n",
    "        # check again\n",
    "        GPUs = GPUtil.getGPUs()\n",
    "        GPU_0_used = GPUs[0].memoryUtil > 0.1\n",
    "        GPU_1_used = GPUs[1].memoryUtil > 0.1\n",
    "        if (not GPU_0_used) and (not GPU_1_used):\n",
    "            print(\"Both GPUs are still free\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"At least one GPU is used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:53:42.443570Z",
     "iopub.status.busy": "2024-03-09T18:53:42.443428Z",
     "iopub.status.idle": "2024-03-09T18:53:42.446240Z",
     "shell.execute_reply": "2024-03-09T18:53:42.445984Z"
    },
    "papermill": {
     "duration": 0.006318,
     "end_time": "2024-03-09T18:53:42.446833",
     "exception": false,
     "start_time": "2024-03-09T18:53:42.440515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install wandb transformers trl datasets \"protobuf==3.20.3\" evaluate\n",
    "# !wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json\n",
    "from utilities.parse_benchmark import parse_benchmark\n",
    "# benchmark = \"MedQA-4\"\n",
    "# benchmark = \"MedQA-5\"\n",
    "benchmark = \"MedMCQA\"\n",
    "# benchmark = \"PubMedQA\"\n",
    "# benchmark = \"bioASQ_with_snippet\"\n",
    "# if benchmark == \"PubMedQA\":\n",
    "#     benchmark_questions, benchmark_answers = parse_benchmark(benchmark, \"test.json\")\n",
    "# else:\n",
    "#     benchmark_questions, benchmark_answers = parse_benchmark(benchmark, \"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b16861",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:53:42.451600Z",
     "iopub.status.busy": "2024-03-09T18:53:42.451484Z",
     "iopub.status.idle": "2024-03-09T18:53:43.346492Z",
     "shell.execute_reply": "2024-03-09T18:53:43.346157Z"
    },
    "papermill": {
     "duration": 0.898305,
     "end_time": "2024-03-09T18:53:43.347234",
     "exception": false,
     "start_time": "2024-03-09T18:53:42.448929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRO_layer_ids is [15] and torch_dtype is torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "size = \"7\"\n",
    "if size == \"7\":\n",
    "    RETRO_layer_ids = [15]\n",
    "    torch_dtype=torch.float32\n",
    "elif size == \"13\":\n",
    "    RETRO_layer_ids = [19]\n",
    "    torch_dtype=torch.bfloat16\n",
    "elif size == \"70\":\n",
    "    RETRO_layer_ids = [39]\n",
    "    torch_dtype=torch.bfloat16\n",
    "    print(\"best of luck training 70b lol\")\n",
    "print(f\"RETRO_layer_ids is {RETRO_layer_ids} and torch_dtype is {torch_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13f2743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:53:43.360161Z",
     "iopub.status.busy": "2024-03-09T18:53:43.359998Z",
     "iopub.status.idle": "2024-03-09T18:53:43.362637Z",
     "shell.execute_reply": "2024-03-09T18:53:43.362407Z"
    },
    "papermill": {
     "duration": 0.012428,
     "end_time": "2024-03-09T18:53:43.363433",
     "exception": false,
     "start_time": "2024-03-09T18:53:43.351005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5605\n",
      "changing total num size to 200000\n",
      "MedMCQA\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "total_num_steps = 11_210 // batch_size\n",
    "print(total_num_steps)\n",
    "if benchmark == \"MedQA-4\" or benchmark == \"MedQA-5\":\n",
    "    total_num_steps = 10178 \n",
    "elif benchmark == \"MedMCQA\":\n",
    "    total_num_steps = 100000 * 2\n",
    "elif benchmark == \"PubMedQA\":\n",
    "    total_num_steps = 1000 \n",
    "elif benchmark == \"bioASQ_with_snippet\":\n",
    "    total_num_steps = 486 \n",
    "print(f\"changing total num size to {total_num_steps}\")\n",
    "print(benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a154f968-da0f-4bdc-bf45-e499d95f0606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:53:43.370938Z",
     "iopub.status.busy": "2024-03-09T18:53:43.370838Z",
     "iopub.status.idle": "2024-03-09T18:53:48.306762Z",
     "shell.execute_reply": "2024-03-09T18:53:48.306538Z"
    },
    "papermill": {
     "duration": 4.940621,
     "end_time": "2024-03-09T18:53:48.307555",
     "exception": false,
     "start_time": "2024-03-09T18:53:43.366934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnelectric\u001b[0m (\u001b[33mneelectric\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/service/BioLlama/wandb/run-20240309_185344-td7xgvdx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmild-galaxy-211\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft/runs/td7xgvdx\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "wandb.init(project=\"biollama_ft\", # the project I am working on\n",
    "           tags=[\"hf_sft\", \"BioLlama\"]) # the Hyperparameters I want to keep track of\n",
    "\n",
    "wandb.alert(\n",
    "    title=\"Starting MedMCQA run\",\n",
    "    text=f\"We have started training\",\n",
    "    level=AlertLevel.WARN,\n",
    "    wait_duration=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "811291cc-b7ce-422a-8971-3cbf9fe10a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:53:48.316062Z",
     "iopub.status.busy": "2024-03-09T18:53:48.315960Z",
     "iopub.status.idle": "2024-03-09T18:53:49.186520Z",
     "shell.execute_reply": "2024-03-09T18:53:49.186171Z"
    },
    "papermill": {
     "duration": 0.875735,
     "end_time": "2024-03-09T18:53:49.187425",
     "exception": false,
     "start_time": "2024-03-09T18:53:48.311690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import json\n",
    "if benchmark == \"PubMedQA\":\n",
    "    artifact_dir = os.getcwd() + \"/benchmarks/PubMedQA/edited\"\n",
    "    dataset = load_dataset(\"json\", data_dir=artifact_dir)\n",
    "else:\n",
    "    if benchmark == \"MedQA-4\":\n",
    "        artifact_dir = os.getcwd() + \"/benchmarks/MedQA-4-option/\"\n",
    "    elif benchmark == \"MedQA-5\":\n",
    "        artifact_dir = os.getcwd() + \"/benchmarks/MedQA-USMLE/\"\n",
    "    elif benchmark == \"MedMCQA\":\n",
    "        artifact_dir = os.getcwd() + \"/benchmarks/MedMCQA/\"\n",
    "    elif benchmark == \"bioASQ_with_snippet\":\n",
    "        print(\"loading bioASQ\")\n",
    "        artifact_dir = os.getcwd() + \"/benchmarks/BioASQ/edited\"\n",
    "    dataset = load_dataset(\"json\", data_dir=artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3f4c49e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:53:49.195615Z",
     "iopub.status.busy": "2024-03-09T18:53:49.195436Z",
     "iopub.status.idle": "2024-03-09T18:53:49.198345Z",
     "shell.execute_reply": "2024-03-09T18:53:49.198163Z"
    },
    "papermill": {
     "duration": 0.007,
     "end_time": "2024-03-09T18:53:49.198878",
     "exception": false,
     "start_time": "2024-03-09T18:53:49.191878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'exp', 'cop', 'opa', 'opb', 'opc', 'opd', 'subject_name', 'topic_name', 'id', 'choice_type'],\n",
       "        num_rows: 182822\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'exp', 'cop', 'opa', 'opb', 'opc', 'opd', 'subject_name', 'topic_name', 'id', 'choice_type'],\n",
       "        num_rows: 4183\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7dda87-d70a-470b-a0f9-040af434dc42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:53:49.205633Z",
     "iopub.status.busy": "2024-03-09T18:53:49.205532Z",
     "iopub.status.idle": "2024-03-09T18:53:49.207798Z",
     "shell.execute_reply": "2024-03-09T18:53:49.207629Z"
    },
    "papermill": {
     "duration": 0.006231,
     "end_time": "2024-03-09T18:53:49.208305",
     "exception": false,
     "start_time": "2024-03-09T18:53:49.202074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4183\n",
      "182822\n"
     ]
    }
   ],
   "source": [
    "if benchmark == \"PubMedQA\" or benchmark == \"bioASQ_with_snippet\":\n",
    "    train_dataset = dataset[\"test\"]\n",
    "else:\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"validation\"]\n",
    "    print(len(eval_dataset))\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a038eb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:53:49.215249Z",
     "iopub.status.busy": "2024-03-09T18:53:49.215156Z",
     "iopub.status.idle": "2024-03-09T18:53:49.219919Z",
     "shell.execute_reply": "2024-03-09T18:53:49.219755Z"
    },
    "papermill": {
     "duration": 0.008892,
     "end_time": "2024-03-09T18:53:49.220425",
     "exception": false,
     "start_time": "2024-03-09T18:53:49.211533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You start all of your responses with <ANSWER> and end them with </ANSWER>, as shown in the following example: \n",
      "<QUESTION> Which of the following is not true for myelinated nerve fibers: \n",
      "(1) Impulse through myelinated fibers is slower than non-myelinated fibers \n",
      "(2) Membrane currents are generated at nodes of Ranvier \n",
      "(3) Saltatory conduction of impulses is seen \n",
      "(4) Local anesthesia is effective only when the nerve is not covered by myelin sheath</QUESTION>\n",
      "<ANSWER> 3</ANSWER>\n",
      "Select the correct choice for the following question. State nothing other than the index of the correct choice, without brackets.\n",
      "<QUESTION>Scrub typhus is transmitted by: September 2004\n",
      "(1) Louse\n",
      "(2) Tick\n",
      "(3) Mite\n",
      "(4) Milk</QUESTION>\n",
      "<ANSWER> 3</ANSWER>\n"
     ]
    }
   ],
   "source": [
    "from utilities.prompts2 import promptify\n",
    "# def create_prompt(row):\n",
    "#     option_string = \"\"\n",
    "#     for option in row[\"options\"].keys():\n",
    "#         option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "#     row[\"option_string\"] = option_string\n",
    "#     return (\"<QUESTION>{question} {option_string}</QUESTION><ANSWER> ({answer_idx}) {answer}</ANSWER>\").format_map(row)\n",
    "def create_prompt(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    MCQ_answer = \"(\" + row['answer_idx'] + \") \" + row[\"answer\"]\n",
    "    question = row[\"question\"] + option_string\n",
    "    promptified = promptify(benchmark, question, retrieval_mode = None, retrieved_chunks = None, model = None)\n",
    "    row[\"promptified\"] = promptified\n",
    "    row[\"MCQ_answer\"] = MCQ_answer\n",
    "    return (\"{promptified} {MCQ_answer}</ANSWER>\").format_map(row)\n",
    "\n",
    "if benchmark == \"MedMCQA\":\n",
    "    def create_prompt(row):\n",
    "        option_string = \"\\n(1) \" + row['opa']\n",
    "        option_string += \"\\n(2) \" + row['opb']\n",
    "        option_string += \"\\n(3) \" + row['opc']\n",
    "        option_string += \"\\n(4) \" + row['opd']\n",
    "        row[\"option_string\"] = option_string\n",
    "        if row['cop'] == 1:\n",
    "            row['answer'] = row['opa']\n",
    "        elif row['cop'] == 2:\n",
    "            row['answer'] = row['opb']\n",
    "        elif row['cop'] == 3:\n",
    "            row['answer'] = row['opc']\n",
    "        elif row['cop'] == 4:\n",
    "            row['answer'] = row['opd']\n",
    "        question = row[\"question\"] + option_string\n",
    "        promptified = promptify(benchmark, question, retrieval_mode = None, retrieved_chunks = None, model = None)\n",
    "        #replace all occurrences of \"{\" with \"(\":\n",
    "        promptified = promptified.replace(\"{\", \"(\")\n",
    "        promptified = promptified.replace(\"}\", \")\")\n",
    "        return (promptified + \" {cop}</ANSWER>\").format_map(row)\n",
    "elif benchmark == \"PubMedQA\":\n",
    "    def create_prompt(row):\n",
    "        snippet_string = \"\"\n",
    "        for snippet in row[\"CONTEXTS\"]:\n",
    "            snippet_string += snippet + \"\\n\"\n",
    "        row[\"snippet_string\"] = snippet_string\n",
    "        row[\"example\"] = \"You start all of your responses with <ANSWER> and end them with </ANSWER>, as shown in the following example:\\n<QUESTION>Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?</QUESTION>\\n<ANSWER> yes</ANSWER>\\nDo not justify your response, respond with only yes, maybe or no.\\n\"\n",
    "        return (\"Using the following text snippets, answer the question that follows.\\n<SNIPPETS>\\n{snippet_string}</SNIPPETS>\\n{example}<QUESTION>{QUESTION}</QUESTION>\\n<ANSWER> {final_decision}</ANSWER>\").format_map(row)\n",
    "elif benchmark == \"bioASQ_with_snippet\":\n",
    "    def create_prompt(row):\n",
    "        question = [row['snippets'], row['question']]\n",
    "        # print(question)\n",
    "        promptified = promptify(\"bioASQ_with_snippet\", question, retrieval_mode = None, retrieved_chunks = None, model = None)\n",
    "        return promptified + \" \" + row['answer'] + \"</ANSWER>\"\n",
    "print(create_prompt(train_dataset[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f1a9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:53:49.227294Z",
     "iopub.status.busy": "2024-03-09T18:53:49.227198Z",
     "iopub.status.idle": "2024-03-09T18:53:49.234464Z",
     "shell.execute_reply": "2024-03-09T18:53:49.234300Z"
    },
    "papermill": {
     "duration": 0.011383,
     "end_time": "2024-03-09T18:53:49.235017",
     "exception": false,
     "start_time": "2024-03-09T18:53:49.223634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You start all of your responses with <ANSWER> and end them with </ANSWER>, as shown in the following example: \n",
      "<QUESTION> Which of the following is not true for myelinated nerve fibers: \n",
      "(1) Impulse through myelinated fibers is slower than non-myelinated fibers \n",
      "(2) Membrane currents are generated at nodes of Ranvier \n",
      "(3) Saltatory conduction of impulses is seen \n",
      "(4) Local anesthesia is effective only when the nerve is not covered by myelin sheath</QUESTION>\n",
      "<ANSWER> 3</ANSWER>\n",
      "Select the correct choice for the following question. State nothing other than the index of the correct choice, without brackets.\n",
      "<QUESTION>Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchyma\n",
      "(1) Hyperplasia\n",
      "(2) Hyperophy\n",
      "(3) Atrophy\n",
      "(4) Dyplasia</QUESTION>\n",
      "<ANSWER> 3</ANSWER>\n"
     ]
    }
   ],
   "source": [
    "def create_prompt_no_answer(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"<QUESTION>{question} {option_string}</QUESTION><ANSWER> \").format_map(row)\n",
    "\n",
    "def return_prompt_no_answer(row):\n",
    "    return {\"text\": create_prompt_no_answer(row)}\n",
    "\n",
    "def return_prompt(row):\n",
    "    return {\"text\": create_prompt(row)}\n",
    "    \n",
    "if benchmark == \"MedQA\":\n",
    "    test_dataset = eval_dataset.map(return_prompt_no_answer)\n",
    "train_dataset_with_texts = train_dataset.map(return_prompt)\n",
    "print(train_dataset_with_texts[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be57c0",
   "metadata": {
    "papermill": {
     "duration": 0.002531,
     "end_time": "2024-03-09T18:53:49.240184",
     "exception": false,
     "start_time": "2024-03-09T18:53:49.237653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37d2f1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:53:49.245823Z",
     "iopub.status.busy": "2024-03-09T18:53:49.245744Z",
     "iopub.status.idle": "2024-03-09T18:54:04.267617Z",
     "shell.execute_reply": "2024-03-09T18:54:04.267265Z"
    },
    "papermill": {
     "duration": 15.026022,
     "end_time": "2024-03-09T18:54:04.268777",
     "exception": false,
     "start_time": "2024-03-09T18:53:49.242755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1/2 [00:04<00:04,  4.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  2.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from utilities.biollama import BioLlama\n",
    "import torch\n",
    "\n",
    "amended_questions = [\"The main calcium pump of the sarcoplasmic reticulum is \"]\n",
    "# answers = [\"Sarcoplasmic reticulum Ca(2+)-ATPase\"] # or \"SERCA\",\"serca2\"\n",
    "prompt = amended_questions[0]\n",
    "model_id = \"meta-llama/Llama-2-\" + size +\"b-chat-hf\"\n",
    "chunk_length = 32\n",
    "\n",
    "BioLlama = BioLlama(\n",
    "    model_id=model_id,\n",
    "    chunk_length=chunk_length,\n",
    "    RETRO_layer_ids=RETRO_layer_ids,\n",
    "    training=True,\n",
    "    torch_dtype=torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76e11ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:54:04.275702Z",
     "iopub.status.busy": "2024-03-09T18:54:04.275413Z",
     "iopub.status.idle": "2024-03-09T18:54:04.277352Z",
     "shell.execute_reply": "2024-03-09T18:54:04.277123Z"
    },
    "papermill": {
     "duration": 0.005888,
     "end_time": "2024-03-09T18:54:04.277862",
     "exception": false,
     "start_time": "2024-03-09T18:54:04.271974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BioLlama.model\n",
    "tokenizer = BioLlama.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:54:04.283761Z",
     "iopub.status.busy": "2024-03-09T18:54:04.283660Z",
     "iopub.status.idle": "2024-03-09T18:54:04.290000Z",
     "shell.execute_reply": "2024-03-09T18:54:04.289789Z"
    },
    "papermill": {
     "duration": 0.009961,
     "end_time": "2024-03-09T18:54:04.290516",
     "exception": false,
     "start_time": "2024-03-09T18:54:04.280555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freezing layers, currently only works for single unfrozen retro layer\n",
      "\n",
      "printing layer 15 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "cca_attn.q_proj.weight, requires_grad = False\n",
      "cca_attn.k_proj.weight, requires_grad = False\n",
      "cca_attn.v_proj.weight, requires_grad = False\n",
      "cca_attn.o_proj.weight, requires_grad = False\n",
      "pre_cca_layernorm.weight, requires_grad = False\n",
      "\n",
      "printing layer 15 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "cca_attn.q_proj.weight, requires_grad = True\n",
      "cca_attn.k_proj.weight, requires_grad = True\n",
      "cca_attn.v_proj.weight, requires_grad = True\n",
      "cca_attn.o_proj.weight, requires_grad = True\n",
      "pre_cca_layernorm.weight, requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "print(\"freezing layers, currently only works for single unfrozen retro layer\")\n",
    "n_freeze = BioLlama.RETRO_layer_ids[0]\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): \n",
    "    param.requires_grad = True\n",
    "#for every parameter in retro_layer_params, print where in the model it comes from (ie is it from self attention, layer norm, etc)\n",
    "print(f\"\\nprinting layer {n_freeze} params\")\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")   \n",
    "\n",
    "list_of_params_to_unfreeze = [\n",
    "    \"cca_attn.q_proj.weight\",\n",
    "    \"cca_attn.k_proj.weight\",\n",
    "    \"cca_attn.v_proj.weight\",\n",
    "    \"cca_attn.o_proj.weight\",\n",
    "    \"pre_cca_layernorm.weight\",\n",
    "]\n",
    "\n",
    "for name, param in model.model.layers[n_freeze].named_parameters(): \n",
    "    if name in list_of_params_to_unfreeze:\n",
    "        param.requires_grad = True\n",
    "print(f\"\\nprinting layer {n_freeze} params\")\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15a29820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:54:04.296463Z",
     "iopub.status.busy": "2024-03-09T18:54:04.296370Z",
     "iopub.status.idle": "2024-03-09T18:54:04.320722Z",
     "shell.execute_reply": "2024-03-09T18:54:04.320528Z"
    },
    "papermill": {
     "duration": 0.027915,
     "end_time": "2024-03-09T18:54:04.321232",
     "exception": false,
     "start_time": "2024-03-09T18:54:04.293317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight, requires_grad = False\n",
      "layers.0.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.0.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.0.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.0.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.0.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.0.mlp.up_proj.weight, requires_grad = False\n",
      "layers.0.mlp.down_proj.weight, requires_grad = False\n",
      "layers.0.input_layernorm.weight, requires_grad = False\n",
      "layers.0.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.1.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.1.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.1.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.1.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.1.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.1.mlp.up_proj.weight, requires_grad = False\n",
      "layers.1.mlp.down_proj.weight, requires_grad = False\n",
      "layers.1.input_layernorm.weight, requires_grad = False\n",
      "layers.1.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.2.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.2.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.2.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.2.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.2.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.2.mlp.up_proj.weight, requires_grad = False\n",
      "layers.2.mlp.down_proj.weight, requires_grad = False\n",
      "layers.2.input_layernorm.weight, requires_grad = False\n",
      "layers.2.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.3.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.3.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.3.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.3.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.3.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.3.mlp.up_proj.weight, requires_grad = False\n",
      "layers.3.mlp.down_proj.weight, requires_grad = False\n",
      "layers.3.input_layernorm.weight, requires_grad = False\n",
      "layers.3.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.4.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.4.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.4.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.4.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.4.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.4.mlp.up_proj.weight, requires_grad = False\n",
      "layers.4.mlp.down_proj.weight, requires_grad = False\n",
      "layers.4.input_layernorm.weight, requires_grad = False\n",
      "layers.4.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.5.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.5.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.5.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.5.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.5.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.5.mlp.up_proj.weight, requires_grad = False\n",
      "layers.5.mlp.down_proj.weight, requires_grad = False\n",
      "layers.5.input_layernorm.weight, requires_grad = False\n",
      "layers.5.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.6.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.6.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.6.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.6.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.6.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.6.mlp.up_proj.weight, requires_grad = False\n",
      "layers.6.mlp.down_proj.weight, requires_grad = False\n",
      "layers.6.input_layernorm.weight, requires_grad = False\n",
      "layers.6.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.7.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.7.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.7.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.7.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.7.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.7.mlp.up_proj.weight, requires_grad = False\n",
      "layers.7.mlp.down_proj.weight, requires_grad = False\n",
      "layers.7.input_layernorm.weight, requires_grad = False\n",
      "layers.7.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.8.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.8.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.8.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.8.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.8.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.8.mlp.up_proj.weight, requires_grad = False\n",
      "layers.8.mlp.down_proj.weight, requires_grad = False\n",
      "layers.8.input_layernorm.weight, requires_grad = False\n",
      "layers.8.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.9.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.9.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.9.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.9.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.9.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.9.mlp.up_proj.weight, requires_grad = False\n",
      "layers.9.mlp.down_proj.weight, requires_grad = False\n",
      "layers.9.input_layernorm.weight, requires_grad = False\n",
      "layers.9.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.10.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.10.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.10.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.10.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.10.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.10.mlp.up_proj.weight, requires_grad = False\n",
      "layers.10.mlp.down_proj.weight, requires_grad = False\n",
      "layers.10.input_layernorm.weight, requires_grad = False\n",
      "layers.10.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.11.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.11.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.11.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.11.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.11.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.11.mlp.up_proj.weight, requires_grad = False\n",
      "layers.11.mlp.down_proj.weight, requires_grad = False\n",
      "layers.11.input_layernorm.weight, requires_grad = False\n",
      "layers.11.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.12.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.12.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.12.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.12.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.12.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.12.mlp.up_proj.weight, requires_grad = False\n",
      "layers.12.mlp.down_proj.weight, requires_grad = False\n",
      "layers.12.input_layernorm.weight, requires_grad = False\n",
      "layers.12.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.13.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.13.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.13.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.13.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.13.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.13.mlp.up_proj.weight, requires_grad = False\n",
      "layers.13.mlp.down_proj.weight, requires_grad = False\n",
      "layers.13.input_layernorm.weight, requires_grad = False\n",
      "layers.13.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.14.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.14.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.14.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.14.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.14.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.14.mlp.up_proj.weight, requires_grad = False\n",
      "layers.14.mlp.down_proj.weight, requires_grad = False\n",
      "layers.14.input_layernorm.weight, requires_grad = False\n",
      "layers.14.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.15.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.15.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.15.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.15.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.15.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.15.mlp.up_proj.weight, requires_grad = False\n",
      "layers.15.mlp.down_proj.weight, requires_grad = False\n",
      "layers.15.input_layernorm.weight, requires_grad = False\n",
      "layers.15.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.15.cca_attn.q_proj.weight, requires_grad = True\n",
      "layers.15.cca_attn.k_proj.weight, requires_grad = True\n",
      "layers.15.cca_attn.v_proj.weight, requires_grad = True\n",
      "layers.15.cca_attn.o_proj.weight, requires_grad = True\n",
      "layers.15.pre_cca_layernorm.weight, requires_grad = True\n",
      "layers.16.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.16.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.16.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.16.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.16.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.16.mlp.up_proj.weight, requires_grad = False\n",
      "layers.16.mlp.down_proj.weight, requires_grad = False\n",
      "layers.16.input_layernorm.weight, requires_grad = False\n",
      "layers.16.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.17.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.17.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.17.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.17.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.17.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.17.mlp.up_proj.weight, requires_grad = False\n",
      "layers.17.mlp.down_proj.weight, requires_grad = False\n",
      "layers.17.input_layernorm.weight, requires_grad = False\n",
      "layers.17.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.18.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.18.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.18.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.18.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.18.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.18.mlp.up_proj.weight, requires_grad = False\n",
      "layers.18.mlp.down_proj.weight, requires_grad = False\n",
      "layers.18.input_layernorm.weight, requires_grad = False\n",
      "layers.18.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.19.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.19.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.19.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.19.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.19.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.19.mlp.up_proj.weight, requires_grad = False\n",
      "layers.19.mlp.down_proj.weight, requires_grad = False\n",
      "layers.19.input_layernorm.weight, requires_grad = False\n",
      "layers.19.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.20.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.20.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.20.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.20.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.20.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.20.mlp.up_proj.weight, requires_grad = False\n",
      "layers.20.mlp.down_proj.weight, requires_grad = False\n",
      "layers.20.input_layernorm.weight, requires_grad = False\n",
      "layers.20.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.21.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.21.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.21.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.21.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.21.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.21.mlp.up_proj.weight, requires_grad = False\n",
      "layers.21.mlp.down_proj.weight, requires_grad = False\n",
      "layers.21.input_layernorm.weight, requires_grad = False\n",
      "layers.21.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.22.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.22.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.22.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.22.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.22.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.22.mlp.up_proj.weight, requires_grad = False\n",
      "layers.22.mlp.down_proj.weight, requires_grad = False\n",
      "layers.22.input_layernorm.weight, requires_grad = False\n",
      "layers.22.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.23.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.23.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.23.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.23.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.23.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.23.mlp.up_proj.weight, requires_grad = False\n",
      "layers.23.mlp.down_proj.weight, requires_grad = False\n",
      "layers.23.input_layernorm.weight, requires_grad = False\n",
      "layers.23.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.24.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.24.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.24.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.24.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.24.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.24.mlp.up_proj.weight, requires_grad = False\n",
      "layers.24.mlp.down_proj.weight, requires_grad = False\n",
      "layers.24.input_layernorm.weight, requires_grad = False\n",
      "layers.24.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.25.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.25.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.25.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.25.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.25.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.25.mlp.up_proj.weight, requires_grad = False\n",
      "layers.25.mlp.down_proj.weight, requires_grad = False\n",
      "layers.25.input_layernorm.weight, requires_grad = False\n",
      "layers.25.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.26.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.26.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.26.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.26.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.26.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.26.mlp.up_proj.weight, requires_grad = False\n",
      "layers.26.mlp.down_proj.weight, requires_grad = False\n",
      "layers.26.input_layernorm.weight, requires_grad = False\n",
      "layers.26.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.27.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.27.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.27.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.27.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.27.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.27.mlp.up_proj.weight, requires_grad = False\n",
      "layers.27.mlp.down_proj.weight, requires_grad = False\n",
      "layers.27.input_layernorm.weight, requires_grad = False\n",
      "layers.27.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.28.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.28.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.28.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.28.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.28.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.28.mlp.up_proj.weight, requires_grad = False\n",
      "layers.28.mlp.down_proj.weight, requires_grad = False\n",
      "layers.28.input_layernorm.weight, requires_grad = False\n",
      "layers.28.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.29.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.29.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.29.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.29.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.29.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.29.mlp.up_proj.weight, requires_grad = False\n",
      "layers.29.mlp.down_proj.weight, requires_grad = False\n",
      "layers.29.input_layernorm.weight, requires_grad = False\n",
      "layers.29.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.30.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.30.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.30.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.30.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.30.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.30.mlp.up_proj.weight, requires_grad = False\n",
      "layers.30.mlp.down_proj.weight, requires_grad = False\n",
      "layers.30.input_layernorm.weight, requires_grad = False\n",
      "layers.30.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.31.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.31.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.31.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.31.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.31.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.31.mlp.up_proj.weight, requires_grad = False\n",
      "layers.31.mlp.down_proj.weight, requires_grad = False\n",
      "layers.31.input_layernorm.weight, requires_grad = False\n",
      "layers.31.post_attention_layernorm.weight, requires_grad = False\n",
      "norm.weight, requires_grad = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-14): 15 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "        (cca_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (pre_cca_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (16-31): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, param in model.model.named_parameters(): \n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")\n",
    "    # param.requires_grad = True\n",
    "\n",
    "BioLlama.model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:54:04.327937Z",
     "iopub.status.busy": "2024-03-09T18:54:04.327844Z",
     "iopub.status.idle": "2024-03-09T18:54:04.329438Z",
     "shell.execute_reply": "2024-03-09T18:54:04.329234Z"
    },
    "papermill": {
     "duration": 0.00565,
     "end_time": "2024-03-09T18:54:04.329913",
     "exception": false,
     "start_time": "2024-03-09T18:54:04.324263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "model.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:54:04.336656Z",
     "iopub.status.busy": "2024-03-09T18:54:04.336562Z",
     "iopub.status.idle": "2024-03-09T18:54:04.339737Z",
     "shell.execute_reply": "2024-03-09T18:54:04.339527Z"
    },
    "papermill": {
     "duration": 0.007165,
     "end_time": "2024-03-09T18:54:04.340165",
     "exception": false,
     "start_time": "2024-03-09T18:54:04.333000",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6805.53M, Trainable: 198.18M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57893e-6e42-4cda-83c8-1e51a86d2da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:17.982179Z",
     "iopub.status.busy": "2024-01-17T00:21:17.981899Z",
     "iopub.status.idle": "2024-01-17T00:21:17.984608Z",
     "shell.execute_reply": "2024-01-17T00:21:17.984325Z"
    },
    "papermill": {
     "duration": 0.002998,
     "end_time": "2024-03-09T18:54:04.346241",
     "exception": false,
     "start_time": "2024-03-09T18:54:04.343243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c86339ca-27e5-496b-9559-3a65bb3c26a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:54:04.353183Z",
     "iopub.status.busy": "2024-03-09T18:54:04.353072Z",
     "iopub.status.idle": "2024-03-09T18:54:04.360624Z",
     "shell.execute_reply": "2024-03-09T18:54:04.360330Z"
    },
    "papermill": {
     "duration": 0.011549,
     "end_time": "2024-03-09T18:54:04.361083",
     "exception": false,
     "start_time": "2024-03-09T18:54:04.349534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\" + size  + \"/\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size//2,\n",
    "    bf16=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=total_num_steps // 10,\n",
    "    num_train_epochs=2,\n",
    "    max_steps=total_num_steps,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=5000,\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=8,\n",
    "    save_strategy=\"epoch\", #changed to epoch so we save every epoch i guess?\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e35a9b70-f36b-4bfd-857f-c80d3450e111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:54:04.368003Z",
     "iopub.status.busy": "2024-03-09T18:54:04.367903Z",
     "iopub.status.idle": "2024-03-09T18:54:04.457924Z",
     "shell.execute_reply": "2024-03-09T18:54:04.457621Z"
    },
    "papermill": {
     "duration": 0.094301,
     "end_time": "2024-03-09T18:54:04.458532",
     "exception": false,
     "start_time": "2024-03-09T18:54:04.364231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "import trl\n",
    "# from utilities.finetuning.sft_trainer import SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset_with_texts,\n",
    "    dataset_text_field=\"text\",\n",
    "    # eval_dataset=test_dataset,\n",
    "    packing=True,\n",
    "    max_seq_length=1024,\n",
    "    args=training_args,\n",
    "    formatting_func=create_prompt,\n",
    "    # compute_metrics=token_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5754789-2e15-4bc9-800c-01f8ffc625e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T18:54:04.467492Z",
     "iopub.status.busy": "2024-03-09T18:54:04.467395Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-03-09T18:54:04.461842",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4054' max='200000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4054/200000 21:28 < 17:18:14, 3.15 it/s, Epoch 0.04/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.990200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.006800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.981100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.974400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.961100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.947600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.972900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.993500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.041900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.998300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.950100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.957400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.971500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.934800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.908200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.932700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.932300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.854100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.883900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.894400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.875600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.862400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.884700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.847400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.774900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.832300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.861800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.805300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.765900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.803300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.793600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>0.752100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>0.786800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.764500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>0.716200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.722200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>0.725400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.717800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>0.709100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>0.699600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.696200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.720600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>0.688700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>0.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>0.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.652300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>0.649800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>0.694300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.660500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.710100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>0.620400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>0.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.635800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>0.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0.618900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.620700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>0.590900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>0.616900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.643800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>0.619300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>0.591600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>0.566900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>0.618500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.633300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>0.574700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>0.568800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>0.609700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>0.610600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.591200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>0.603500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>0.615400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>0.585600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>0.594500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.595300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>0.615700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>0.602700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>0.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>0.591800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.576700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>0.545200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>0.564600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>0.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>0.587100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.589800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>0.556600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>0.551500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>0.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>0.609100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.557100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>848</td>\n",
       "      <td>0.548900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>0.543800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>0.534100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>0.581300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.560800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>0.599700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>0.537400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>904</td>\n",
       "      <td>0.556400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912</td>\n",
       "      <td>0.590200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.549900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>0.512100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>0.523300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>0.500200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>0.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.536500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>968</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>976</td>\n",
       "      <td>0.509100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>984</td>\n",
       "      <td>0.526800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>992</td>\n",
       "      <td>0.541700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>0.549800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1016</td>\n",
       "      <td>0.566400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>0.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1032</td>\n",
       "      <td>0.575500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.548600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048</td>\n",
       "      <td>0.568500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1056</td>\n",
       "      <td>0.554900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1064</td>\n",
       "      <td>0.563800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072</td>\n",
       "      <td>0.553600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.498500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1088</td>\n",
       "      <td>0.561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1096</td>\n",
       "      <td>0.513400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1104</td>\n",
       "      <td>0.513200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1112</td>\n",
       "      <td>0.504100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.502400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1128</td>\n",
       "      <td>0.540100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1136</td>\n",
       "      <td>0.557400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1144</td>\n",
       "      <td>0.578800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1152</td>\n",
       "      <td>0.556300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.531500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1168</td>\n",
       "      <td>0.541400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1176</td>\n",
       "      <td>0.541900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1184</td>\n",
       "      <td>0.523000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1192</td>\n",
       "      <td>0.523500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1208</td>\n",
       "      <td>0.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1216</td>\n",
       "      <td>0.541900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1224</td>\n",
       "      <td>0.510500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1232</td>\n",
       "      <td>0.517300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.482300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1248</td>\n",
       "      <td>0.520300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1256</td>\n",
       "      <td>0.497100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1264</td>\n",
       "      <td>0.510300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1272</td>\n",
       "      <td>0.537800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.546100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1288</td>\n",
       "      <td>0.479600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1296</td>\n",
       "      <td>0.563900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1304</td>\n",
       "      <td>0.539700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1312</td>\n",
       "      <td>0.485200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.526200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1328</td>\n",
       "      <td>0.581700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1336</td>\n",
       "      <td>0.536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1344</td>\n",
       "      <td>0.557500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1352</td>\n",
       "      <td>0.510300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.494700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1368</td>\n",
       "      <td>0.536800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1376</td>\n",
       "      <td>0.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1384</td>\n",
       "      <td>0.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1392</td>\n",
       "      <td>0.482900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1408</td>\n",
       "      <td>0.525900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1416</td>\n",
       "      <td>0.503700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1424</td>\n",
       "      <td>0.504800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1432</td>\n",
       "      <td>0.538800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.497800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1448</td>\n",
       "      <td>0.478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1456</td>\n",
       "      <td>0.527800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1464</td>\n",
       "      <td>0.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1472</td>\n",
       "      <td>0.475500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.493000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1488</td>\n",
       "      <td>0.463200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1496</td>\n",
       "      <td>0.516300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1504</td>\n",
       "      <td>0.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1512</td>\n",
       "      <td>0.479500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1528</td>\n",
       "      <td>0.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1536</td>\n",
       "      <td>0.517900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1544</td>\n",
       "      <td>0.451800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1552</td>\n",
       "      <td>0.491800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.519500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1568</td>\n",
       "      <td>0.480400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1576</td>\n",
       "      <td>0.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1584</td>\n",
       "      <td>0.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1592</td>\n",
       "      <td>0.490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.466300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1608</td>\n",
       "      <td>0.478600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1616</td>\n",
       "      <td>0.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1624</td>\n",
       "      <td>0.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1632</td>\n",
       "      <td>0.497900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.494600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1648</td>\n",
       "      <td>0.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1656</td>\n",
       "      <td>0.478800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1664</td>\n",
       "      <td>0.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1672</td>\n",
       "      <td>0.492500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.478200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1688</td>\n",
       "      <td>0.530400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1696</td>\n",
       "      <td>0.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1704</td>\n",
       "      <td>0.491300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1712</td>\n",
       "      <td>0.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.482300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1728</td>\n",
       "      <td>0.521000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1736</td>\n",
       "      <td>0.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1744</td>\n",
       "      <td>0.464200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1752</td>\n",
       "      <td>0.486400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.479200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1768</td>\n",
       "      <td>0.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1776</td>\n",
       "      <td>0.510300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1784</td>\n",
       "      <td>0.542800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1792</td>\n",
       "      <td>0.532800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.583200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1808</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1816</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1824</td>\n",
       "      <td>0.468100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1832</td>\n",
       "      <td>0.504200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.473200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1848</td>\n",
       "      <td>0.477400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1856</td>\n",
       "      <td>0.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>0.529100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1872</td>\n",
       "      <td>0.488500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.451800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1888</td>\n",
       "      <td>0.486300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1896</td>\n",
       "      <td>0.497300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1904</td>\n",
       "      <td>0.540400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1912</td>\n",
       "      <td>0.497700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.501300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1928</td>\n",
       "      <td>0.515600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1936</td>\n",
       "      <td>0.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1944</td>\n",
       "      <td>0.469300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1952</td>\n",
       "      <td>0.492700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.474600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1968</td>\n",
       "      <td>0.474800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1976</td>\n",
       "      <td>0.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1984</td>\n",
       "      <td>0.483600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1992</td>\n",
       "      <td>0.472200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.487400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2008</td>\n",
       "      <td>0.458700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016</td>\n",
       "      <td>0.487900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2024</td>\n",
       "      <td>0.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2032</td>\n",
       "      <td>0.471500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.509300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2048</td>\n",
       "      <td>0.492500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2056</td>\n",
       "      <td>0.483600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2064</td>\n",
       "      <td>0.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2072</td>\n",
       "      <td>0.486000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2088</td>\n",
       "      <td>0.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2096</td>\n",
       "      <td>0.501600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2104</td>\n",
       "      <td>0.485100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2112</td>\n",
       "      <td>0.542400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.497600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2128</td>\n",
       "      <td>0.485400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2136</td>\n",
       "      <td>0.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2144</td>\n",
       "      <td>0.479900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2152</td>\n",
       "      <td>0.455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.471800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2168</td>\n",
       "      <td>0.494600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2176</td>\n",
       "      <td>0.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2184</td>\n",
       "      <td>0.461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2192</td>\n",
       "      <td>0.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.472400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2208</td>\n",
       "      <td>0.465300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2216</td>\n",
       "      <td>0.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2224</td>\n",
       "      <td>0.427800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2232</td>\n",
       "      <td>0.456200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.458300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2248</td>\n",
       "      <td>0.493600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2256</td>\n",
       "      <td>0.463600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2264</td>\n",
       "      <td>0.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2272</td>\n",
       "      <td>0.444300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.486700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2288</td>\n",
       "      <td>0.467900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2296</td>\n",
       "      <td>0.405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2304</td>\n",
       "      <td>0.460600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2312</td>\n",
       "      <td>0.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.473200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2328</td>\n",
       "      <td>0.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2336</td>\n",
       "      <td>0.447400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2344</td>\n",
       "      <td>0.455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2352</td>\n",
       "      <td>0.495700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.506200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2368</td>\n",
       "      <td>0.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2376</td>\n",
       "      <td>0.505200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2384</td>\n",
       "      <td>0.455700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2392</td>\n",
       "      <td>0.459800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.494500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2408</td>\n",
       "      <td>0.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2416</td>\n",
       "      <td>0.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2424</td>\n",
       "      <td>0.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2432</td>\n",
       "      <td>0.469700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.450200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2448</td>\n",
       "      <td>0.464800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2456</td>\n",
       "      <td>0.485100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2464</td>\n",
       "      <td>0.465800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2472</td>\n",
       "      <td>0.447500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2488</td>\n",
       "      <td>0.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2496</td>\n",
       "      <td>0.467400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2504</td>\n",
       "      <td>0.439800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2512</td>\n",
       "      <td>0.430300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.458100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2528</td>\n",
       "      <td>0.455600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2536</td>\n",
       "      <td>0.500900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2544</td>\n",
       "      <td>0.466700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2552</td>\n",
       "      <td>0.469700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2568</td>\n",
       "      <td>0.430400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2576</td>\n",
       "      <td>0.440700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2584</td>\n",
       "      <td>0.469500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2592</td>\n",
       "      <td>0.460100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.450100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2608</td>\n",
       "      <td>0.443700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2616</td>\n",
       "      <td>0.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2624</td>\n",
       "      <td>0.442800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2632</td>\n",
       "      <td>0.462300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.471300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2648</td>\n",
       "      <td>0.498000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2656</td>\n",
       "      <td>0.481300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2664</td>\n",
       "      <td>0.452500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2672</td>\n",
       "      <td>0.485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2688</td>\n",
       "      <td>0.435800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2696</td>\n",
       "      <td>0.432800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2704</td>\n",
       "      <td>0.459500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2712</td>\n",
       "      <td>0.464300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2728</td>\n",
       "      <td>0.440800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2736</td>\n",
       "      <td>0.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2744</td>\n",
       "      <td>0.439600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2752</td>\n",
       "      <td>0.483100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.444600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2768</td>\n",
       "      <td>0.443300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2776</td>\n",
       "      <td>0.452100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2784</td>\n",
       "      <td>0.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2792</td>\n",
       "      <td>0.456900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.417400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2808</td>\n",
       "      <td>0.505600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2816</td>\n",
       "      <td>0.441400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2824</td>\n",
       "      <td>0.465200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2832</td>\n",
       "      <td>0.395500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.466500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2848</td>\n",
       "      <td>0.450400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2856</td>\n",
       "      <td>0.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2864</td>\n",
       "      <td>0.497900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2872</td>\n",
       "      <td>0.431700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.481300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2888</td>\n",
       "      <td>0.436900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2896</td>\n",
       "      <td>0.465300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2904</td>\n",
       "      <td>0.447900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2912</td>\n",
       "      <td>0.454500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.434700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2928</td>\n",
       "      <td>0.492500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2936</td>\n",
       "      <td>0.439700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2944</td>\n",
       "      <td>0.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2952</td>\n",
       "      <td>0.411900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.440600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2968</td>\n",
       "      <td>0.435900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2976</td>\n",
       "      <td>0.498600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2984</td>\n",
       "      <td>0.434400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2992</td>\n",
       "      <td>0.428700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.441400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3008</td>\n",
       "      <td>0.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3016</td>\n",
       "      <td>0.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3024</td>\n",
       "      <td>0.463900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3032</td>\n",
       "      <td>0.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.456600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3048</td>\n",
       "      <td>0.433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3056</td>\n",
       "      <td>0.458900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3064</td>\n",
       "      <td>0.458500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3072</td>\n",
       "      <td>0.434900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3088</td>\n",
       "      <td>0.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3096</td>\n",
       "      <td>0.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3104</td>\n",
       "      <td>0.420100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3112</td>\n",
       "      <td>0.432200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3128</td>\n",
       "      <td>0.446100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3136</td>\n",
       "      <td>0.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3144</td>\n",
       "      <td>0.429200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3152</td>\n",
       "      <td>0.444600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>0.449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3168</td>\n",
       "      <td>0.400800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3176</td>\n",
       "      <td>0.492500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3184</td>\n",
       "      <td>0.466800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3192</td>\n",
       "      <td>0.441600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.453300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3208</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3216</td>\n",
       "      <td>0.421900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3224</td>\n",
       "      <td>0.443600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3232</td>\n",
       "      <td>0.489500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.425100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3248</td>\n",
       "      <td>0.505500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3256</td>\n",
       "      <td>0.461300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3264</td>\n",
       "      <td>0.409900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3272</td>\n",
       "      <td>0.426100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.410700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3288</td>\n",
       "      <td>0.441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3296</td>\n",
       "      <td>0.433000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3304</td>\n",
       "      <td>0.438900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3312</td>\n",
       "      <td>0.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>0.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3328</td>\n",
       "      <td>0.375700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3336</td>\n",
       "      <td>0.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3344</td>\n",
       "      <td>0.424400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3352</td>\n",
       "      <td>0.449300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>0.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3368</td>\n",
       "      <td>0.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3376</td>\n",
       "      <td>0.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3384</td>\n",
       "      <td>0.437600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3392</td>\n",
       "      <td>0.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.456500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3408</td>\n",
       "      <td>0.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3416</td>\n",
       "      <td>0.447500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3424</td>\n",
       "      <td>0.488800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3432</td>\n",
       "      <td>0.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>0.452700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3448</td>\n",
       "      <td>0.401900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3456</td>\n",
       "      <td>0.502800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3464</td>\n",
       "      <td>0.411700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3472</td>\n",
       "      <td>0.453400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>0.463200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3488</td>\n",
       "      <td>0.417700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3496</td>\n",
       "      <td>0.471300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3504</td>\n",
       "      <td>0.451300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3512</td>\n",
       "      <td>0.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>0.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3528</td>\n",
       "      <td>0.451300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3536</td>\n",
       "      <td>0.444200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3544</td>\n",
       "      <td>0.467400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3552</td>\n",
       "      <td>0.403900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>0.481400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3568</td>\n",
       "      <td>0.481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3576</td>\n",
       "      <td>0.431100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3584</td>\n",
       "      <td>0.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3592</td>\n",
       "      <td>0.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3608</td>\n",
       "      <td>0.453800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3616</td>\n",
       "      <td>0.430900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3624</td>\n",
       "      <td>0.477900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3632</td>\n",
       "      <td>0.448700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>0.425800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3648</td>\n",
       "      <td>0.448200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3656</td>\n",
       "      <td>0.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3664</td>\n",
       "      <td>0.441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3672</td>\n",
       "      <td>0.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>0.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3688</td>\n",
       "      <td>0.414700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3696</td>\n",
       "      <td>0.430700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3704</td>\n",
       "      <td>0.418400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3712</td>\n",
       "      <td>0.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>0.424500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3728</td>\n",
       "      <td>0.432700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3736</td>\n",
       "      <td>0.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3744</td>\n",
       "      <td>0.414700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3752</td>\n",
       "      <td>0.414100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>0.422600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3768</td>\n",
       "      <td>0.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3776</td>\n",
       "      <td>0.448300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3784</td>\n",
       "      <td>0.456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3792</td>\n",
       "      <td>0.426500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.466200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3808</td>\n",
       "      <td>0.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3816</td>\n",
       "      <td>0.394300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3824</td>\n",
       "      <td>0.455900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3832</td>\n",
       "      <td>0.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>0.459100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3848</td>\n",
       "      <td>0.453900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3856</td>\n",
       "      <td>0.459600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3864</td>\n",
       "      <td>0.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3872</td>\n",
       "      <td>0.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>0.479600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3888</td>\n",
       "      <td>0.423900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3896</td>\n",
       "      <td>0.450100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3904</td>\n",
       "      <td>0.453400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3912</td>\n",
       "      <td>0.440600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>0.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3928</td>\n",
       "      <td>0.449300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3936</td>\n",
       "      <td>0.455700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3944</td>\n",
       "      <td>0.430300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3952</td>\n",
       "      <td>0.439800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>0.426100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3968</td>\n",
       "      <td>0.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3976</td>\n",
       "      <td>0.450300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3984</td>\n",
       "      <td>0.421400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3992</td>\n",
       "      <td>0.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.461600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4008</td>\n",
       "      <td>0.426200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4016</td>\n",
       "      <td>0.403800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4024</td>\n",
       "      <td>0.423200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4032</td>\n",
       "      <td>0.431900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>0.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4048</td>\n",
       "      <td>0.461600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#very hacky but maybe this will work:\n",
    "tokenizer.model_input_names = ['labels', 'input_ids', 'attention_mask']\n",
    "# trainer.args.train_batch_size = 1\n",
    "# self.args.train_batch_size\n",
    "\n",
    "#also hacky, but could work:\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Starting training\")\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca3011",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e06dfb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(size)\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\" + benchmark + \"/\" + size  + \"/\"\n",
    "print(RETRO_layer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ca6fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:52:49.215531Z",
     "iopub.status.busy": "2024-01-17T00:52:49.215430Z",
     "iopub.status.idle": "2024-01-17T00:53:05.370183Z",
     "shell.execute_reply": "2024-01-17T00:53:05.369679Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.abspath(output_dir))\n",
    "trainer.save_model(output_dir)\n",
    "# !ls -l $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff2529",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load this local model here and use it to generate some text\n",
    "print(output_dir)\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import time\n",
    "# import torch\n",
    "# from utilities.biollama import BioLlama\n",
    "\n",
    "# chunk_length = 32\n",
    "\n",
    "# BioLlama = BioLlama(model_id=output_dir, \n",
    "#     chunk_length=chunk_length, \n",
    "#     RETRO_layer_ids = RETRO_layer_ids, \n",
    "#     training=False, \n",
    "#     torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f493d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BioLlama.training = False\n",
    "import time\n",
    "prompt  = '<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \\n (A) Ampicillin\\n (B) Ceftriaxone\\n (C) Ciprofloxacin\\n (D) Doxycycline\\n (E) Nitrofurantoin</QUESTION>\\n<ANSWER> '\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99856974",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt2 = '<QUESTION>A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby? \\n (A) Placing the infant in a supine position on a firm mattress while sleeping\\n (B) Routine postnatal electrocardiogram (ECG)\\n (C) Keeping the infant covered and maintaining a high room temperature\\n (D) Application of a device to maintain the sleeping position\\n (E) Avoiding pacifier use during sleep</QUESTION>\\n<ANSWER> '\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt2, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad4c40",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt3 = \"<QUESTION>A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation? \\n (A) Abnormal migration of ventral pancreatic bud\\n (B) Complete failure of proximal duodenum to recanalize\\n (C) Error in neural crest cell migration\\n (D) Abnormal hypertrophy of the pylorus\\n (E) Failure of lateral body folds to move ventrally and fuse in the midline</QUESTION>\\n<ANSWER> \"\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt3, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24725d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt4 = \"<QUESTION>A 20-year-old woman presents with menorrhagia for the past several years. She says that her menses ‚Äúhave always been heavy‚Äù, and she has experienced easy bruising for as long as she can remember. Family history is significant for her mother, who had similar problems with bruising easily. The patient's vital signs include: heart rate 98/min, respiratory rate 14/min, temperature 36.1¬∞C (96.9¬∞F), and blood pressure 110/87 mm Hg. Physical examination is unremarkable. Laboratory tests show the following: platelet count 200,000/mm3, PT 12 seconds, and PTT 43 seconds. Which of the following is the most likely cause of this patient‚Äôs symptoms? \\n (A) Factor V Leiden\\n (B) Hemophilia A\\n (C) Lupus anticoagulant\\n (D) Protein C deficiency\\n (E) Von Willebrand disease</QUESTION>\\n<ANSWER> \"\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt4, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883afe6b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1afa7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "BioLlama_finetuning_WandB_HF.ipynb",
   "output_path": "output_biollama.ipynb",
   "parameters": {},
   "start_time": "2024-03-09T18:53:11.606097",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}