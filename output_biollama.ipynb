{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb384650",
   "metadata": {
    "papermill": {
     "duration": 0.002594,
     "end_time": "2024-02-16T17:49:27.335501",
     "exception": false,
     "start_time": "2024-02-16T17:49:27.332907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is taken directly from https://github.com/tcapelle/llm_recipes/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
   "metadata": {
    "papermill": {
     "duration": 0.002491,
     "end_time": "2024-02-16T17:49:27.346893",
     "exception": false,
     "start_time": "2024-02-16T17:49:27.344402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# From Llama to Alpaca: Finetunning and LLM with Weights & Biases\n",
    "In this notebooks you will learn how to finetune a pretrained LLama model on an Instruction dataset. We will use an updated version of the Alpaca dataset that, instead of davinci-003 (GPT3) generations uses GPT4 to get an even better instruction dataset! More details on the [official repo page](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data)\n",
    "\n",
    "> This notebook requires a A100/A10 GPU with at least 24GB of memory. You could tweak the params down and run on a T4 but it would take very long time\n",
    "\n",
    "This notebooks has a companion project and [report](wandb.me/alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7744abe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:27.351832Z",
     "iopub.status.busy": "2024-02-16T17:49:27.351481Z",
     "iopub.status.idle": "2024-02-16T17:49:27.353617Z",
     "shell.execute_reply": "2024-02-16T17:49:27.353317Z"
    },
    "papermill": {
     "duration": 0.005352,
     "end_time": "2024-02-16T17:49:27.354181",
     "exception": false,
     "start_time": "2024-02-16T17:49:27.348829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install wandb transformers trl datasets \"protobuf==3.20.3\" evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693a36b-5cdc-4582-844b-8dd52fa522c5",
   "metadata": {
    "papermill": {
     "duration": 0.001935,
     "end_time": "2024-02-16T17:49:27.358055",
     "exception": false,
     "start_time": "2024-02-16T17:49:27.356120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## With Huggingface TRL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04c0a5-f481-4364-880d-10c254388987",
   "metadata": {
    "papermill": {
     "duration": 0.001927,
     "end_time": "2024-02-16T17:49:27.361908",
     "exception": false,
     "start_time": "2024-02-16T17:49:27.359981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's grab the Alpaca (GPT-4 curated instructions and outputs) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:27.366545Z",
     "iopub.status.busy": "2024-02-16T17:49:27.366243Z",
     "iopub.status.idle": "2024-02-16T17:49:27.381558Z",
     "shell.execute_reply": "2024-02-16T17:49:27.381286Z"
    },
    "papermill": {
     "duration": 0.01827,
     "end_time": "2024-02-16T17:49:27.382099",
     "exception": false,
     "start_time": "2024-02-16T17:49:27.363829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Benchmark from MedQA-USMLE/US/test.jsonl\n",
      "Benchmark contains 1273 questions, made up of 1273 with 5 options and 0 with non-5 options\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json\n",
    "# from uparse_benchmark import parse_benchmark\n",
    "# from ..utilities.parse_benchmark import parse_benchmark\n",
    "from utilities.parse_benchmark import parse_benchmark\n",
    "\n",
    "benchmark = \"MedQA\"\n",
    "benchmark_questions, benchmark_answers = parse_benchmark(benchmark)\n",
    "# print(benchmark_questions[0])\n",
    "# print(benchmark_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a154f968-da0f-4bdc-bf45-e499d95f0606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:27.386785Z",
     "iopub.status.busy": "2024-02-16T17:49:27.386518Z",
     "iopub.status.idle": "2024-02-16T17:49:32.176934Z",
     "shell.execute_reply": "2024-02-16T17:49:32.176653Z"
    },
    "papermill": {
     "duration": 4.793474,
     "end_time": "2024-02-16T17:49:32.177542",
     "exception": false,
     "start_time": "2024-02-16T17:49:27.384068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnelectric\u001b[0m (\u001b[33mneelectric\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/service/BioLlama/wandb/run-20240216_174928-nrrfuzgz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msparkling-snake-127\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft/runs/nrrfuzgz\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/neelectric/biollama_ft/runs/nrrfuzgz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fdad70e9e10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"biollama_ft\", # the project I am working on\n",
    "           tags=[\"hf_sft\", \"BioLlama\"]) # the Hyperparameters I want to keep track of\n",
    "# artifact = wandb.use_artifact('Neelectric/MedQA-USMLE', type='dataset')\n",
    "# artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811291cc-b7ce-422a-8971-3cbf9fe10a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:32.189421Z",
     "iopub.status.busy": "2024-02-16T17:49:32.189114Z",
     "iopub.status.idle": "2024-02-16T17:49:32.978730Z",
     "shell.execute_reply": "2024-02-16T17:49:32.978445Z"
    },
    "papermill": {
     "duration": 0.79967,
     "end_time": "2024-02-16T17:49:32.979704",
     "exception": false,
     "start_time": "2024-02-16T17:49:32.180034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# print(artifact_dir)\n",
    "artifact_dir = os.getcwd() + \"/benchmarks/MedQA-USMLE/\"\n",
    "from datasets import load_dataset\n",
    "#dataset = load_dataset(\"Neelectric/MedQA-USMLE\")\n",
    "medqa = load_dataset(\"json\", data_dir=artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24cf6197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:32.995098Z",
     "iopub.status.busy": "2024-02-16T17:49:32.994381Z",
     "iopub.status.idle": "2024-02-16T17:49:32.996967Z",
     "shell.execute_reply": "2024-02-16T17:49:32.996719Z"
    },
    "papermill": {
     "duration": 0.013481,
     "end_time": "2024-02-16T17:49:32.997647",
     "exception": false,
     "start_time": "2024-02-16T17:49:32.984166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trying gsutil for SciFive pretraining corpus\n",
    "# !pip install gsutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# abs_1_16 = pd.read_csv(\"abs_1_16.tsv\", sep='\\t')\n",
    "# abs_1_30 = pd.read_csv(\"abs_1_30.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f4c49e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:33.006265Z",
     "iopub.status.busy": "2024-02-16T17:49:33.006086Z",
     "iopub.status.idle": "2024-02-16T17:49:33.007591Z",
     "shell.execute_reply": "2024-02-16T17:49:33.007361Z"
    },
    "papermill": {
     "duration": 0.006589,
     "end_time": "2024-02-16T17:49:33.008236",
     "exception": false,
     "start_time": "2024-02-16T17:49:33.001647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# abs_1_16 = abs_1_16.dropna()\n",
    "# count_nans = abs_1_16.iloc[:, 0].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb96e0-b2f2-4a79-ba65-e1c8d6395d54",
   "metadata": {
    "papermill": {
     "duration": 0.003243,
     "end_time": "2024-02-16T17:49:33.015558",
     "exception": false,
     "start_time": "2024-02-16T17:49:33.012315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's log the dataset also as a table so we can inspect it on the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c7dda87-d70a-470b-a0f9-040af434dc42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:33.020585Z",
     "iopub.status.busy": "2024-02-16T17:49:33.020502Z",
     "iopub.status.idle": "2024-02-16T17:49:33.022846Z",
     "shell.execute_reply": "2024-02-16T17:49:33.022666Z"
    },
    "papermill": {
     "duration": 0.005426,
     "end_time": "2024-02-16T17:49:33.023289",
     "exception": false,
     "start_time": "2024-02-16T17:49:33.017863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10178\n",
      "1272\n"
     ]
    }
   ],
   "source": [
    "train_dataset = medqa[\"train\"]\n",
    "eval_dataset = medqa[\"validation\"]\n",
    "#print sizes\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "# turn both of these into only half their size\n",
    "# train_dataset = train_dataset.select(range(0, len(train_dataset)//2))\n",
    "# eval_dataset = eval_dataset.select(range(0, len(eval_dataset)//2))\n",
    "\n",
    "# print(len(train_dataset))\n",
    "# print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a038eb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:33.028287Z",
     "iopub.status.busy": "2024-02-16T17:49:33.028149Z",
     "iopub.status.idle": "2024-02-16T17:49:33.030631Z",
     "shell.execute_reply": "2024-02-16T17:49:33.030464Z"
    },
    "papermill": {
     "duration": 0.005507,
     "end_time": "2024-02-16T17:49:33.031096",
     "exception": false,
     "start_time": "2024-02-16T17:49:33.025589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<QUESTION>A 20-year-old woman presents with menorrhagia for the past several years. She says that her menses ‚Äúhave always been heavy‚Äù, and she has experienced easy bruising for as long as she can remember. Family history is significant for her mother, who had similar problems with bruising easily. The patient's vital signs include: heart rate 98/min, respiratory rate 14/min, temperature 36.1¬∞C (96.9¬∞F), and blood pressure 110/87 mm Hg. Physical examination is unremarkable. Laboratory tests show the following: platelet count 200,000/mm3, PT 12 seconds, and PTT 43 seconds. Which of the following is the most likely cause of this patient‚Äôs symptoms? \\n (A) Factor V Leiden\\n (B) Hemophilia A\\n (C) Lupus anticoagulant\\n (D) Protein C deficiency\\n (E) Von Willebrand disease</QUESTION><ANSWER> (E) Von Willebrand disease</ANSWER>\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_prompt(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"<QUESTION>{question} {option_string}</QUESTION><ANSWER> ({answer_idx}) {answer}</ANSWER>\").format_map(row)\n",
    "create_prompt(train_dataset[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f1a9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:33.036242Z",
     "iopub.status.busy": "2024-02-16T17:49:33.036106Z",
     "iopub.status.idle": "2024-02-16T17:49:33.040653Z",
     "shell.execute_reply": "2024-02-16T17:49:33.040484Z"
    },
    "papermill": {
     "duration": 0.007646,
     "end_time": "2024-02-16T17:49:33.041134",
     "exception": false,
     "start_time": "2024-02-16T17:49:33.033488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt_no_answer(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"<QUESTION>{question} {option_string}</QUESTION><ANSWER> \").format_map(row)\n",
    "\n",
    "def return_prompt_no_answer(row):\n",
    "    return {\"text\": create_prompt_no_answer(row)}\n",
    "\n",
    "def return_prompt(row):\n",
    "    return {\"text\": create_prompt(row)}\n",
    "    \n",
    "test_dataset = eval_dataset.map(return_prompt_no_answer)\n",
    "# print(test_dataset[0][\"text\"])\n",
    "train_dataset_with_texts = train_dataset.map(return_prompt)\n",
    "# print(train_dataset_with_texts[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79dbbb0-6dac-4f78-a862-8201088c9d57",
   "metadata": {
    "papermill": {
     "duration": 0.00235,
     "end_time": "2024-02-16T17:49:33.045836",
     "exception": false,
     "start_time": "2024-02-16T17:49:33.043486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Training the full models is expensive, but if you have a GPU that can fit the full model, you can skip this part. Let's just train the last 8 layers of the model (Llama2-7B has 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37d2f1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:33.050905Z",
     "iopub.status.busy": "2024-02-16T17:49:33.050724Z",
     "iopub.status.idle": "2024-02-16T17:49:44.595617Z",
     "shell.execute_reply": "2024-02-16T17:49:44.595229Z"
    },
    "papermill": {
     "duration": 11.548589,
     "end_time": "2024-02-16T17:49:44.596740",
     "exception": false,
     "start_time": "2024-02-16T17:49:33.048151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                    | 1/2 [00:03<00:03,  3.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from utilities.biollama import BioLlama\n",
    "\n",
    "# questions = [\"Which is the main calcium pump of the sarcoplasmic reticulum? Answer:\"]\n",
    "amended_questions = [\"The main calcium pump of the sarcoplasmic reticulum is \"]\n",
    "questions = amended_questions\n",
    "# answers = [\"Sarcoplasmic reticulum Ca(2+)-ATPase\"] # or \"SERCA\",\"serca2\"\n",
    "\n",
    "prompt = questions[0]\n",
    "# model_id = \"TheBloke/Llama-2-7b-chat-GPTQ\"\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "chunk_length = 32\n",
    "\n",
    "RETRO_layer_ids = [15]\n",
    "\n",
    "BioLlama = BioLlama(\n",
    "    model_id=model_id,\n",
    "    chunk_length=chunk_length,\n",
    "    RETRO_layer_ids=RETRO_layer_ids,\n",
    "    training=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e11ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:44.610635Z",
     "iopub.status.busy": "2024-02-16T17:49:44.610424Z",
     "iopub.status.idle": "2024-02-16T17:49:44.612215Z",
     "shell.execute_reply": "2024-02-16T17:49:44.612007Z"
    },
    "papermill": {
     "duration": 0.0114,
     "end_time": "2024-02-16T17:49:44.612883",
     "exception": false,
     "start_time": "2024-02-16T17:49:44.601483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(BioLlama.model)\n",
    "model = BioLlama.model\n",
    "tokenizer = BioLlama.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:44.621912Z",
     "iopub.status.busy": "2024-02-16T17:49:44.621684Z",
     "iopub.status.idle": "2024-02-16T17:49:44.628659Z",
     "shell.execute_reply": "2024-02-16T17:49:44.628449Z"
    },
    "papermill": {
     "duration": 0.01238,
     "end_time": "2024-02-16T17:49:44.629355",
     "exception": false,
     "start_time": "2024-02-16T17:49:44.616975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freezing layers, currently only works for single unfrozen retro layer\n",
      "printing layer 14 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "\n",
      "printing layer 15 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "cca_attn.q_proj.weight, requires_grad = False\n",
      "cca_attn.k_proj.weight, requires_grad = False\n",
      "cca_attn.v_proj.weight, requires_grad = False\n",
      "cca_attn.o_proj.weight, requires_grad = False\n",
      "pre_cca_layernorm.weight, requires_grad = False\n",
      "\n",
      "printing layer 15 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "cca_attn.q_proj.weight, requires_grad = True\n",
      "cca_attn.k_proj.weight, requires_grad = True\n",
      "cca_attn.v_proj.weight, requires_grad = True\n",
      "cca_attn.o_proj.weight, requires_grad = True\n",
      "pre_cca_layernorm.weight, requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "print(\"freezing layers, currently only works for single unfrozen retro layer\")\n",
    "n_freeze = BioLlama.RETRO_layer_ids[0]\n",
    "# n_freeze = 15\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): \n",
    "    param.requires_grad = True\n",
    "#for every parameter in retro_layer_params, print where in the model it comes from (ie is it from self attention, layer norm, etc)\n",
    "print(\"printing layer 14 params\")\n",
    "for name, param in model.model.layers[14].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\") \n",
    "print(\"\\nprinting layer 15 params\")\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")   \n",
    "\n",
    "list_of_params_to_unfreeze = [\n",
    "    \"cca_attn.q_proj.weight\",\n",
    "    \"cca_attn.k_proj.weight\",\n",
    "    \"cca_attn.v_proj.weight\",\n",
    "    \"cca_attn.o_proj.weight\",\n",
    "    \"pre_cca_layernorm.weight\",\n",
    "]\n",
    "\n",
    "for name, param in model.model.layers[n_freeze].named_parameters(): \n",
    "    if name in list_of_params_to_unfreeze:\n",
    "        param.requires_grad = True\n",
    "print(\"\\nprinting layer 15 params\")\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:44.638643Z",
     "iopub.status.busy": "2024-02-16T17:49:44.638414Z",
     "iopub.status.idle": "2024-02-16T17:49:44.640195Z",
     "shell.execute_reply": "2024-02-16T17:49:44.639916Z"
    },
    "papermill": {
     "duration": 0.007161,
     "end_time": "2024-02-16T17:49:44.640849",
     "exception": false,
     "start_time": "2024-02-16T17:49:44.633688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "model.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:44.649945Z",
     "iopub.status.busy": "2024-02-16T17:49:44.649737Z",
     "iopub.status.idle": "2024-02-16T17:49:44.652746Z",
     "shell.execute_reply": "2024-02-16T17:49:44.652545Z"
    },
    "papermill": {
     "duration": 0.008319,
     "end_time": "2024-02-16T17:49:44.653436",
     "exception": false,
     "start_time": "2024-02-16T17:49:44.645117",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6805.53M, Trainable: 198.18M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c57893e-6e42-4cda-83c8-1e51a86d2da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:44.662591Z",
     "iopub.status.busy": "2024-02-16T17:49:44.662455Z",
     "iopub.status.idle": "2024-02-16T17:49:44.664462Z",
     "shell.execute_reply": "2024-02-16T17:49:44.664251Z"
    },
    "papermill": {
     "duration": 0.007447,
     "end_time": "2024-02-16T17:49:44.665162",
     "exception": false,
     "start_time": "2024-02-16T17:49:44.657715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5605\n",
      "changing total num size to 5000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "total_num_steps = 11_210 // batch_size\n",
    "print(total_num_steps)\n",
    "\n",
    "\n",
    "total_num_steps = 5000\n",
    "print(f\"changing total num size to {total_num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c86339ca-27e5-496b-9559-3a65bb3c26a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:44.674363Z",
     "iopub.status.busy": "2024-02-16T17:49:44.674181Z",
     "iopub.status.idle": "2024-02-16T17:49:44.708322Z",
     "shell.execute_reply": "2024-02-16T17:49:44.708011Z"
    },
    "papermill": {
     "duration": 0.039778,
     "end_time": "2024-02-16T17:49:44.709255",
     "exception": false,
     "start_time": "2024-02-16T17:49:44.669477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size//2,\n",
    "    bf16=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=total_num_steps // 10,\n",
    "    num_train_epochs=10,\n",
    "    max_steps=total_num_steps,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=total_num_steps // 6,\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=8,\n",
    "    save_strategy=\"epoch\", #changed to epoch so we save every epoch i guess?\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e35a9b70-f36b-4bfd-857f-c80d3450e111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:44.718993Z",
     "iopub.status.busy": "2024-02-16T17:49:44.718767Z",
     "iopub.status.idle": "2024-02-16T17:49:44.766755Z",
     "shell.execute_reply": "2024-02-16T17:49:44.766446Z"
    },
    "papermill": {
     "duration": 0.05365,
     "end_time": "2024-02-16T17:49:44.767574",
     "exception": false,
     "start_time": "2024-02-16T17:49:44.713924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from utils import LLMSampleCB, token_accuracy\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset_with_texts,\n",
    "    dataset_text_field=\"text\",\n",
    "    eval_dataset=test_dataset,\n",
    "    packing=True,\n",
    "    max_seq_length=1024,\n",
    "    args=training_args,\n",
    "    formatting_func=create_prompt,\n",
    "    # compute_metrics=token_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "480e9afd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:44.777117Z",
     "iopub.status.busy": "2024-02-16T17:49:44.776996Z",
     "iopub.status.idle": "2024-02-16T17:49:44.779035Z",
     "shell.execute_reply": "2024-02-16T17:49:44.778810Z"
    },
    "papermill": {
     "duration": 0.007741,
     "end_time": "2024-02-16T17:49:44.779836",
     "exception": false,
     "start_time": "2024-02-16T17:49:44.772095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "len(train_dataset_with_texts)\n",
    "#train_dataset_with_texts is of type dataset. we want to create a copy that exists only \n",
    "#of the items from 9240 onwards\n",
    "print(type(train_dataset_with_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5754789-2e15-4bc9-800c-01f8ffc625e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T17:49:44.789199Z",
     "iopub.status.busy": "2024-02-16T17:49:44.789120Z",
     "iopub.status.idle": "2024-02-16T18:23:22.034333Z",
     "shell.execute_reply": "2024-02-16T18:23:22.033903Z"
    },
    "papermill": {
     "duration": 2017.251152,
     "end_time": "2024-02-16T18:23:22.035446",
     "exception": false,
     "start_time": "2024-02-16T17:49:44.784294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 33:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>833</td>\n",
       "      <td>1.310800</td>\n",
       "      <td>1.358173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1666</td>\n",
       "      <td>1.207300</td>\n",
       "      <td>1.302233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2499</td>\n",
       "      <td>1.053900</td>\n",
       "      <td>1.252271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3332</td>\n",
       "      <td>0.992700</td>\n",
       "      <td>1.207682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4165</td>\n",
       "      <td>0.831400</td>\n",
       "      <td>1.182753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4998</td>\n",
       "      <td>0.872300</td>\n",
       "      <td>1.175648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/utils.py:268: UserWarning: The dataset reached end and the iterator is reset to the start.\n",
      "  warnings.warn(\"The dataset reached end and the iterator is reset to the start.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.004 MB of 0.004 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.004 MB of 0.034 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.004 MB of 0.034 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.034 MB of 0.034 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÜ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ‚ñá‚ñà‚ñà‚ñÅ‚ñá‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ‚ñá‚ñà‚ñà‚ñÅ‚ñá‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 1.17565\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 64.9421\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 19.587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 19.587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 0.98\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 5000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.7987\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 1.07845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2012.2374\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 4.97\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.485\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33msparkling-snake-127\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft/runs/nrrfuzgz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240216_174928-nrrfuzgz/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#very hacky but maybe this will work:\n",
    "tokenizer.model_input_names = ['labels', 'input_ids', 'attention_mask']\n",
    "# trainer.args.train_batch_size = 1\n",
    "# self.args.train_batch_size\n",
    "\n",
    "#also hacky, but could work:\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Starting training\")\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "684ca6fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T18:23:22.050316Z",
     "iopub.status.busy": "2024-02-16T18:23:22.049893Z",
     "iopub.status.idle": "2024-02-16T18:23:57.551710Z",
     "shell.execute_reply": "2024-02-16T18:23:57.551302Z"
    },
    "papermill": {
     "duration": 35.511946,
     "end_time": "2024-02-16T18:23:57.552836",
     "exception": false,
     "start_time": "2024-02-16T18:23:22.040890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/service/BioLlama/utilities/finetuning/biollama_training_output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 26586516\r\n",
      "drwxrwxr-x 2 service service       4096 Feb 13 01:39 checkpoint-100\r\n",
      "drwxrwxr-x 2 service service       4096 Feb 16 18:23 checkpoint-5000\r\n",
      "-rw-rw-r-- 1 service service        690 Feb 16 18:23 config.json\r\n",
      "-rw-rw-r-- 1 service service        184 Feb 16 18:23 generation_config.json\r\n",
      "drwxrwxr-x 2 service service       4096 Feb 16 17:49 logs\r\n",
      "-rw-rw-r-- 1 service service 4840396416 Feb 16 18:23 model-00001-of-00006.safetensors\r\n",
      "-rw-rw-r-- 1 service service 4857206856 Feb 16 18:23 model-00002-of-00006.safetensors\r\n",
      "-rw-rw-r-- 1 service service 4991441360 Feb 16 18:23 model-00003-of-00006.safetensors\r\n",
      "-rw-rw-r-- 1 service service 4991424864 Feb 16 18:23 model-00004-of-00006.safetensors\r\n",
      "-rw-rw-r-- 1 service service 4857206904 Feb 16 18:23 model-00005-of-00006.safetensors\r\n",
      "-rw-rw-r-- 1 service service 2684472112 Feb 16 18:23 model-00006-of-00006.safetensors\r\n",
      "-rw-rw-r-- 1 service service      24362 Feb 16 18:23 model.safetensors.index.json\r\n",
      "-rw-rw-r-- 1 service service        437 Feb 16 18:23 special_tokens_map.json\r\n",
      "-rw-rw-r-- 1 service service       1762 Feb 16 18:23 tokenizer_config.json\r\n",
      "-rw-rw-r-- 1 service service    1842767 Feb 16 18:23 tokenizer.json\r\n",
      "-rw-rw-r-- 1 service service     499723 Feb 16 18:23 tokenizer.model\r\n",
      "-rw-rw-r-- 1 service service       4792 Feb 16 18:23 training_args.bin\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath(output_dir))\n",
    "trainer.save_model(output_dir)\n",
    "!ls -l $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16ff2529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T18:23:57.563818Z",
     "iopub.status.busy": "2024-02-16T18:23:57.563712Z",
     "iopub.status.idle": "2024-02-16T18:24:04.406783Z",
     "shell.execute_reply": "2024-02-16T18:24:04.406398Z"
    },
    "papermill": {
     "duration": 6.849638,
     "end_time": "2024-02-16T18:24:04.407759",
     "exception": false,
     "start_time": "2024-02-16T18:23:57.558121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/service/BioLlama/utilities/finetuning/biollama_training_output/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                 | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                       | 1/6 [00:00<00:03,  1.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                      | 2/6 [00:01<00:02,  1.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                    | 3/6 [00:02<00:02,  1.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 4/6 [00:02<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  3.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at /home/service/BioLlama/utilities/finetuning/biollama_training_output/ were not used when initializing LlamaForCausalLM: ['model.layers.15.cca_attn.k_proj.weight', 'model.layers.15.cca_attn.o_proj.weight', 'model.layers.15.cca_attn.q_proj.weight', 'model.layers.15.cca_attn.v_proj.weight', 'model.layers.15.pre_cca_layernorm.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "#load this local model here and use it to generate some text\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\"\n",
    "print(output_dir)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "from utilities.biollama import BioLlama\n",
    "#answers = [\"Sarcoplasmic reticulum Ca(2+)-ATPase\"] # or \"SERCA\",\"serca2\"\n",
    "\n",
    "chunk_length = 32\n",
    "\n",
    "BioLlama = BioLlama(model_id=output_dir, chunk_length=chunk_length, RETRO_layer_ids = [15], training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f20f493d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T18:24:04.421489Z",
     "iopub.status.busy": "2024-02-16T18:24:04.421349Z",
     "iopub.status.idle": "2024-02-16T18:24:22.157006Z",
     "shell.execute_reply": "2024-02-16T18:24:22.156646Z"
    },
    "papermill": {
     "duration": 17.746324,
     "end_time": "2024-02-16T18:24:22.157880",
     "exception": false,
     "start_time": "2024-02-16T18:24:04.411556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \n",
      " (A) Ampicillin\n",
      " (B) Ceftriaxone\n",
      " (C) Ciprofloxacin\n",
      " (D) Doxycycline\n",
      " (E) Nitrofurantoin</QUESTION>\n",
      "<ANSWER> \n",
      " (D) Doxycycline</ANSWER>\n",
      "Time taken for generation: 17.732990503311157\n",
      "Tokens per second: 14.26719311402998\n"
     ]
    }
   ],
   "source": [
    "prompt  = '<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \\n (A) Ampicillin\\n (B) Ceftriaxone\\n (C) Ciprofloxacin\\n (D) Doxycycline\\n (E) Nitrofurantoin</QUESTION>\\n<ANSWER> '\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99856974",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T18:24:22.173663Z",
     "iopub.status.busy": "2024-02-16T18:24:22.173506Z",
     "iopub.status.idle": "2024-02-16T18:24:39.758258Z",
     "shell.execute_reply": "2024-02-16T18:24:39.757909Z"
    },
    "papermill": {
     "duration": 17.595133,
     "end_time": "2024-02-16T18:24:39.759136",
     "exception": false,
     "start_time": "2024-02-16T18:24:22.164003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby? \n",
      " (A) Placing the infant in a supine position on a firm mattress while sleeping\n",
      " (B) Routine postnatal electrocardiogram (ECG)\n",
      " (C) Keeping the infant covered and maintaining a high room temperature\n",
      " (D) Application of a device to maintain the sleeping position\n",
      " (E) Avoiding pacifier use during sleep</QUESTION>\n",
      "<ANSWER> \n",
      " (D) Application of device to maintain sleep position</ANSWER>\n",
      "Time taken for generation: 17.58214783668518\n",
      "Tokens per second: 10.123905318814503\n"
     ]
    }
   ],
   "source": [
    "prompt2 = '<QUESTION>A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby? \\n (A) Placing the infant in a supine position on a firm mattress while sleeping\\n (B) Routine postnatal electrocardiogram (ECG)\\n (C) Keeping the infant covered and maintaining a high room temperature\\n (D) Application of a device to maintain the sleeping position\\n (E) Avoiding pacifier use during sleep</QUESTION>\\n<ANSWER> '\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt2, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16ad4c40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T18:24:39.771887Z",
     "iopub.status.busy": "2024-02-16T18:24:39.771761Z",
     "iopub.status.idle": "2024-02-16T18:25:03.618218Z",
     "shell.execute_reply": "2024-02-16T18:25:03.617770Z"
    },
    "papermill": {
     "duration": 23.853657,
     "end_time": "2024-02-16T18:25:03.619086",
     "exception": false,
     "start_time": "2024-02-16T18:24:39.765429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation? \n",
      " (A) Abnormal migration of ventral pancreatic bud\n",
      " (B) Complete failure of proximal duodenum to recanalize\n",
      " (C) Error in neural crest cell migration\n",
      " (D) Abnormal hypertrophy of the pylorus\n",
      " (E) Failure of lateral body folds to move ventrally and fuse in the midline</QUESTION>\n",
      "<ANSWER> \n",
      " (D) Abnormal hypertrophy of the pylorus</ANSWER>\n",
      "Time taken for generation: 23.843688249588013\n",
      "Tokens per second: 10.191376328039297\n"
     ]
    }
   ],
   "source": [
    "prompt3 = \"<QUESTION>A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation? \\n (A) Abnormal migration of ventral pancreatic bud\\n (B) Complete failure of proximal duodenum to recanalize\\n (C) Error in neural crest cell migration\\n (D) Abnormal hypertrophy of the pylorus\\n (E) Failure of lateral body folds to move ventrally and fuse in the midline</QUESTION>\\n<ANSWER> \"\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt3, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f24725d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T18:25:03.634214Z",
     "iopub.status.busy": "2024-02-16T18:25:03.634099Z",
     "iopub.status.idle": "2024-02-16T18:25:21.265375Z",
     "shell.execute_reply": "2024-02-16T18:25:21.264919Z"
    },
    "papermill": {
     "duration": 17.641168,
     "end_time": "2024-02-16T18:25:21.266237",
     "exception": false,
     "start_time": "2024-02-16T18:25:03.625069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A 20-year-old woman presents with menorrhagia for the past several years. She says that her menses ‚Äúhave always been heavy‚Äù, and she has experienced easy bruising for as long as she can remember. Family history is significant for her mother, who had similar problems with bruising easily. The patient's vital signs include: heart rate 98/min, respiratory rate 14/min, temperature 36.1¬∞C (96.9¬∞F), and blood pressure 110/87 mm Hg. Physical examination is unremarkable. Laboratory tests show the following: platelet count 200,000/mm3, PT 12 seconds, and PTT 43 seconds. Which of the following is the most likely cause of this patient‚Äôs symptoms? \n",
      " (A) Factor V Leiden\n",
      " (B) Hemophilia A\n",
      " (C) Lupus anticoagulant\n",
      " (D) Protein C deficiency\n",
      " (E) Von Willebrand disease</QUESTION>\n",
      "<ANSWER> \n",
      " (A) Factor V Leiden</ANSWER>\n",
      "Time taken for generation: 17.628591299057007\n",
      "Tokens per second: 14.91894590658917\n"
     ]
    }
   ],
   "source": [
    "prompt4 = \"<QUESTION>A 20-year-old woman presents with menorrhagia for the past several years. She says that her menses ‚Äúhave always been heavy‚Äù, and she has experienced easy bruising for as long as she can remember. Family history is significant for her mother, who had similar problems with bruising easily. The patient's vital signs include: heart rate 98/min, respiratory rate 14/min, temperature 36.1¬∞C (96.9¬∞F), and blood pressure 110/87 mm Hg. Physical examination is unremarkable. Laboratory tests show the following: platelet count 200,000/mm3, PT 12 seconds, and PTT 43 seconds. Which of the following is the most likely cause of this patient‚Äôs symptoms? \\n (A) Factor V Leiden\\n (B) Hemophilia A\\n (C) Lupus anticoagulant\\n (D) Protein C deficiency\\n (E) Von Willebrand disease</QUESTION>\\n<ANSWER> \"\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt4, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883afe6b",
   "metadata": {
    "papermill": {
     "duration": 0.008487,
     "end_time": "2024-02-16T18:25:21.281024",
     "exception": false,
     "start_time": "2024-02-16T18:25:21.272537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1afa7",
   "metadata": {
    "papermill": {
     "duration": 0.005473,
     "end_time": "2024-02-16T18:25:21.292066",
     "exception": false,
     "start_time": "2024-02-16T18:25:21.286593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2157.495538,
   "end_time": "2024-02-16T18:25:24.138917",
   "environment_variables": {},
   "exception": null,
   "input_path": "BioLlama_finetuning_WandB_HF.ipynb",
   "output_path": "output_biollama.ipynb",
   "parameters": {},
   "start_time": "2024-02-16T17:49:26.643379",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}