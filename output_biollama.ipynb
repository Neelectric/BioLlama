{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb384650",
   "metadata": {
    "papermill": {
     "duration": 0.002664,
     "end_time": "2024-03-18T19:05:00.881009",
     "exception": false,
     "start_time": "2024-03-18T19:05:00.878345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is taken directly from https://github.com/tcapelle/llm_recipes/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
   "metadata": {
    "papermill": {
     "duration": 0.00293,
     "end_time": "2024-03-18T19:05:00.892685",
     "exception": false,
     "start_time": "2024-03-18T19:05:00.889755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Finetuning Llama-2 to produce BioLlama using HF and WanB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d50c75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:00.897615Z",
     "iopub.status.busy": "2024-03-18T19:05:00.897431Z",
     "iopub.status.idle": "2024-03-18T19:05:00.900948Z",
     "shell.execute_reply": "2024-03-18T19:05:00.900664Z"
    },
    "papermill": {
     "duration": 0.006901,
     "end_time": "2024-03-18T19:05:00.901579",
     "exception": false,
     "start_time": "2024-03-18T19:05:00.894678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "#print python version\n",
    "import sys\n",
    "print(sys.version)\n",
    "# print hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa5b061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:00.906136Z",
     "iopub.status.busy": "2024-03-18T19:05:00.906014Z",
     "iopub.status.idle": "2024-03-18T19:05:00.907911Z",
     "shell.execute_reply": "2024-03-18T19:05:00.907665Z"
    },
    "papermill": {
     "duration": 0.004922,
     "end_time": "2024-03-18T19:05:00.908492",
     "exception": false,
     "start_time": "2024-03-18T19:05:00.903570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import GPUtil\n",
    "# import time\n",
    "\n",
    "# while True:\n",
    "#     GPUs = GPUtil.getGPUs()\n",
    "#     GPU_0_used = GPUs[0].memoryUtil > 0.1\n",
    "#     GPU_1_used = GPUs[1].memoryUtil > 0.1\n",
    "#     if (not GPU_0_used) and (not GPU_1_used):\n",
    "#         print(\"Both GPUs are free\")\n",
    "#         # wait for 30 seconds:\n",
    "#         time.sleep(30)\n",
    "#         # check again\n",
    "#         GPUs = GPUtil.getGPUs()\n",
    "#         GPU_0_used = GPUs[0].memoryUtil > 0.1\n",
    "#         GPU_1_used = GPUs[1].memoryUtil > 0.1\n",
    "#         if (not GPU_0_used) and (not GPU_1_used):\n",
    "#             print(\"Both GPUs are still free\")\n",
    "#             break\n",
    "#         else:\n",
    "#             print(\"At least one GPU is used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:00.913200Z",
     "iopub.status.busy": "2024-03-18T19:05:00.912950Z",
     "iopub.status.idle": "2024-03-18T19:05:00.915334Z",
     "shell.execute_reply": "2024-03-18T19:05:00.915080Z"
    },
    "papermill": {
     "duration": 0.005509,
     "end_time": "2024-03-18T19:05:00.915978",
     "exception": false,
     "start_time": "2024-03-18T19:05:00.910469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install wandb transformers trl datasets \"protobuf==3.20.3\" evaluate\n",
    "# !wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json\n",
    "from utilities.parse_benchmark import parse_benchmark\n",
    "# benchmark = \"MedQA-4\"\n",
    "benchmark = \"MedQA-5\"\n",
    "# benchmark = \"MedMCQA\"\n",
    "# benchmark = \"PubMedQA\"\n",
    "# benchmark = \"bioASQ_with_snippet\"\n",
    "# if benchmark == \"PubMedQA\":\n",
    "#     benchmark_questions, benchmark_answers = parse_benchmark(benchmark, \"test.json\")\n",
    "# else:\n",
    "#     benchmark_questions, benchmark_answers = parse_benchmark(benchmark, \"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b16861",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:00.920791Z",
     "iopub.status.busy": "2024-03-18T19:05:00.920670Z",
     "iopub.status.idle": "2024-03-18T19:05:01.756353Z",
     "shell.execute_reply": "2024-03-18T19:05:01.756022Z"
    },
    "papermill": {
     "duration": 0.838951,
     "end_time": "2024-03-18T19:05:01.757077",
     "exception": false,
     "start_time": "2024-03-18T19:05:00.918126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRO_layer_ids is [19] and torch_dtype is torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "size = \"13\"\n",
    "if size == \"7\":\n",
    "    RETRO_layer_ids = [15]\n",
    "    torch_dtype=torch.float32\n",
    "elif size == \"13\":\n",
    "    RETRO_layer_ids = [19]\n",
    "    torch_dtype=torch.bfloat16\n",
    "elif size == \"70\":\n",
    "    RETRO_layer_ids = [39]\n",
    "    torch_dtype=torch.bfloat16\n",
    "    print(\"best of luck training 70b lol\")\n",
    "print(f\"RETRO_layer_ids is {RETRO_layer_ids} and torch_dtype is {torch_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f13f2743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:01.769992Z",
     "iopub.status.busy": "2024-03-18T19:05:01.769736Z",
     "iopub.status.idle": "2024-03-18T19:05:01.772100Z",
     "shell.execute_reply": "2024-03-18T19:05:01.771888Z"
    },
    "papermill": {
     "duration": 0.012138,
     "end_time": "2024-03-18T19:05:01.772872",
     "exception": false,
     "start_time": "2024-03-18T19:05:01.760734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5605\n",
      "changing total num size to 20356\n",
      "MedQA-5\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "total_num_steps = 11_210 // batch_size\n",
    "print(total_num_steps)\n",
    "if benchmark == \"MedQA-4\" or benchmark == \"MedQA-5\":\n",
    "    total_num_steps = 10178 * 2\n",
    "elif benchmark == \"MedMCQA\":\n",
    "    total_num_steps = 100000 * 2\n",
    "elif benchmark == \"PubMedQA\":\n",
    "    total_num_steps = 1000 * 2 \n",
    "elif benchmark == \"bioASQ_with_snippet\":\n",
    "    total_num_steps = 486 * 2\n",
    "print(f\"changing total num size to {total_num_steps}\")\n",
    "print(benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a154f968-da0f-4bdc-bf45-e499d95f0606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:01.780121Z",
     "iopub.status.busy": "2024-03-18T19:05:01.780015Z",
     "iopub.status.idle": "2024-03-18T19:05:06.972724Z",
     "shell.execute_reply": "2024-03-18T19:05:06.972278Z"
    },
    "papermill": {
     "duration": 5.197372,
     "end_time": "2024-03-18T19:05:06.973605",
     "exception": false,
     "start_time": "2024-03-18T19:05:01.776233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnelectric\u001b[0m (\u001b[33mneelectric\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/service/BioLlama/wandb/run-20240318_190503-lyhpr3tx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msandy-pyramid-220\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft/runs/lyhpr3tx\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "wandb.init(project=\"biollama_ft\", # the project I am working on\n",
    "           tags=[\"hf_sft\", \"BioLlama\"]) # the Hyperparameters I want to keep track of\n",
    "\n",
    "wandb.alert(\n",
    "    title=\"Starting MedMCQA run\",\n",
    "    text=f\"We have started training\",\n",
    "    level=AlertLevel.WARN,\n",
    "    wait_duration=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811291cc-b7ce-422a-8971-3cbf9fe10a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:06.982309Z",
     "iopub.status.busy": "2024-03-18T19:05:06.982173Z",
     "iopub.status.idle": "2024-03-18T19:05:07.784323Z",
     "shell.execute_reply": "2024-03-18T19:05:07.784044Z"
    },
    "papermill": {
     "duration": 0.807497,
     "end_time": "2024-03-18T19:05:07.785213",
     "exception": false,
     "start_time": "2024-03-18T19:05:06.977716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import json\n",
    "if benchmark == \"PubMedQA\":\n",
    "    artifact_dir = os.getcwd() + \"/benchmarks/PubMedQA/edited\"\n",
    "    dataset = load_dataset(\"json\", data_dir=artifact_dir)\n",
    "else:\n",
    "    if benchmark == \"MedQA-4\":\n",
    "        artifact_dir = os.getcwd() + \"/benchmarks/MedQA-4-option/\"\n",
    "    elif benchmark == \"MedQA-5\":\n",
    "        artifact_dir = os.getcwd() + \"/benchmarks/MedQA-USMLE/\"\n",
    "    elif benchmark == \"MedMCQA\":\n",
    "        artifact_dir = os.getcwd() + \"/benchmarks/MedMCQA/\"\n",
    "    elif benchmark == \"bioASQ_with_snippet\":\n",
    "        print(\"loading bioASQ\")\n",
    "        artifact_dir = os.getcwd() + \"/benchmarks/BioASQ/edited\"\n",
    "    dataset = load_dataset(\"json\", data_dir=artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3f4c49e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:07.795161Z",
     "iopub.status.busy": "2024-03-18T19:05:07.793782Z",
     "iopub.status.idle": "2024-03-18T19:05:07.799735Z",
     "shell.execute_reply": "2024-03-18T19:05:07.799491Z"
    },
    "papermill": {
     "duration": 0.01096,
     "end_time": "2024-03-18T19:05:07.800412",
     "exception": false,
     "start_time": "2024-03-18T19:05:07.789452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 10178\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 1272\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 1273\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c7dda87-d70a-470b-a0f9-040af434dc42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:07.808541Z",
     "iopub.status.busy": "2024-03-18T19:05:07.808360Z",
     "iopub.status.idle": "2024-03-18T19:05:07.811246Z",
     "shell.execute_reply": "2024-03-18T19:05:07.811009Z"
    },
    "papermill": {
     "duration": 0.007897,
     "end_time": "2024-03-18T19:05:07.812173",
     "exception": false,
     "start_time": "2024-03-18T19:05:07.804276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1272\n",
      "10178\n"
     ]
    }
   ],
   "source": [
    "if benchmark == \"PubMedQA\" or benchmark == \"bioASQ_with_snippet\":\n",
    "    train_dataset = dataset[\"test\"]\n",
    "else:\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"validation\"]\n",
    "    print(len(eval_dataset))\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a038eb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:07.819866Z",
     "iopub.status.busy": "2024-03-18T19:05:07.819782Z",
     "iopub.status.idle": "2024-03-18T19:05:07.824812Z",
     "shell.execute_reply": "2024-03-18T19:05:07.824635Z"
    },
    "papermill": {
     "duration": 0.009351,
     "end_time": "2024-03-18T19:05:07.825259",
     "exception": false,
     "start_time": "2024-03-18T19:05:07.815908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You start all of your responses with <ANSWER> and end them with </ANSWER>, as shown in the following example: \n",
      "<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\n",
      " (A) Ampicillin\n",
      " (B) Ceftriaxone\n",
      " (C) Ciprofloxacin\n",
      " (D) Doxycycline\n",
      " (E) Nitrofurantoin</QUESTION>\n",
      "<ANSWER> (E) Nitrofurantoin</ANSWER>\n",
      "<QUESTION>A 40-year-old zookeeper presents to the emergency department complaining of severe abdominal pain that radiates to her back, and nausea. The pain started 2 days ago and slowly increased until she could not tolerate it any longer. Past medical history is significant for hypertension and hypothyroidism. Additionally, she reports that she was recently stung by one of the zoo‚Äôs smaller scorpions, but did not seek medical treatment. She takes aspirin, levothyroxine, oral contraceptive pills, and a multivitamin daily. Family history is noncontributory. Today, her blood pressure is 108/58 mm Hg, heart rate is 99/min, respiratory rate is 21/min, and temperature is 37.0¬∞C (98.6¬∞F). On physical exam, she is a well-developed, obese female that looks unwell. Her heart has a regular rate and rhythm. Radial pulses are weak but symmetric. Her lungs are clear to auscultation bilaterally. Her lateral left ankle is swollen, erythematous, and painful to palpate. An abdominal CT is consistent with acute pancreatitis. Which of the following is the most likely etiology for this patient‚Äôs disease?\n",
      " (A) Aspirin\n",
      " (B) Oral contraceptive pills\n",
      " (C) Scorpion sting\n",
      " (D) Hypothyroidism\n",
      " (E) Obesity</QUESTION>\n",
      "<ANSWER> (C) Scorpion sting</ANSWER>\n"
     ]
    }
   ],
   "source": [
    "from utilities.prompts2 import promptify\n",
    "# def create_prompt(row):\n",
    "#     option_string = \"\"\n",
    "#     for option in row[\"options\"].keys():\n",
    "#         option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "#     row[\"option_string\"] = option_string\n",
    "#     return (\"<QUESTION>{question} {option_string}</QUESTION><ANSWER> ({answer_idx}) {answer}</ANSWER>\").format_map(row)\n",
    "def create_prompt(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    MCQ_answer = \"(\" + row['answer_idx'] + \") \" + row[\"answer\"]\n",
    "    question = row[\"question\"] + option_string\n",
    "    promptified = promptify(benchmark, question, retrieval_mode = None, retrieved_chunks = None, model = None)\n",
    "    row[\"promptified\"] = promptified\n",
    "    row[\"MCQ_answer\"] = MCQ_answer\n",
    "    return (\"{promptified} {MCQ_answer}</ANSWER>\").format_map(row)\n",
    "\n",
    "if benchmark == \"MedMCQA\":\n",
    "    def create_prompt(row):\n",
    "        option_string = \"\\n(1) \" + row['opa']\n",
    "        option_string += \"\\n(2) \" + row['opb']\n",
    "        option_string += \"\\n(3) \" + row['opc']\n",
    "        option_string += \"\\n(4) \" + row['opd']\n",
    "        row[\"option_string\"] = option_string\n",
    "        if row['cop'] == 1:\n",
    "            row['answer'] = row['opa']\n",
    "        elif row['cop'] == 2:\n",
    "            row['answer'] = row['opb']\n",
    "        elif row['cop'] == 3:\n",
    "            row['answer'] = row['opc']\n",
    "        elif row['cop'] == 4:\n",
    "            row['answer'] = row['opd']\n",
    "        question = row[\"question\"] + option_string\n",
    "        promptified = promptify(benchmark, question, retrieval_mode = None, retrieved_chunks = None, model = None)\n",
    "        #replace all occurrences of \"{\" with \"(\":\n",
    "        promptified = promptified.replace(\"{\", \"(\")\n",
    "        promptified = promptified.replace(\"}\", \")\")\n",
    "        return (promptified + \" {cop}</ANSWER>\").format_map(row)\n",
    "elif benchmark == \"PubMedQA\":\n",
    "    def create_prompt(row):\n",
    "        snippet_string = \"\"\n",
    "        for snippet in row[\"CONTEXTS\"]:\n",
    "            snippet_string += snippet + \"\\n\"\n",
    "        row[\"snippet_string\"] = snippet_string\n",
    "        row[\"example\"] = \"You start all of your responses with <ANSWER> and end them with </ANSWER>, as shown in the following example:\\n<QUESTION>Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?</QUESTION>\\n<ANSWER> yes</ANSWER>\\nDo not justify your response, respond with only yes, maybe or no.\\n\"\n",
    "        return (\"Using the following text snippets, answer the question that follows.\\n<SNIPPETS>\\n{snippet_string}</SNIPPETS>\\n{example}<QUESTION>{QUESTION}</QUESTION>\\n<ANSWER> {final_decision}</ANSWER>\").format_map(row)\n",
    "elif benchmark == \"bioASQ_with_snippet\":\n",
    "    def create_prompt(row):\n",
    "        question = [row['snippets'], row['question']]\n",
    "        # print(question)\n",
    "        promptified = promptify(\"bioASQ_with_snippet\", question, retrieval_mode = None, retrieved_chunks = None, model = None)\n",
    "        return promptified + \" \" + row['answer'] + \"</ANSWER>\"\n",
    "print(create_prompt(train_dataset[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8f1a9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:07.830712Z",
     "iopub.status.busy": "2024-03-18T19:05:07.830600Z",
     "iopub.status.idle": "2024-03-18T19:05:07.839815Z",
     "shell.execute_reply": "2024-03-18T19:05:07.839638Z"
    },
    "papermill": {
     "duration": 0.012653,
     "end_time": "2024-03-18T19:05:07.840365",
     "exception": false,
     "start_time": "2024-03-18T19:05:07.827712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You start all of your responses with <ANSWER> and end them with </ANSWER>, as shown in the following example: \n",
      "<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\n",
      " (A) Ampicillin\n",
      " (B) Ceftriaxone\n",
      " (C) Ciprofloxacin\n",
      " (D) Doxycycline\n",
      " (E) Nitrofurantoin</QUESTION>\n",
      "<ANSWER> (E) Nitrofurantoin</ANSWER>\n",
      "<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\n",
      " (A) Ampicillin\n",
      " (B) Ceftriaxone\n",
      " (C) Ciprofloxacin\n",
      " (D) Doxycycline\n",
      " (E) Nitrofurantoin</QUESTION>\n",
      "<ANSWER> (E) Nitrofurantoin</ANSWER>\n"
     ]
    }
   ],
   "source": [
    "def create_prompt_no_answer(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"<QUESTION>{question} {option_string}</QUESTION><ANSWER> \").format_map(row)\n",
    "\n",
    "def return_prompt_no_answer(row):\n",
    "    return {\"text\": create_prompt_no_answer(row)}\n",
    "\n",
    "def return_prompt(row):\n",
    "    return {\"text\": create_prompt(row)}\n",
    "    \n",
    "if benchmark == \"MedQA\":\n",
    "    test_dataset = eval_dataset.map(return_prompt_no_answer)\n",
    "train_dataset_with_texts = train_dataset.map(return_prompt)\n",
    "print(train_dataset_with_texts[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be57c0",
   "metadata": {
    "papermill": {
     "duration": 0.002456,
     "end_time": "2024-03-18T19:05:07.845316",
     "exception": false,
     "start_time": "2024-03-18T19:05:07.842860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37d2f1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:07.850812Z",
     "iopub.status.busy": "2024-03-18T19:05:07.850700Z",
     "iopub.status.idle": "2024-03-18T19:05:22.598328Z",
     "shell.execute_reply": "2024-03-18T19:05:22.597840Z"
    },
    "papermill": {
     "duration": 14.751631,
     "end_time": "2024-03-18T19:05:22.599410",
     "exception": false,
     "start_time": "2024-03-18T19:05:07.847779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                    | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                             | 1/3 [00:03<00:07,  3.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 2/3 [00:05<00:02,  2.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from utilities.biollama import BioLlama\n",
    "import torch\n",
    "\n",
    "amended_questions = [\"The main calcium pump of the sarcoplasmic reticulum is \"]\n",
    "# answers = [\"Sarcoplasmic reticulum Ca(2+)-ATPase\"] # or \"SERCA\",\"serca2\"\n",
    "prompt = amended_questions[0]\n",
    "model_id = \"meta-llama/Llama-2-\" + size +\"b-chat-hf\"\n",
    "chunk_length = 32\n",
    "\n",
    "BioLlama = BioLlama(\n",
    "    model_id=model_id,\n",
    "    chunk_length=chunk_length,\n",
    "    RETRO_layer_ids=RETRO_layer_ids,\n",
    "    training=True,\n",
    "    torch_dtype=torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76e11ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:22.608781Z",
     "iopub.status.busy": "2024-03-18T19:05:22.608552Z",
     "iopub.status.idle": "2024-03-18T19:05:22.610538Z",
     "shell.execute_reply": "2024-03-18T19:05:22.610272Z"
    },
    "papermill": {
     "duration": 0.007425,
     "end_time": "2024-03-18T19:05:22.611327",
     "exception": false,
     "start_time": "2024-03-18T19:05:22.603902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BioLlama.model\n",
    "tokenizer = BioLlama.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:22.619999Z",
     "iopub.status.busy": "2024-03-18T19:05:22.619900Z",
     "iopub.status.idle": "2024-03-18T19:05:22.626647Z",
     "shell.execute_reply": "2024-03-18T19:05:22.626370Z"
    },
    "papermill": {
     "duration": 0.012025,
     "end_time": "2024-03-18T19:05:22.627431",
     "exception": false,
     "start_time": "2024-03-18T19:05:22.615406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freezing layers, currently only works for single unfrozen retro layer\n",
      "\n",
      "printing layer 19 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "cca_attn.q_proj.weight, requires_grad = False\n",
      "cca_attn.k_proj.weight, requires_grad = False\n",
      "cca_attn.v_proj.weight, requires_grad = False\n",
      "cca_attn.o_proj.weight, requires_grad = False\n",
      "pre_cca_layernorm.weight, requires_grad = False\n",
      "\n",
      "printing layer 19 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "cca_attn.q_proj.weight, requires_grad = True\n",
      "cca_attn.k_proj.weight, requires_grad = True\n",
      "cca_attn.v_proj.weight, requires_grad = True\n",
      "cca_attn.o_proj.weight, requires_grad = True\n",
      "pre_cca_layernorm.weight, requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "print(\"freezing layers, currently only works for single unfrozen retro layer\")\n",
    "n_freeze = BioLlama.RETRO_layer_ids[0]\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): \n",
    "    param.requires_grad = True\n",
    "#for every parameter in retro_layer_params, print where in the model it comes from (ie is it from self attention, layer norm, etc)\n",
    "print(f\"\\nprinting layer {n_freeze} params\")\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")   \n",
    "\n",
    "list_of_params_to_unfreeze = [\n",
    "    \"cca_attn.q_proj.weight\",\n",
    "    \"cca_attn.k_proj.weight\",\n",
    "    \"cca_attn.v_proj.weight\",\n",
    "    \"cca_attn.o_proj.weight\",\n",
    "    \"pre_cca_layernorm.weight\",\n",
    "]\n",
    "\n",
    "for name, param in model.model.layers[n_freeze].named_parameters(): \n",
    "    if name in list_of_params_to_unfreeze:\n",
    "        param.requires_grad = True\n",
    "print(f\"\\nprinting layer {n_freeze} params\")\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15a29820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:22.636331Z",
     "iopub.status.busy": "2024-03-18T19:05:22.636118Z",
     "iopub.status.idle": "2024-03-18T19:05:22.666857Z",
     "shell.execute_reply": "2024-03-18T19:05:22.666617Z"
    },
    "papermill": {
     "duration": 0.035999,
     "end_time": "2024-03-18T19:05:22.667630",
     "exception": false,
     "start_time": "2024-03-18T19:05:22.631631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight, requires_grad = False\n",
      "layers.0.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.0.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.0.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.0.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.0.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.0.mlp.up_proj.weight, requires_grad = False\n",
      "layers.0.mlp.down_proj.weight, requires_grad = False\n",
      "layers.0.input_layernorm.weight, requires_grad = False\n",
      "layers.0.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.1.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.1.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.1.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.1.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.1.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.1.mlp.up_proj.weight, requires_grad = False\n",
      "layers.1.mlp.down_proj.weight, requires_grad = False\n",
      "layers.1.input_layernorm.weight, requires_grad = False\n",
      "layers.1.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.2.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.2.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.2.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.2.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.2.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.2.mlp.up_proj.weight, requires_grad = False\n",
      "layers.2.mlp.down_proj.weight, requires_grad = False\n",
      "layers.2.input_layernorm.weight, requires_grad = False\n",
      "layers.2.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.3.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.3.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.3.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.3.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.3.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.3.mlp.up_proj.weight, requires_grad = False\n",
      "layers.3.mlp.down_proj.weight, requires_grad = False\n",
      "layers.3.input_layernorm.weight, requires_grad = False\n",
      "layers.3.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.4.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.4.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.4.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.4.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.4.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.4.mlp.up_proj.weight, requires_grad = False\n",
      "layers.4.mlp.down_proj.weight, requires_grad = False\n",
      "layers.4.input_layernorm.weight, requires_grad = False\n",
      "layers.4.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.5.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.5.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.5.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.5.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.5.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.5.mlp.up_proj.weight, requires_grad = False\n",
      "layers.5.mlp.down_proj.weight, requires_grad = False\n",
      "layers.5.input_layernorm.weight, requires_grad = False\n",
      "layers.5.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.6.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.6.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.6.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.6.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.6.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.6.mlp.up_proj.weight, requires_grad = False\n",
      "layers.6.mlp.down_proj.weight, requires_grad = False\n",
      "layers.6.input_layernorm.weight, requires_grad = False\n",
      "layers.6.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.7.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.7.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.7.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.7.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.7.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.7.mlp.up_proj.weight, requires_grad = False\n",
      "layers.7.mlp.down_proj.weight, requires_grad = False\n",
      "layers.7.input_layernorm.weight, requires_grad = False\n",
      "layers.7.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.8.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.8.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.8.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.8.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.8.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.8.mlp.up_proj.weight, requires_grad = False\n",
      "layers.8.mlp.down_proj.weight, requires_grad = False\n",
      "layers.8.input_layernorm.weight, requires_grad = False\n",
      "layers.8.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.9.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.9.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.9.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.9.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.9.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.9.mlp.up_proj.weight, requires_grad = False\n",
      "layers.9.mlp.down_proj.weight, requires_grad = False\n",
      "layers.9.input_layernorm.weight, requires_grad = False\n",
      "layers.9.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.10.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.10.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.10.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.10.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.10.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.10.mlp.up_proj.weight, requires_grad = False\n",
      "layers.10.mlp.down_proj.weight, requires_grad = False\n",
      "layers.10.input_layernorm.weight, requires_grad = False\n",
      "layers.10.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.11.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.11.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.11.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.11.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.11.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.11.mlp.up_proj.weight, requires_grad = False\n",
      "layers.11.mlp.down_proj.weight, requires_grad = False\n",
      "layers.11.input_layernorm.weight, requires_grad = False\n",
      "layers.11.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.12.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.12.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.12.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.12.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.12.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.12.mlp.up_proj.weight, requires_grad = False\n",
      "layers.12.mlp.down_proj.weight, requires_grad = False\n",
      "layers.12.input_layernorm.weight, requires_grad = False\n",
      "layers.12.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.13.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.13.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.13.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.13.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.13.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.13.mlp.up_proj.weight, requires_grad = False\n",
      "layers.13.mlp.down_proj.weight, requires_grad = False\n",
      "layers.13.input_layernorm.weight, requires_grad = False\n",
      "layers.13.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.14.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.14.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.14.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.14.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.14.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.14.mlp.up_proj.weight, requires_grad = False\n",
      "layers.14.mlp.down_proj.weight, requires_grad = False\n",
      "layers.14.input_layernorm.weight, requires_grad = False\n",
      "layers.14.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.15.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.15.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.15.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.15.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.15.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.15.mlp.up_proj.weight, requires_grad = False\n",
      "layers.15.mlp.down_proj.weight, requires_grad = False\n",
      "layers.15.input_layernorm.weight, requires_grad = False\n",
      "layers.15.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.16.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.16.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.16.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.16.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.16.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.16.mlp.up_proj.weight, requires_grad = False\n",
      "layers.16.mlp.down_proj.weight, requires_grad = False\n",
      "layers.16.input_layernorm.weight, requires_grad = False\n",
      "layers.16.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.17.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.17.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.17.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.17.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.17.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.17.mlp.up_proj.weight, requires_grad = False\n",
      "layers.17.mlp.down_proj.weight, requires_grad = False\n",
      "layers.17.input_layernorm.weight, requires_grad = False\n",
      "layers.17.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.18.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.18.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.18.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.18.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.18.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.18.mlp.up_proj.weight, requires_grad = False\n",
      "layers.18.mlp.down_proj.weight, requires_grad = False\n",
      "layers.18.input_layernorm.weight, requires_grad = False\n",
      "layers.18.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.19.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.19.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.19.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.19.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.19.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.19.mlp.up_proj.weight, requires_grad = False\n",
      "layers.19.mlp.down_proj.weight, requires_grad = False\n",
      "layers.19.input_layernorm.weight, requires_grad = False\n",
      "layers.19.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.19.cca_attn.q_proj.weight, requires_grad = True\n",
      "layers.19.cca_attn.k_proj.weight, requires_grad = True\n",
      "layers.19.cca_attn.v_proj.weight, requires_grad = True\n",
      "layers.19.cca_attn.o_proj.weight, requires_grad = True\n",
      "layers.19.pre_cca_layernorm.weight, requires_grad = True\n",
      "layers.20.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.20.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.20.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.20.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.20.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.20.mlp.up_proj.weight, requires_grad = False\n",
      "layers.20.mlp.down_proj.weight, requires_grad = False\n",
      "layers.20.input_layernorm.weight, requires_grad = False\n",
      "layers.20.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.21.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.21.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.21.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.21.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.21.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.21.mlp.up_proj.weight, requires_grad = False\n",
      "layers.21.mlp.down_proj.weight, requires_grad = False\n",
      "layers.21.input_layernorm.weight, requires_grad = False\n",
      "layers.21.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.22.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.22.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.22.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.22.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.22.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.22.mlp.up_proj.weight, requires_grad = False\n",
      "layers.22.mlp.down_proj.weight, requires_grad = False\n",
      "layers.22.input_layernorm.weight, requires_grad = False\n",
      "layers.22.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.23.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.23.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.23.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.23.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.23.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.23.mlp.up_proj.weight, requires_grad = False\n",
      "layers.23.mlp.down_proj.weight, requires_grad = False\n",
      "layers.23.input_layernorm.weight, requires_grad = False\n",
      "layers.23.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.24.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.24.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.24.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.24.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.24.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.24.mlp.up_proj.weight, requires_grad = False\n",
      "layers.24.mlp.down_proj.weight, requires_grad = False\n",
      "layers.24.input_layernorm.weight, requires_grad = False\n",
      "layers.24.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.25.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.25.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.25.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.25.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.25.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.25.mlp.up_proj.weight, requires_grad = False\n",
      "layers.25.mlp.down_proj.weight, requires_grad = False\n",
      "layers.25.input_layernorm.weight, requires_grad = False\n",
      "layers.25.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.26.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.26.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.26.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.26.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.26.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.26.mlp.up_proj.weight, requires_grad = False\n",
      "layers.26.mlp.down_proj.weight, requires_grad = False\n",
      "layers.26.input_layernorm.weight, requires_grad = False\n",
      "layers.26.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.27.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.27.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.27.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.27.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.27.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.27.mlp.up_proj.weight, requires_grad = False\n",
      "layers.27.mlp.down_proj.weight, requires_grad = False\n",
      "layers.27.input_layernorm.weight, requires_grad = False\n",
      "layers.27.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.28.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.28.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.28.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.28.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.28.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.28.mlp.up_proj.weight, requires_grad = False\n",
      "layers.28.mlp.down_proj.weight, requires_grad = False\n",
      "layers.28.input_layernorm.weight, requires_grad = False\n",
      "layers.28.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.29.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.29.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.29.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.29.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.29.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.29.mlp.up_proj.weight, requires_grad = False\n",
      "layers.29.mlp.down_proj.weight, requires_grad = False\n",
      "layers.29.input_layernorm.weight, requires_grad = False\n",
      "layers.29.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.30.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.30.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.30.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.30.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.30.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.30.mlp.up_proj.weight, requires_grad = False\n",
      "layers.30.mlp.down_proj.weight, requires_grad = False\n",
      "layers.30.input_layernorm.weight, requires_grad = False\n",
      "layers.30.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.31.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.31.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.31.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.31.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.31.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.31.mlp.up_proj.weight, requires_grad = False\n",
      "layers.31.mlp.down_proj.weight, requires_grad = False\n",
      "layers.31.input_layernorm.weight, requires_grad = False\n",
      "layers.31.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.32.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.32.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.32.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.32.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.32.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.32.mlp.up_proj.weight, requires_grad = False\n",
      "layers.32.mlp.down_proj.weight, requires_grad = False\n",
      "layers.32.input_layernorm.weight, requires_grad = False\n",
      "layers.32.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.33.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.33.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.33.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.33.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.33.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.33.mlp.up_proj.weight, requires_grad = False\n",
      "layers.33.mlp.down_proj.weight, requires_grad = False\n",
      "layers.33.input_layernorm.weight, requires_grad = False\n",
      "layers.33.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.34.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.34.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.34.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.34.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.34.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.34.mlp.up_proj.weight, requires_grad = False\n",
      "layers.34.mlp.down_proj.weight, requires_grad = False\n",
      "layers.34.input_layernorm.weight, requires_grad = False\n",
      "layers.34.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.35.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.35.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.35.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.35.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.35.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.35.mlp.up_proj.weight, requires_grad = False\n",
      "layers.35.mlp.down_proj.weight, requires_grad = False\n",
      "layers.35.input_layernorm.weight, requires_grad = False\n",
      "layers.35.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.36.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.36.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.36.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.36.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.36.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.36.mlp.up_proj.weight, requires_grad = False\n",
      "layers.36.mlp.down_proj.weight, requires_grad = False\n",
      "layers.36.input_layernorm.weight, requires_grad = False\n",
      "layers.36.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.37.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.37.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.37.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.37.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.37.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.37.mlp.up_proj.weight, requires_grad = False\n",
      "layers.37.mlp.down_proj.weight, requires_grad = False\n",
      "layers.37.input_layernorm.weight, requires_grad = False\n",
      "layers.37.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.38.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.38.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.38.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.38.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.38.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.38.mlp.up_proj.weight, requires_grad = False\n",
      "layers.38.mlp.down_proj.weight, requires_grad = False\n",
      "layers.38.input_layernorm.weight, requires_grad = False\n",
      "layers.38.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.39.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.39.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.39.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.39.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.39.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.39.mlp.up_proj.weight, requires_grad = False\n",
      "layers.39.mlp.down_proj.weight, requires_grad = False\n",
      "layers.39.input_layernorm.weight, requires_grad = False\n",
      "layers.39.post_attention_layernorm.weight, requires_grad = False\n",
      "norm.weight, requires_grad = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120)\n",
       "    (layers): ModuleList(\n",
       "      (0-18): 19 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "        (cca_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (pre_cca_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (20-39): 20 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, param in model.model.named_parameters(): \n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")\n",
    "    # param.requires_grad = True\n",
    "\n",
    "BioLlama.model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:22.678187Z",
     "iopub.status.busy": "2024-03-18T19:05:22.677960Z",
     "iopub.status.idle": "2024-03-18T19:05:22.679915Z",
     "shell.execute_reply": "2024-03-18T19:05:22.679614Z"
    },
    "papermill": {
     "duration": 0.008464,
     "end_time": "2024-03-18T19:05:22.680774",
     "exception": false,
     "start_time": "2024-03-18T19:05:22.672310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "model.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:22.690509Z",
     "iopub.status.busy": "2024-03-18T19:05:22.690413Z",
     "iopub.status.idle": "2024-03-18T19:05:22.693784Z",
     "shell.execute_reply": "2024-03-18T19:05:22.693504Z"
    },
    "papermill": {
     "duration": 0.009098,
     "end_time": "2024-03-18T19:05:22.694582",
     "exception": false,
     "start_time": "2024-03-18T19:05:22.685484",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 13120.73M, Trainable: 268.70M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c86339ca-27e5-496b-9559-3a65bb3c26a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:22.704494Z",
     "iopub.status.busy": "2024-03-18T19:05:22.704227Z",
     "iopub.status.idle": "2024-03-18T19:05:22.712269Z",
     "shell.execute_reply": "2024-03-18T19:05:22.712016Z"
    },
    "papermill": {
     "duration": 0.013851,
     "end_time": "2024-03-18T19:05:22.713067",
     "exception": false,
     "start_time": "2024-03-18T19:05:22.699216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\" + size  + \"/\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size//2,\n",
    "    bf16=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=total_num_steps // 10,\n",
    "    num_train_epochs=2,\n",
    "    max_steps=total_num_steps,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=5000,\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=8,\n",
    "    save_strategy=\"epoch\", #changed to epoch so we save every epoch i guess?\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e35a9b70-f36b-4bfd-857f-c80d3450e111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:22.723004Z",
     "iopub.status.busy": "2024-03-18T19:05:22.722792Z",
     "iopub.status.idle": "2024-03-18T19:05:22.815657Z",
     "shell.execute_reply": "2024-03-18T19:05:22.815279Z"
    },
    "papermill": {
     "duration": 0.098718,
     "end_time": "2024-03-18T19:05:22.816483",
     "exception": false,
     "start_time": "2024-03-18T19:05:22.717765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "import trl\n",
    "# from utilities.finetuning.sft_trainer import SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset_with_texts,\n",
    "    dataset_text_field=\"text\",\n",
    "    # eval_dataset=test_dataset,\n",
    "    packing=True,\n",
    "    max_seq_length=1024,\n",
    "    args=training_args,\n",
    "    formatting_func=create_prompt,\n",
    "    # compute_metrics=token_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5754789-2e15-4bc9-800c-01f8ffc625e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T19:05:22.826292Z",
     "iopub.status.busy": "2024-03-18T19:05:22.826184Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-03-18T19:05:22.821075",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5609' max='20356' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5609/20356 44:20 < 1:56:37, 2.11 it/s, Epoch 1.10/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.169200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.191200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.124600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.145400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.120900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.135100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>1.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.085900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>1.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>1.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>1.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.977100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.970700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.920100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.939900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.918100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.997200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.919200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.880300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.916200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.898700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.867600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>0.813100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>0.823100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.855700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>0.854600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.835500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>0.799900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.765700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>0.818200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>0.762900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.803200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.761100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>0.807600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>0.770200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>0.738000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.771000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>0.807900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>0.781200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.817400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.795900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.767600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>0.731400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>0.779700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.787800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>0.742000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0.801700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>0.740600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>0.767100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.706400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>0.761700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>0.726900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>0.723600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>0.710800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.730400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>0.727200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>0.734700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>0.778600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>0.691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.715300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>0.764800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>0.713300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>0.727700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>0.698600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.675600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>0.695600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>0.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>0.724100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>0.729900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.724700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>0.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>0.759300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>0.706600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>0.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.684700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>0.771000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>0.743200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>0.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.693400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>848</td>\n",
       "      <td>0.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>0.793500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>0.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>0.705200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.653300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>0.734900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>904</td>\n",
       "      <td>0.714400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912</td>\n",
       "      <td>0.728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.704500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>0.695600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>0.644600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>0.677300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>0.710100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.743800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>968</td>\n",
       "      <td>0.723600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>976</td>\n",
       "      <td>0.707400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>984</td>\n",
       "      <td>0.692800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>992</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>0.728700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1016</td>\n",
       "      <td>0.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>0.705700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1032</td>\n",
       "      <td>0.704700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048</td>\n",
       "      <td>0.657800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1056</td>\n",
       "      <td>0.709500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1064</td>\n",
       "      <td>0.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.725500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1088</td>\n",
       "      <td>0.672700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1096</td>\n",
       "      <td>0.678500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1104</td>\n",
       "      <td>0.646800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1112</td>\n",
       "      <td>0.685600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1128</td>\n",
       "      <td>0.706600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1136</td>\n",
       "      <td>0.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1144</td>\n",
       "      <td>0.680300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1152</td>\n",
       "      <td>0.707700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.665600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1168</td>\n",
       "      <td>0.649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1176</td>\n",
       "      <td>0.672100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1184</td>\n",
       "      <td>0.689300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1192</td>\n",
       "      <td>0.682500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1208</td>\n",
       "      <td>0.640200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1216</td>\n",
       "      <td>0.676300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1224</td>\n",
       "      <td>0.655800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1232</td>\n",
       "      <td>0.651200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.699200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1248</td>\n",
       "      <td>0.617100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1256</td>\n",
       "      <td>0.645900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1264</td>\n",
       "      <td>0.688500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1272</td>\n",
       "      <td>0.715800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.631200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1288</td>\n",
       "      <td>0.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1296</td>\n",
       "      <td>0.630100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1304</td>\n",
       "      <td>0.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1312</td>\n",
       "      <td>0.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1328</td>\n",
       "      <td>0.658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1336</td>\n",
       "      <td>0.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1344</td>\n",
       "      <td>0.654400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1352</td>\n",
       "      <td>0.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.638300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1368</td>\n",
       "      <td>0.704300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1376</td>\n",
       "      <td>0.626800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1384</td>\n",
       "      <td>0.625800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1392</td>\n",
       "      <td>0.663300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.663400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1408</td>\n",
       "      <td>0.653800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1416</td>\n",
       "      <td>0.727300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1424</td>\n",
       "      <td>0.667700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1432</td>\n",
       "      <td>0.626400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1448</td>\n",
       "      <td>0.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1456</td>\n",
       "      <td>0.684200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1464</td>\n",
       "      <td>0.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1472</td>\n",
       "      <td>0.650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.661300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1488</td>\n",
       "      <td>0.659600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1496</td>\n",
       "      <td>0.624900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1504</td>\n",
       "      <td>0.620800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1512</td>\n",
       "      <td>0.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.650100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1528</td>\n",
       "      <td>0.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1536</td>\n",
       "      <td>0.580700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1544</td>\n",
       "      <td>0.626100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1552</td>\n",
       "      <td>0.598800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.644100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1568</td>\n",
       "      <td>0.599200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1576</td>\n",
       "      <td>0.641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1584</td>\n",
       "      <td>0.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1592</td>\n",
       "      <td>0.655600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.652700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1608</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1616</td>\n",
       "      <td>0.624100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1624</td>\n",
       "      <td>0.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1632</td>\n",
       "      <td>0.659700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.647800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1648</td>\n",
       "      <td>0.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1656</td>\n",
       "      <td>0.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1664</td>\n",
       "      <td>0.639800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1672</td>\n",
       "      <td>0.656100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1688</td>\n",
       "      <td>0.650400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1696</td>\n",
       "      <td>0.722600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1704</td>\n",
       "      <td>0.616600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1712</td>\n",
       "      <td>0.616200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.648600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1728</td>\n",
       "      <td>0.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1736</td>\n",
       "      <td>0.685300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1744</td>\n",
       "      <td>0.669800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1752</td>\n",
       "      <td>0.627700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1768</td>\n",
       "      <td>0.644700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1776</td>\n",
       "      <td>0.632500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1784</td>\n",
       "      <td>0.595800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1792</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.640800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1808</td>\n",
       "      <td>0.580300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1816</td>\n",
       "      <td>0.615800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1824</td>\n",
       "      <td>0.659300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1832</td>\n",
       "      <td>0.653100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1848</td>\n",
       "      <td>0.699400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1856</td>\n",
       "      <td>0.622300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>0.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1872</td>\n",
       "      <td>0.600200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.669500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1888</td>\n",
       "      <td>0.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1896</td>\n",
       "      <td>0.595800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1904</td>\n",
       "      <td>0.613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1912</td>\n",
       "      <td>0.707600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1928</td>\n",
       "      <td>0.662300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1936</td>\n",
       "      <td>0.602800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1944</td>\n",
       "      <td>0.661700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1952</td>\n",
       "      <td>0.631800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1968</td>\n",
       "      <td>0.677700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1976</td>\n",
       "      <td>0.640900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1984</td>\n",
       "      <td>0.700300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1992</td>\n",
       "      <td>0.661800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.647300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2008</td>\n",
       "      <td>0.624200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016</td>\n",
       "      <td>0.657300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2024</td>\n",
       "      <td>0.623200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2032</td>\n",
       "      <td>0.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.628700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2048</td>\n",
       "      <td>0.591000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2056</td>\n",
       "      <td>0.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2064</td>\n",
       "      <td>0.611300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2072</td>\n",
       "      <td>0.598200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.660700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2088</td>\n",
       "      <td>0.639100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2096</td>\n",
       "      <td>0.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2104</td>\n",
       "      <td>0.598600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2112</td>\n",
       "      <td>0.692800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2128</td>\n",
       "      <td>0.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2136</td>\n",
       "      <td>0.619000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2144</td>\n",
       "      <td>0.697600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2152</td>\n",
       "      <td>0.653700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2168</td>\n",
       "      <td>0.632900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2176</td>\n",
       "      <td>0.575700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2184</td>\n",
       "      <td>0.642600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2192</td>\n",
       "      <td>0.629900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.621500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2208</td>\n",
       "      <td>0.638300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2216</td>\n",
       "      <td>0.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2224</td>\n",
       "      <td>0.589400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2232</td>\n",
       "      <td>0.673900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.679300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2248</td>\n",
       "      <td>0.633900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2256</td>\n",
       "      <td>0.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2264</td>\n",
       "      <td>0.689300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2272</td>\n",
       "      <td>0.629600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2288</td>\n",
       "      <td>0.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2296</td>\n",
       "      <td>0.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2304</td>\n",
       "      <td>0.617900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2312</td>\n",
       "      <td>0.625300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.633400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2328</td>\n",
       "      <td>0.619200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2336</td>\n",
       "      <td>0.615500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2344</td>\n",
       "      <td>0.643100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2352</td>\n",
       "      <td>0.633300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2368</td>\n",
       "      <td>0.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2376</td>\n",
       "      <td>0.574900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2384</td>\n",
       "      <td>0.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2392</td>\n",
       "      <td>0.626900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2408</td>\n",
       "      <td>0.606700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2416</td>\n",
       "      <td>0.661700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2424</td>\n",
       "      <td>0.607200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2432</td>\n",
       "      <td>0.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.627700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2448</td>\n",
       "      <td>0.604900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2456</td>\n",
       "      <td>0.637200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2464</td>\n",
       "      <td>0.656500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2472</td>\n",
       "      <td>0.664700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2488</td>\n",
       "      <td>0.667500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2496</td>\n",
       "      <td>0.661200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2504</td>\n",
       "      <td>0.627400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2512</td>\n",
       "      <td>0.638100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.604400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2528</td>\n",
       "      <td>0.633200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2536</td>\n",
       "      <td>0.618100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2544</td>\n",
       "      <td>0.596100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2552</td>\n",
       "      <td>0.583100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.621800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2568</td>\n",
       "      <td>0.644400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2576</td>\n",
       "      <td>0.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2584</td>\n",
       "      <td>0.648100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2592</td>\n",
       "      <td>0.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.587800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2608</td>\n",
       "      <td>0.654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2616</td>\n",
       "      <td>0.622500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2624</td>\n",
       "      <td>0.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2632</td>\n",
       "      <td>0.628100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.566100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2648</td>\n",
       "      <td>0.673500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2656</td>\n",
       "      <td>0.620100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2664</td>\n",
       "      <td>0.609300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2672</td>\n",
       "      <td>0.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.621900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2688</td>\n",
       "      <td>0.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2696</td>\n",
       "      <td>0.560300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2704</td>\n",
       "      <td>0.618100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2712</td>\n",
       "      <td>0.586400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.632800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2728</td>\n",
       "      <td>0.626400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2736</td>\n",
       "      <td>0.640400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2744</td>\n",
       "      <td>0.650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2752</td>\n",
       "      <td>0.556900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2768</td>\n",
       "      <td>0.625100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2776</td>\n",
       "      <td>0.630600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2784</td>\n",
       "      <td>0.627400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2792</td>\n",
       "      <td>0.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.643600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2808</td>\n",
       "      <td>0.632900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2816</td>\n",
       "      <td>0.632400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2824</td>\n",
       "      <td>0.613400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2832</td>\n",
       "      <td>0.598700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2848</td>\n",
       "      <td>0.605600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2856</td>\n",
       "      <td>0.640800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2864</td>\n",
       "      <td>0.582900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2872</td>\n",
       "      <td>0.641600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2888</td>\n",
       "      <td>0.638700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2896</td>\n",
       "      <td>0.569100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2904</td>\n",
       "      <td>0.594000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2912</td>\n",
       "      <td>0.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.612700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2928</td>\n",
       "      <td>0.620400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2936</td>\n",
       "      <td>0.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2944</td>\n",
       "      <td>0.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2952</td>\n",
       "      <td>0.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.634100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2968</td>\n",
       "      <td>0.588400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2976</td>\n",
       "      <td>0.616100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2984</td>\n",
       "      <td>0.589600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2992</td>\n",
       "      <td>0.576800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.599800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3008</td>\n",
       "      <td>0.584300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3016</td>\n",
       "      <td>0.620600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3024</td>\n",
       "      <td>0.574400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3032</td>\n",
       "      <td>0.586900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.591000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3048</td>\n",
       "      <td>0.582100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3056</td>\n",
       "      <td>0.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3064</td>\n",
       "      <td>0.633900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3072</td>\n",
       "      <td>0.616800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.612600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3088</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3096</td>\n",
       "      <td>0.629700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3104</td>\n",
       "      <td>0.608600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3112</td>\n",
       "      <td>0.594200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.639600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3128</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3136</td>\n",
       "      <td>0.575500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3144</td>\n",
       "      <td>0.566600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3152</td>\n",
       "      <td>0.584700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>0.606500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3168</td>\n",
       "      <td>0.626200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3176</td>\n",
       "      <td>0.623300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3184</td>\n",
       "      <td>0.608800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3192</td>\n",
       "      <td>0.583700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.582400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3208</td>\n",
       "      <td>0.591400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3216</td>\n",
       "      <td>0.603300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3224</td>\n",
       "      <td>0.562200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3232</td>\n",
       "      <td>0.630500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3248</td>\n",
       "      <td>0.655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3256</td>\n",
       "      <td>0.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3264</td>\n",
       "      <td>0.583200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3272</td>\n",
       "      <td>0.653900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.623500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3288</td>\n",
       "      <td>0.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3296</td>\n",
       "      <td>0.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3304</td>\n",
       "      <td>0.616100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3312</td>\n",
       "      <td>0.579200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>0.597800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3328</td>\n",
       "      <td>0.601300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3336</td>\n",
       "      <td>0.622700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3344</td>\n",
       "      <td>0.609800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3352</td>\n",
       "      <td>0.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>0.610100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3368</td>\n",
       "      <td>0.647100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3376</td>\n",
       "      <td>0.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3384</td>\n",
       "      <td>0.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3392</td>\n",
       "      <td>0.578300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.568300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3408</td>\n",
       "      <td>0.568900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3416</td>\n",
       "      <td>0.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3424</td>\n",
       "      <td>0.597100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3432</td>\n",
       "      <td>0.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>0.622400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3448</td>\n",
       "      <td>0.625100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3456</td>\n",
       "      <td>0.602000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3464</td>\n",
       "      <td>0.605300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3472</td>\n",
       "      <td>0.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>0.637800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3488</td>\n",
       "      <td>0.604400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3496</td>\n",
       "      <td>0.577500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3504</td>\n",
       "      <td>0.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3512</td>\n",
       "      <td>0.621500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>0.590400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3528</td>\n",
       "      <td>0.579900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3536</td>\n",
       "      <td>0.546700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3544</td>\n",
       "      <td>0.539700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3552</td>\n",
       "      <td>0.569600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>0.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3568</td>\n",
       "      <td>0.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3576</td>\n",
       "      <td>0.607600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3584</td>\n",
       "      <td>0.544900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3592</td>\n",
       "      <td>0.578800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3608</td>\n",
       "      <td>0.521600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3616</td>\n",
       "      <td>0.612800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3624</td>\n",
       "      <td>0.548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3632</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>0.555400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3648</td>\n",
       "      <td>0.577200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3656</td>\n",
       "      <td>0.564600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3664</td>\n",
       "      <td>0.524200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3672</td>\n",
       "      <td>0.553500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>0.549000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3688</td>\n",
       "      <td>0.547200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3696</td>\n",
       "      <td>0.582100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3704</td>\n",
       "      <td>0.523300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3712</td>\n",
       "      <td>0.617600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>0.592200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3728</td>\n",
       "      <td>0.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3736</td>\n",
       "      <td>0.592700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3744</td>\n",
       "      <td>0.600900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3752</td>\n",
       "      <td>0.521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>0.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3768</td>\n",
       "      <td>0.613800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3776</td>\n",
       "      <td>0.595400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3784</td>\n",
       "      <td>0.557500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3792</td>\n",
       "      <td>0.616300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.568400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3808</td>\n",
       "      <td>0.547800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3816</td>\n",
       "      <td>0.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3824</td>\n",
       "      <td>0.552600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3832</td>\n",
       "      <td>0.553200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>0.608400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3848</td>\n",
       "      <td>0.614500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3856</td>\n",
       "      <td>0.608500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3864</td>\n",
       "      <td>0.529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3872</td>\n",
       "      <td>0.559800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>0.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3888</td>\n",
       "      <td>0.539200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3896</td>\n",
       "      <td>0.533100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3904</td>\n",
       "      <td>0.564700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3912</td>\n",
       "      <td>0.540400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>0.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3928</td>\n",
       "      <td>0.546500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3936</td>\n",
       "      <td>0.520600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3944</td>\n",
       "      <td>0.576800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3952</td>\n",
       "      <td>0.558100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>0.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3968</td>\n",
       "      <td>0.535500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3976</td>\n",
       "      <td>0.572100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3984</td>\n",
       "      <td>0.582300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3992</td>\n",
       "      <td>0.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.615500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4008</td>\n",
       "      <td>0.555000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4016</td>\n",
       "      <td>0.572200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4024</td>\n",
       "      <td>0.572300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4032</td>\n",
       "      <td>0.554900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>0.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4048</td>\n",
       "      <td>0.539500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4056</td>\n",
       "      <td>0.581700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4064</td>\n",
       "      <td>0.594800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4072</td>\n",
       "      <td>0.555300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>0.571400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4088</td>\n",
       "      <td>0.598200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4096</td>\n",
       "      <td>0.525300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4104</td>\n",
       "      <td>0.595000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4112</td>\n",
       "      <td>0.553400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>0.554200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4128</td>\n",
       "      <td>0.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4136</td>\n",
       "      <td>0.529700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4144</td>\n",
       "      <td>0.559900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4152</td>\n",
       "      <td>0.560400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>0.507100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4168</td>\n",
       "      <td>0.529200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4176</td>\n",
       "      <td>0.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4184</td>\n",
       "      <td>0.532800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4192</td>\n",
       "      <td>0.551100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.514600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4208</td>\n",
       "      <td>0.506900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4216</td>\n",
       "      <td>0.478400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4224</td>\n",
       "      <td>0.517800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4232</td>\n",
       "      <td>0.563500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>0.524700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4248</td>\n",
       "      <td>0.540700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4256</td>\n",
       "      <td>0.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4264</td>\n",
       "      <td>0.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4272</td>\n",
       "      <td>0.499700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280</td>\n",
       "      <td>0.490500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4288</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4296</td>\n",
       "      <td>0.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4304</td>\n",
       "      <td>0.497800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4312</td>\n",
       "      <td>0.559900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>0.527700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4328</td>\n",
       "      <td>0.516100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4336</td>\n",
       "      <td>0.557100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4344</td>\n",
       "      <td>0.533200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4352</td>\n",
       "      <td>0.503300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4360</td>\n",
       "      <td>0.513900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4368</td>\n",
       "      <td>0.497700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4376</td>\n",
       "      <td>0.488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4384</td>\n",
       "      <td>0.535700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4392</td>\n",
       "      <td>0.490600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4408</td>\n",
       "      <td>0.513100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4416</td>\n",
       "      <td>0.511700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4424</td>\n",
       "      <td>0.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4432</td>\n",
       "      <td>0.605700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4440</td>\n",
       "      <td>0.499600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4448</td>\n",
       "      <td>0.551200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4456</td>\n",
       "      <td>0.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4464</td>\n",
       "      <td>0.555800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4472</td>\n",
       "      <td>0.492400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4480</td>\n",
       "      <td>0.536400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4488</td>\n",
       "      <td>0.509800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4496</td>\n",
       "      <td>0.521000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4504</td>\n",
       "      <td>0.535200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4512</td>\n",
       "      <td>0.522200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4520</td>\n",
       "      <td>0.554500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4528</td>\n",
       "      <td>0.555200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4536</td>\n",
       "      <td>0.560700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4544</td>\n",
       "      <td>0.539200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4552</td>\n",
       "      <td>0.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4560</td>\n",
       "      <td>0.529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4568</td>\n",
       "      <td>0.533500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4576</td>\n",
       "      <td>0.484400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4584</td>\n",
       "      <td>0.540200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4592</td>\n",
       "      <td>0.534200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.537600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4608</td>\n",
       "      <td>0.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4616</td>\n",
       "      <td>0.525300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4624</td>\n",
       "      <td>0.564200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4632</td>\n",
       "      <td>0.516000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4640</td>\n",
       "      <td>0.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4648</td>\n",
       "      <td>0.549700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4656</td>\n",
       "      <td>0.522300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4664</td>\n",
       "      <td>0.510100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4672</td>\n",
       "      <td>0.513300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4680</td>\n",
       "      <td>0.565200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4688</td>\n",
       "      <td>0.530800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4696</td>\n",
       "      <td>0.547500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4704</td>\n",
       "      <td>0.583400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4712</td>\n",
       "      <td>0.541800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4720</td>\n",
       "      <td>0.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4728</td>\n",
       "      <td>0.501400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4736</td>\n",
       "      <td>0.502700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4744</td>\n",
       "      <td>0.474400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4752</td>\n",
       "      <td>0.572900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4760</td>\n",
       "      <td>0.498000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4768</td>\n",
       "      <td>0.518500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4776</td>\n",
       "      <td>0.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4784</td>\n",
       "      <td>0.519900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4792</td>\n",
       "      <td>0.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.524100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4808</td>\n",
       "      <td>0.520200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4816</td>\n",
       "      <td>0.486100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4824</td>\n",
       "      <td>0.470800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4832</td>\n",
       "      <td>0.477600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4840</td>\n",
       "      <td>0.552100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4848</td>\n",
       "      <td>0.517200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4856</td>\n",
       "      <td>0.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4864</td>\n",
       "      <td>0.503700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4872</td>\n",
       "      <td>0.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4880</td>\n",
       "      <td>0.500200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4888</td>\n",
       "      <td>0.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4896</td>\n",
       "      <td>0.527600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4904</td>\n",
       "      <td>0.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4912</td>\n",
       "      <td>0.507300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4920</td>\n",
       "      <td>0.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4928</td>\n",
       "      <td>0.488200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4936</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4944</td>\n",
       "      <td>0.556900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4952</td>\n",
       "      <td>0.522600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4960</td>\n",
       "      <td>0.517700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4968</td>\n",
       "      <td>0.529500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4976</td>\n",
       "      <td>0.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4984</td>\n",
       "      <td>0.480900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4992</td>\n",
       "      <td>0.486300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.471300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5008</td>\n",
       "      <td>0.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5016</td>\n",
       "      <td>0.530400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5024</td>\n",
       "      <td>0.497800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5032</td>\n",
       "      <td>0.502100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5040</td>\n",
       "      <td>0.512500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5048</td>\n",
       "      <td>0.518800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5056</td>\n",
       "      <td>0.525300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5064</td>\n",
       "      <td>0.587200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5072</td>\n",
       "      <td>0.515600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5080</td>\n",
       "      <td>0.457400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5088</td>\n",
       "      <td>0.470100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5096</td>\n",
       "      <td>0.505900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5104</td>\n",
       "      <td>0.554600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5112</td>\n",
       "      <td>0.495800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>0.531400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5128</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5136</td>\n",
       "      <td>0.503600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5144</td>\n",
       "      <td>0.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5152</td>\n",
       "      <td>0.510900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5160</td>\n",
       "      <td>0.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5168</td>\n",
       "      <td>0.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5176</td>\n",
       "      <td>0.544400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5184</td>\n",
       "      <td>0.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5192</td>\n",
       "      <td>0.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.497400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5208</td>\n",
       "      <td>0.496200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5216</td>\n",
       "      <td>0.480800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5224</td>\n",
       "      <td>0.549300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5232</td>\n",
       "      <td>0.518800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5240</td>\n",
       "      <td>0.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5248</td>\n",
       "      <td>0.515700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5256</td>\n",
       "      <td>0.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5264</td>\n",
       "      <td>0.553900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5272</td>\n",
       "      <td>0.521000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5280</td>\n",
       "      <td>0.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5288</td>\n",
       "      <td>0.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5296</td>\n",
       "      <td>0.576200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5304</td>\n",
       "      <td>0.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5312</td>\n",
       "      <td>0.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5320</td>\n",
       "      <td>0.489900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5328</td>\n",
       "      <td>0.451500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5336</td>\n",
       "      <td>0.527900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5344</td>\n",
       "      <td>0.510600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5352</td>\n",
       "      <td>0.517800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5360</td>\n",
       "      <td>0.518500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5368</td>\n",
       "      <td>0.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5376</td>\n",
       "      <td>0.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5384</td>\n",
       "      <td>0.522200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5392</td>\n",
       "      <td>0.535100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.501500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5408</td>\n",
       "      <td>0.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5416</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5424</td>\n",
       "      <td>0.484200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5432</td>\n",
       "      <td>0.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5440</td>\n",
       "      <td>0.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5448</td>\n",
       "      <td>0.522200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5456</td>\n",
       "      <td>0.501900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5464</td>\n",
       "      <td>0.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5472</td>\n",
       "      <td>0.542100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5480</td>\n",
       "      <td>0.513400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5488</td>\n",
       "      <td>0.535900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5496</td>\n",
       "      <td>0.518500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5504</td>\n",
       "      <td>0.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5512</td>\n",
       "      <td>0.534900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5520</td>\n",
       "      <td>0.533900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5528</td>\n",
       "      <td>0.442500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5536</td>\n",
       "      <td>0.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5544</td>\n",
       "      <td>0.501400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5552</td>\n",
       "      <td>0.471300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5560</td>\n",
       "      <td>0.495300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5568</td>\n",
       "      <td>0.515700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5576</td>\n",
       "      <td>0.509200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5584</td>\n",
       "      <td>0.522100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5592</td>\n",
       "      <td>0.470300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.551000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/utils.py:268: UserWarning: The dataset reached end and the iterator is reset to the start.\n",
      "  warnings.warn(\"The dataset reached end and the iterator is reset to the start.\")\n"
     ]
    }
   ],
   "source": [
    "#very hacky but maybe this will work:\n",
    "tokenizer.model_input_names = ['labels', 'input_ids', 'attention_mask']\n",
    "# trainer.args.train_batch_size = 1\n",
    "# self.args.train_batch_size\n",
    "\n",
    "#also hacky, but could work:\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Starting training\")\n",
    "trainer.train()\n",
    "\n",
    "wandb.alert(\n",
    "    title=\"Finished training\",\n",
    "    text=f\"We have finished the training run and saved the model.\",\n",
    "    level=AlertLevel.WARN,\n",
    "    wait_duration=300,\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e06dfb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(size)\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\" + benchmark + \"/\" + size  + \"/\"\n",
    "print(RETRO_layer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ca6fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:52:49.215531Z",
     "iopub.status.busy": "2024-01-17T00:52:49.215430Z",
     "iopub.status.idle": "2024-01-17T00:53:05.370183Z",
     "shell.execute_reply": "2024-01-17T00:53:05.369679Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.abspath(output_dir))\n",
    "trainer.save_model(output_dir)\n",
    "# !ls -l $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff2529",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load this local model here and use it to generate some text\n",
    "print(output_dir)\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import time\n",
    "# import torch\n",
    "# from utilities.biollama import BioLlama\n",
    "\n",
    "# chunk_length = 32\n",
    "\n",
    "# BioLlama = BioLlama(model_id=output_dir, \n",
    "#     chunk_length=chunk_length, \n",
    "#     RETRO_layer_ids = RETRO_layer_ids, \n",
    "#     training=False, \n",
    "#     torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f493d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BioLlama.training = False\n",
    "import time\n",
    "prompt  = '<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \\n (A) Ampicillin\\n (B) Ceftriaxone\\n (C) Ciprofloxacin\\n (D) Doxycycline\\n (E) Nitrofurantoin</QUESTION>\\n<ANSWER> '\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99856974",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt2 = '<QUESTION>A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby? \\n (A) Placing the infant in a supine position on a firm mattress while sleeping\\n (B) Routine postnatal electrocardiogram (ECG)\\n (C) Keeping the infant covered and maintaining a high room temperature\\n (D) Application of a device to maintain the sleeping position\\n (E) Avoiding pacifier use during sleep</QUESTION>\\n<ANSWER> '\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt2, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad4c40",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt3 = \"<QUESTION>A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation? \\n (A) Abnormal migration of ventral pancreatic bud\\n (B) Complete failure of proximal duodenum to recanalize\\n (C) Error in neural crest cell migration\\n (D) Abnormal hypertrophy of the pylorus\\n (E) Failure of lateral body folds to move ventrally and fuse in the midline</QUESTION>\\n<ANSWER> \"\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt3, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24725d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt4 = \"<QUESTION>A 20-year-old woman presents with menorrhagia for the past several years. She says that her menses ‚Äúhave always been heavy‚Äù, and she has experienced easy bruising for as long as she can remember. Family history is significant for her mother, who had similar problems with bruising easily. The patient's vital signs include: heart rate 98/min, respiratory rate 14/min, temperature 36.1¬∞C (96.9¬∞F), and blood pressure 110/87 mm Hg. Physical examination is unremarkable. Laboratory tests show the following: platelet count 200,000/mm3, PT 12 seconds, and PTT 43 seconds. Which of the following is the most likely cause of this patient‚Äôs symptoms? \\n (A) Factor V Leiden\\n (B) Hemophilia A\\n (C) Lupus anticoagulant\\n (D) Protein C deficiency\\n (E) Von Willebrand disease</QUESTION>\\n<ANSWER> \"\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt4, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883afe6b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1afa7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "BioLlama_finetuning_WandB_HF.ipynb",
   "output_path": "output_biollama.ipynb",
   "parameters": {},
   "start_time": "2024-03-18T19:05:00.183092",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}