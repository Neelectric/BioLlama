{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb384650",
   "metadata": {
    "papermill": {
     "duration": 0.002551,
     "end_time": "2024-03-04T19:50:10.775578",
     "exception": false,
     "start_time": "2024-03-04T19:50:10.773027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is taken directly from https://github.com/tcapelle/llm_recipes/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
   "metadata": {
    "papermill": {
     "duration": 0.002591,
     "end_time": "2024-03-04T19:50:10.780219",
     "exception": false,
     "start_time": "2024-03-04T19:50:10.777628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Finetuning Llama-2 to produce BioLlama using HF and WanB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:10.784718Z",
     "iopub.status.busy": "2024-03-04T19:50:10.784587Z",
     "iopub.status.idle": "2024-03-04T19:50:10.797701Z",
     "shell.execute_reply": "2024-03-04T19:50:10.797330Z"
    },
    "papermill": {
     "duration": 0.016266,
     "end_time": "2024-03-04T19:50:10.798329",
     "exception": false,
     "start_time": "2024-03-04T19:50:10.782063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark contains 1000 questions\n"
     ]
    }
   ],
   "source": [
    "# !pip install wandb transformers trl datasets \"protobuf==3.20.3\" evaluate\n",
    "# !wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json\n",
    "from utilities.parse_benchmark import parse_benchmark\n",
    "# benchmark = \"MedQA-4\"\n",
    "# benchmark = \"MedQA-5\"\n",
    "# benchmark = \"MedMCQA\"\n",
    "benchmark = \"PubMedQA\"\n",
    "# benchmark = \"bioASQ_with_snippet\"\n",
    "if benchmark == \"PubMedQA\":\n",
    "    benchmark_questions, benchmark_answers = parse_benchmark(benchmark, \"test.json\")\n",
    "else:\n",
    "    benchmark_questions, benchmark_answers = parse_benchmark(benchmark, \"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a154f968-da0f-4bdc-bf45-e499d95f0606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:10.802540Z",
     "iopub.status.busy": "2024-03-04T19:50:10.802414Z",
     "iopub.status.idle": "2024-03-04T19:50:15.785576Z",
     "shell.execute_reply": "2024-03-04T19:50:15.785303Z"
    },
    "papermill": {
     "duration": 4.985965,
     "end_time": "2024-03-04T19:50:15.786149",
     "exception": false,
     "start_time": "2024-03-04T19:50:10.800184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnelectric\u001b[0m (\u001b[33mneelectric\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/service/BioLlama/wandb/run-20240304_195011-icmdu63q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdeep-brook-174\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft/runs/icmdu63q\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/neelectric/biollama_ft/runs/icmdu63q?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc98372b5d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"biollama_ft\", # the project I am working on\n",
    "           tags=[\"hf_sft\", \"BioLlama\"]) # the Hyperparameters I want to keep track of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811291cc-b7ce-422a-8971-3cbf9fe10a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:15.797822Z",
     "iopub.status.busy": "2024-03-04T19:50:15.797700Z",
     "iopub.status.idle": "2024-03-04T19:50:16.541945Z",
     "shell.execute_reply": "2024-03-04T19:50:16.541666Z"
    },
    "papermill": {
     "duration": 0.754331,
     "end_time": "2024-03-04T19:50:16.542901",
     "exception": false,
     "start_time": "2024-03-04T19:50:15.788570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import json\n",
    "if benchmark == \"PubMedQA\":\n",
    "    artifact_dir = os.getcwd() + \"/benchmarks/PubMedQA/edited\"\n",
    "    dataset = load_dataset(\"json\", data_dir=artifact_dir)\n",
    "else:\n",
    "    if benchmark == \"MedQA-4\":\n",
    "        artifact_dir = os.getcwd() + \"/benchmarks/MedQA-4-option/\"\n",
    "    elif benchmark == \"MedQA-5\":\n",
    "        artifact_dir = os.getcwd() + \"/benchmarks/MedQA-USMLE/\"\n",
    "    elif benchmark == \"MedMCQA\":\n",
    "        artifact_dir = os.getcwd() + \"/benchmarks/MedMCQA/\"\n",
    "    elif benchmark == \"bioASQ_with_snippet\":\n",
    "        print(\"loading bioASQ\")\n",
    "        artifact_dir = os.getcwd() + \"/benchmarks/BioASQ/edited\"\n",
    "    dataset = load_dataset(\"json\", data_dir=artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3f4c49e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:16.550931Z",
     "iopub.status.busy": "2024-03-04T19:50:16.550404Z",
     "iopub.status.idle": "2024-03-04T19:50:16.553690Z",
     "shell.execute_reply": "2024-03-04T19:50:16.553473Z"
    },
    "papermill": {
     "duration": 0.007934,
     "end_time": "2024-03-04T19:50:16.554340",
     "exception": false,
     "start_time": "2024-03-04T19:50:16.546406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['CONTEXTS', 'QUESTION', 'final_decision'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c7dda87-d70a-470b-a0f9-040af434dc42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:16.561758Z",
     "iopub.status.busy": "2024-03-04T19:50:16.561583Z",
     "iopub.status.idle": "2024-03-04T19:50:16.564377Z",
     "shell.execute_reply": "2024-03-04T19:50:16.564149Z"
    },
    "papermill": {
     "duration": 0.007382,
     "end_time": "2024-03-04T19:50:16.565015",
     "exception": false,
     "start_time": "2024-03-04T19:50:16.557633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "if benchmark == \"PubMedQA\" or benchmark == \"bioASQ_with_snippet\":\n",
    "    train_dataset = dataset[\"test\"]\n",
    "else:\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"validation\"]\n",
    "    print(len(eval_dataset))\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a038eb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:16.572638Z",
     "iopub.status.busy": "2024-03-04T19:50:16.572462Z",
     "iopub.status.idle": "2024-03-04T19:50:16.578634Z",
     "shell.execute_reply": "2024-03-04T19:50:16.578402Z"
    },
    "papermill": {
     "duration": 0.010712,
     "end_time": "2024-03-04T19:50:16.579278",
     "exception": false,
     "start_time": "2024-03-04T19:50:16.568566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following text snippets, answer the question that follows.\n",
      "<SNIPPETS>\n",
      "From March 2007 to January 2011, 88 DBE procedures were performed on 66 patients. Indications included evaluation anemia/gastrointestinal bleed, small bowel IBD and dilation of strictures. Video-capsule endoscopy (VCE) was used prior to DBE in 43 of the 66 patients prior to DBE evaluation.\n",
      "The mean age was 62 years. Thirty-two patients were female, 15 were African-American; 44 antegrade and 44 retrograde DBEs were performed. The mean time per antegrade DBE was 107.4¬±30.0 minutes with a distance of 318.4¬±152.9 cm reached past the pylorus. The mean time per lower DBE was 100.7¬±27.3 minutes with 168.9¬±109.1 cm meters past the ileocecal valve reached. Endoscopic therapy in the form of electrocautery to ablate bleeding sources was performed in 20 patients (30.3%), biopsy in 17 patients (25.8%) and dilation of Crohn's-related small bowel strictures in 4 (6.1%). 43 VCEs with pathology noted were performed prior to DBE, with findings endoscopically confirmed in 32 cases (74.4%). In 3 cases the DBE showed findings not noted on VCE.\n",
      "</SNIPPETS>\n",
      "You start all of your responses with <ANSWER> and end them with </ANSWER>, as shown in the following example:\n",
      "<QUESTION>Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?</QUESTION>\n",
      "<ANSWER> yes</ANSWER>\n",
      "Do not justify your response, respond with only yes, maybe or no.\n",
      "<QUESTION>Double balloon enteroscopy: is it efficacious and safe in a community setting?</QUESTION>\n",
      "<ANSWER> yes</ANSWER>\n"
     ]
    }
   ],
   "source": [
    "from utilities.prompts2 import promptify\n",
    "# def create_prompt(row):\n",
    "#     option_string = \"\"\n",
    "#     for option in row[\"options\"].keys():\n",
    "#         option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "#     row[\"option_string\"] = option_string\n",
    "#     return (\"<QUESTION>{question} {option_string}</QUESTION><ANSWER> ({answer_idx}) {answer}</ANSWER>\").format_map(row)\n",
    "def create_prompt(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    MCQ_answer = \"(\" + row['answer_idx'] + \") \" + row[\"answer\"]\n",
    "    question = row[\"question\"] + option_string\n",
    "    promptified = promptify(benchmark, question, retrieval_mode = None, retrieved_chunks = None, model = None)\n",
    "    row[\"promptified\"] = promptified\n",
    "    row[\"MCQ_answer\"] = MCQ_answer\n",
    "    return (\"{promptified} {MCQ_answer}</ANSWER>\").format_map(row)\n",
    "\n",
    "if benchmark == \"MedMCQA\":\n",
    "    def create_prompt(row):\n",
    "        option_string = \"\\n(1) \" + row['opa']\n",
    "        option_string += \"\\n(2) \" + row['opb']\n",
    "        option_string += \"\\n(3) \" + row['opc']\n",
    "        option_string += \"\\n(4) \" + row['opd']\n",
    "        row[\"option_string\"] = option_string\n",
    "        if row['cop'] == 1:\n",
    "            row['answer'] = row['opa']\n",
    "        elif row['cop'] == 2:\n",
    "            row['answer'] = row['opb']\n",
    "        elif row['cop'] == 3:\n",
    "            row['answer'] = row['opc']\n",
    "        elif row['cop'] == 4:\n",
    "            row['answer'] = row['opd']\n",
    "        return (\"<QUESTION>{question} {option_string}</QUESTION><ANSWER> ({cop}) {answer}</ANSWER>\").format_map(row)\n",
    "elif benchmark == \"PubMedQA\":\n",
    "    def create_prompt(row):\n",
    "        snippet_string = \"\"\n",
    "        for snippet in row[\"CONTEXTS\"]:\n",
    "            snippet_string += snippet + \"\\n\"\n",
    "        row[\"snippet_string\"] = snippet_string\n",
    "        row[\"example\"] = \"You start all of your responses with <ANSWER> and end them with </ANSWER>, as shown in the following example:\\n<QUESTION>Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?</QUESTION>\\n<ANSWER> yes</ANSWER>\\nDo not justify your response, respond with only yes, maybe or no.\\n\"\n",
    "        return (\"Using the following text snippets, answer the question that follows.\\n<SNIPPETS>\\n{snippet_string}</SNIPPETS>\\n{example}<QUESTION>{QUESTION}</QUESTION>\\n<ANSWER> {final_decision}</ANSWER>\").format_map(row)\n",
    "elif benchmark == \"bioASQ_with_snippet\":\n",
    "    def create_prompt(row):\n",
    "        question = [row['snippets'], row['question']]\n",
    "        # print(question)\n",
    "        promptified = promptify(\"bioASQ_with_snippet\", question, retrieval_mode = None, retrieved_chunks = None, model = None)\n",
    "        return promptified + \" \" + row['answer'] + \"</ANSWER>\"\n",
    "print(create_prompt(train_dataset[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f1a9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:16.584834Z",
     "iopub.status.busy": "2024-03-04T19:50:16.584746Z",
     "iopub.status.idle": "2024-03-04T19:50:16.589359Z",
     "shell.execute_reply": "2024-03-04T19:50:16.589129Z"
    },
    "papermill": {
     "duration": 0.007787,
     "end_time": "2024-03-04T19:50:16.589833",
     "exception": false,
     "start_time": "2024-03-04T19:50:16.582046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following text snippets, answer the question that follows.\n",
      "<SNIPPETS>\n",
      "Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cells at the center of these areoles and progresses outwards, stopping approximately five cells from the vasculature. The role of mitochondria during PCD has been recognized in animals; however, it has been less studied during PCD in plants.\n",
      "The following paper elucidates the role of mitochondrial dynamics during developmentally regulated PCD in vivo in A. madagascariensis. A single areole within a window stage leaf (PCD is occurring) was divided into three areas based on the progression of PCD; cells that will not undergo PCD (NPCD), cells in early stages of PCD (EPCD), and cells in late stages of PCD (LPCD). Window stage leaves were stained with the mitochondrial dye MitoTracker Red CMXRos and examined. Mitochondrial dynamics were delineated into four categories (M1-M4) based on characteristics including distribution, motility, and membrane potential (ŒîŒ®m). A TUNEL assay showed fragmented nDNA in a gradient over these mitochondrial stages. Chloroplasts and transvacuolar strands were also examined using live cell imaging. The possible importance of mitochondrial permeability transition pore (PTP) formation during PCD was indirectly examined via in vivo cyclosporine A (CsA) treatment. This treatment resulted in lace plant leaves with a significantly lower number of perforations compared to controls, and that displayed mitochondrial dynamics similar to that of non-PCD cells.\n",
      "</SNIPPETS>\n",
      "You start all of your responses with <ANSWER> and end them with </ANSWER>, as shown in the following example:\n",
      "<QUESTION>Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?</QUESTION>\n",
      "<ANSWER> yes</ANSWER>\n",
      "Do not justify your response, respond with only yes, maybe or no.\n",
      "<QUESTION>Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?</QUESTION>\n",
      "<ANSWER> yes</ANSWER>\n"
     ]
    }
   ],
   "source": [
    "def create_prompt_no_answer(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"<QUESTION>{question} {option_string}</QUESTION><ANSWER> \").format_map(row)\n",
    "\n",
    "def return_prompt_no_answer(row):\n",
    "    return {\"text\": create_prompt_no_answer(row)}\n",
    "\n",
    "def return_prompt(row):\n",
    "    return {\"text\": create_prompt(row)}\n",
    "    \n",
    "if benchmark == \"MedQA\":\n",
    "    test_dataset = eval_dataset.map(return_prompt_no_answer)\n",
    "train_dataset_with_texts = train_dataset.map(return_prompt)\n",
    "print(train_dataset_with_texts[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5be57c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:16.594977Z",
     "iopub.status.busy": "2024-03-04T19:50:16.594802Z",
     "iopub.status.idle": "2024-03-04T19:50:17.372868Z",
     "shell.execute_reply": "2024-03-04T19:50:17.372635Z"
    },
    "papermill": {
     "duration": 0.781274,
     "end_time": "2024-03-04T19:50:17.373396",
     "exception": false,
     "start_time": "2024-03-04T19:50:16.592122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRO_layer_ids is [19] and torch_dtype is torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "size = \"13\"\n",
    "if size == \"7\":\n",
    "    RETRO_layer_ids = [15]\n",
    "    torch_dtype=torch.float32\n",
    "elif size == \"13\":\n",
    "    RETRO_layer_ids = [19]\n",
    "    torch_dtype=torch.bfloat16\n",
    "elif size == \"70\":\n",
    "    RETRO_layer_ids = [39]\n",
    "    torch_dtype=torch.float16\n",
    "    print(\"best of luck training 70b lol\")\n",
    "print(f\"RETRO_layer_ids is {RETRO_layer_ids} and torch_dtype is {torch_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d2f1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:17.385033Z",
     "iopub.status.busy": "2024-03-04T19:50:17.384861Z",
     "iopub.status.idle": "2024-03-04T19:50:29.350210Z",
     "shell.execute_reply": "2024-03-04T19:50:29.349799Z"
    },
    "papermill": {
     "duration": 11.975433,
     "end_time": "2024-03-04T19:50:29.351395",
     "exception": false,
     "start_time": "2024-03-04T19:50:17.375962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                | 1/3 [00:01<00:03,  1.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 2/3 [00:03<00:01,  1.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:05<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:05<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from utilities.biollama import BioLlama\n",
    "import torch\n",
    "\n",
    "amended_questions = [\"The main calcium pump of the sarcoplasmic reticulum is \"]\n",
    "# answers = [\"Sarcoplasmic reticulum Ca(2+)-ATPase\"] # or \"SERCA\",\"serca2\"\n",
    "prompt = amended_questions[0]\n",
    "model_id = \"meta-llama/Llama-2-\" + size +\"b-chat-hf\"\n",
    "chunk_length = 32\n",
    "\n",
    "BioLlama = BioLlama(\n",
    "    model_id=model_id,\n",
    "    chunk_length=chunk_length,\n",
    "    RETRO_layer_ids=RETRO_layer_ids,\n",
    "    training=True,\n",
    "    torch_dtype=torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e11ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:29.365447Z",
     "iopub.status.busy": "2024-03-04T19:50:29.365191Z",
     "iopub.status.idle": "2024-03-04T19:50:29.367097Z",
     "shell.execute_reply": "2024-03-04T19:50:29.366884Z"
    },
    "papermill": {
     "duration": 0.011617,
     "end_time": "2024-03-04T19:50:29.367761",
     "exception": false,
     "start_time": "2024-03-04T19:50:29.356144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BioLlama.model\n",
    "tokenizer = BioLlama.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:29.376725Z",
     "iopub.status.busy": "2024-03-04T19:50:29.376647Z",
     "iopub.status.idle": "2024-03-04T19:50:29.383490Z",
     "shell.execute_reply": "2024-03-04T19:50:29.383170Z"
    },
    "papermill": {
     "duration": 0.012303,
     "end_time": "2024-03-04T19:50:29.384273",
     "exception": false,
     "start_time": "2024-03-04T19:50:29.371970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freezing layers, currently only works for single unfrozen retro layer\n",
      "\n",
      "printing layer 19 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "cca_attn.q_proj.weight, requires_grad = False\n",
      "cca_attn.k_proj.weight, requires_grad = False\n",
      "cca_attn.v_proj.weight, requires_grad = False\n",
      "cca_attn.o_proj.weight, requires_grad = False\n",
      "pre_cca_layernorm.weight, requires_grad = False\n",
      "\n",
      "printing layer 19 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "cca_attn.q_proj.weight, requires_grad = True\n",
      "cca_attn.k_proj.weight, requires_grad = True\n",
      "cca_attn.v_proj.weight, requires_grad = True\n",
      "cca_attn.o_proj.weight, requires_grad = True\n",
      "pre_cca_layernorm.weight, requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "print(\"freezing layers, currently only works for single unfrozen retro layer\")\n",
    "n_freeze = BioLlama.RETRO_layer_ids[0]\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): \n",
    "    param.requires_grad = True\n",
    "#for every parameter in retro_layer_params, print where in the model it comes from (ie is it from self attention, layer norm, etc)\n",
    "print(f\"\\nprinting layer {n_freeze} params\")\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")   \n",
    "\n",
    "list_of_params_to_unfreeze = [\n",
    "    \"cca_attn.q_proj.weight\",\n",
    "    \"cca_attn.k_proj.weight\",\n",
    "    \"cca_attn.v_proj.weight\",\n",
    "    \"cca_attn.o_proj.weight\",\n",
    "    \"pre_cca_layernorm.weight\",\n",
    "]\n",
    "\n",
    "for name, param in model.model.layers[n_freeze].named_parameters(): \n",
    "    if name in list_of_params_to_unfreeze:\n",
    "        param.requires_grad = True\n",
    "print(f\"\\nprinting layer {n_freeze} params\")\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15a29820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:29.393331Z",
     "iopub.status.busy": "2024-03-04T19:50:29.393232Z",
     "iopub.status.idle": "2024-03-04T19:50:29.423544Z",
     "shell.execute_reply": "2024-03-04T19:50:29.423262Z"
    },
    "papermill": {
     "duration": 0.035722,
     "end_time": "2024-03-04T19:50:29.424314",
     "exception": false,
     "start_time": "2024-03-04T19:50:29.388592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight, requires_grad = False\n",
      "layers.0.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.0.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.0.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.0.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.0.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.0.mlp.up_proj.weight, requires_grad = False\n",
      "layers.0.mlp.down_proj.weight, requires_grad = False\n",
      "layers.0.input_layernorm.weight, requires_grad = False\n",
      "layers.0.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.1.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.1.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.1.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.1.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.1.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.1.mlp.up_proj.weight, requires_grad = False\n",
      "layers.1.mlp.down_proj.weight, requires_grad = False\n",
      "layers.1.input_layernorm.weight, requires_grad = False\n",
      "layers.1.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.2.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.2.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.2.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.2.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.2.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.2.mlp.up_proj.weight, requires_grad = False\n",
      "layers.2.mlp.down_proj.weight, requires_grad = False\n",
      "layers.2.input_layernorm.weight, requires_grad = False\n",
      "layers.2.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.3.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.3.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.3.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.3.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.3.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.3.mlp.up_proj.weight, requires_grad = False\n",
      "layers.3.mlp.down_proj.weight, requires_grad = False\n",
      "layers.3.input_layernorm.weight, requires_grad = False\n",
      "layers.3.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.4.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.4.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.4.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.4.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.4.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.4.mlp.up_proj.weight, requires_grad = False\n",
      "layers.4.mlp.down_proj.weight, requires_grad = False\n",
      "layers.4.input_layernorm.weight, requires_grad = False\n",
      "layers.4.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.5.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.5.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.5.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.5.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.5.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.5.mlp.up_proj.weight, requires_grad = False\n",
      "layers.5.mlp.down_proj.weight, requires_grad = False\n",
      "layers.5.input_layernorm.weight, requires_grad = False\n",
      "layers.5.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.6.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.6.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.6.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.6.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.6.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.6.mlp.up_proj.weight, requires_grad = False\n",
      "layers.6.mlp.down_proj.weight, requires_grad = False\n",
      "layers.6.input_layernorm.weight, requires_grad = False\n",
      "layers.6.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.7.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.7.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.7.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.7.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.7.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.7.mlp.up_proj.weight, requires_grad = False\n",
      "layers.7.mlp.down_proj.weight, requires_grad = False\n",
      "layers.7.input_layernorm.weight, requires_grad = False\n",
      "layers.7.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.8.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.8.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.8.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.8.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.8.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.8.mlp.up_proj.weight, requires_grad = False\n",
      "layers.8.mlp.down_proj.weight, requires_grad = False\n",
      "layers.8.input_layernorm.weight, requires_grad = False\n",
      "layers.8.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.9.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.9.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.9.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.9.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.9.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.9.mlp.up_proj.weight, requires_grad = False\n",
      "layers.9.mlp.down_proj.weight, requires_grad = False\n",
      "layers.9.input_layernorm.weight, requires_grad = False\n",
      "layers.9.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.10.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.10.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.10.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.10.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.10.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.10.mlp.up_proj.weight, requires_grad = False\n",
      "layers.10.mlp.down_proj.weight, requires_grad = False\n",
      "layers.10.input_layernorm.weight, requires_grad = False\n",
      "layers.10.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.11.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.11.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.11.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.11.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.11.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.11.mlp.up_proj.weight, requires_grad = False\n",
      "layers.11.mlp.down_proj.weight, requires_grad = False\n",
      "layers.11.input_layernorm.weight, requires_grad = False\n",
      "layers.11.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.12.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.12.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.12.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.12.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.12.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.12.mlp.up_proj.weight, requires_grad = False\n",
      "layers.12.mlp.down_proj.weight, requires_grad = False\n",
      "layers.12.input_layernorm.weight, requires_grad = False\n",
      "layers.12.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.13.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.13.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.13.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.13.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.13.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.13.mlp.up_proj.weight, requires_grad = False\n",
      "layers.13.mlp.down_proj.weight, requires_grad = False\n",
      "layers.13.input_layernorm.weight, requires_grad = False\n",
      "layers.13.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.14.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.14.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.14.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.14.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.14.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.14.mlp.up_proj.weight, requires_grad = False\n",
      "layers.14.mlp.down_proj.weight, requires_grad = False\n",
      "layers.14.input_layernorm.weight, requires_grad = False\n",
      "layers.14.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.15.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.15.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.15.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.15.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.15.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.15.mlp.up_proj.weight, requires_grad = False\n",
      "layers.15.mlp.down_proj.weight, requires_grad = False\n",
      "layers.15.input_layernorm.weight, requires_grad = False\n",
      "layers.15.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.16.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.16.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.16.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.16.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.16.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.16.mlp.up_proj.weight, requires_grad = False\n",
      "layers.16.mlp.down_proj.weight, requires_grad = False\n",
      "layers.16.input_layernorm.weight, requires_grad = False\n",
      "layers.16.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.17.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.17.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.17.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.17.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.17.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.17.mlp.up_proj.weight, requires_grad = False\n",
      "layers.17.mlp.down_proj.weight, requires_grad = False\n",
      "layers.17.input_layernorm.weight, requires_grad = False\n",
      "layers.17.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.18.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.18.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.18.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.18.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.18.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.18.mlp.up_proj.weight, requires_grad = False\n",
      "layers.18.mlp.down_proj.weight, requires_grad = False\n",
      "layers.18.input_layernorm.weight, requires_grad = False\n",
      "layers.18.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.19.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.19.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.19.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.19.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.19.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.19.mlp.up_proj.weight, requires_grad = False\n",
      "layers.19.mlp.down_proj.weight, requires_grad = False\n",
      "layers.19.input_layernorm.weight, requires_grad = False\n",
      "layers.19.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.19.cca_attn.q_proj.weight, requires_grad = True\n",
      "layers.19.cca_attn.k_proj.weight, requires_grad = True\n",
      "layers.19.cca_attn.v_proj.weight, requires_grad = True\n",
      "layers.19.cca_attn.o_proj.weight, requires_grad = True\n",
      "layers.19.pre_cca_layernorm.weight, requires_grad = True\n",
      "layers.20.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.20.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.20.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.20.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.20.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.20.mlp.up_proj.weight, requires_grad = False\n",
      "layers.20.mlp.down_proj.weight, requires_grad = False\n",
      "layers.20.input_layernorm.weight, requires_grad = False\n",
      "layers.20.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.21.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.21.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.21.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.21.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.21.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.21.mlp.up_proj.weight, requires_grad = False\n",
      "layers.21.mlp.down_proj.weight, requires_grad = False\n",
      "layers.21.input_layernorm.weight, requires_grad = False\n",
      "layers.21.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.22.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.22.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.22.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.22.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.22.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.22.mlp.up_proj.weight, requires_grad = False\n",
      "layers.22.mlp.down_proj.weight, requires_grad = False\n",
      "layers.22.input_layernorm.weight, requires_grad = False\n",
      "layers.22.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.23.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.23.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.23.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.23.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.23.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.23.mlp.up_proj.weight, requires_grad = False\n",
      "layers.23.mlp.down_proj.weight, requires_grad = False\n",
      "layers.23.input_layernorm.weight, requires_grad = False\n",
      "layers.23.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.24.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.24.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.24.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.24.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.24.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.24.mlp.up_proj.weight, requires_grad = False\n",
      "layers.24.mlp.down_proj.weight, requires_grad = False\n",
      "layers.24.input_layernorm.weight, requires_grad = False\n",
      "layers.24.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.25.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.25.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.25.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.25.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.25.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.25.mlp.up_proj.weight, requires_grad = False\n",
      "layers.25.mlp.down_proj.weight, requires_grad = False\n",
      "layers.25.input_layernorm.weight, requires_grad = False\n",
      "layers.25.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.26.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.26.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.26.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.26.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.26.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.26.mlp.up_proj.weight, requires_grad = False\n",
      "layers.26.mlp.down_proj.weight, requires_grad = False\n",
      "layers.26.input_layernorm.weight, requires_grad = False\n",
      "layers.26.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.27.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.27.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.27.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.27.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.27.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.27.mlp.up_proj.weight, requires_grad = False\n",
      "layers.27.mlp.down_proj.weight, requires_grad = False\n",
      "layers.27.input_layernorm.weight, requires_grad = False\n",
      "layers.27.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.28.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.28.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.28.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.28.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.28.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.28.mlp.up_proj.weight, requires_grad = False\n",
      "layers.28.mlp.down_proj.weight, requires_grad = False\n",
      "layers.28.input_layernorm.weight, requires_grad = False\n",
      "layers.28.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.29.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.29.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.29.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.29.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.29.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.29.mlp.up_proj.weight, requires_grad = False\n",
      "layers.29.mlp.down_proj.weight, requires_grad = False\n",
      "layers.29.input_layernorm.weight, requires_grad = False\n",
      "layers.29.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.30.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.30.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.30.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.30.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.30.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.30.mlp.up_proj.weight, requires_grad = False\n",
      "layers.30.mlp.down_proj.weight, requires_grad = False\n",
      "layers.30.input_layernorm.weight, requires_grad = False\n",
      "layers.30.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.31.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.31.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.31.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.31.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.31.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.31.mlp.up_proj.weight, requires_grad = False\n",
      "layers.31.mlp.down_proj.weight, requires_grad = False\n",
      "layers.31.input_layernorm.weight, requires_grad = False\n",
      "layers.31.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.32.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.32.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.32.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.32.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.32.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.32.mlp.up_proj.weight, requires_grad = False\n",
      "layers.32.mlp.down_proj.weight, requires_grad = False\n",
      "layers.32.input_layernorm.weight, requires_grad = False\n",
      "layers.32.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.33.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.33.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.33.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.33.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.33.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.33.mlp.up_proj.weight, requires_grad = False\n",
      "layers.33.mlp.down_proj.weight, requires_grad = False\n",
      "layers.33.input_layernorm.weight, requires_grad = False\n",
      "layers.33.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.34.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.34.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.34.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.34.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.34.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.34.mlp.up_proj.weight, requires_grad = False\n",
      "layers.34.mlp.down_proj.weight, requires_grad = False\n",
      "layers.34.input_layernorm.weight, requires_grad = False\n",
      "layers.34.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.35.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.35.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.35.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.35.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.35.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.35.mlp.up_proj.weight, requires_grad = False\n",
      "layers.35.mlp.down_proj.weight, requires_grad = False\n",
      "layers.35.input_layernorm.weight, requires_grad = False\n",
      "layers.35.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.36.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.36.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.36.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.36.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.36.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.36.mlp.up_proj.weight, requires_grad = False\n",
      "layers.36.mlp.down_proj.weight, requires_grad = False\n",
      "layers.36.input_layernorm.weight, requires_grad = False\n",
      "layers.36.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.37.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.37.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.37.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.37.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.37.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.37.mlp.up_proj.weight, requires_grad = False\n",
      "layers.37.mlp.down_proj.weight, requires_grad = False\n",
      "layers.37.input_layernorm.weight, requires_grad = False\n",
      "layers.37.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.38.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.38.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.38.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.38.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.38.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.38.mlp.up_proj.weight, requires_grad = False\n",
      "layers.38.mlp.down_proj.weight, requires_grad = False\n",
      "layers.38.input_layernorm.weight, requires_grad = False\n",
      "layers.38.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.39.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.39.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.39.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.39.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.39.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.39.mlp.up_proj.weight, requires_grad = False\n",
      "layers.39.mlp.down_proj.weight, requires_grad = False\n",
      "layers.39.input_layernorm.weight, requires_grad = False\n",
      "layers.39.post_attention_layernorm.weight, requires_grad = False\n",
      "norm.weight, requires_grad = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120)\n",
       "    (layers): ModuleList(\n",
       "      (0-18): 19 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "        (cca_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (pre_cca_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (20-39): 20 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, param in model.model.named_parameters(): \n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")\n",
    "    # param.requires_grad = True\n",
    "\n",
    "BioLlama.model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:29.434460Z",
     "iopub.status.busy": "2024-03-04T19:50:29.434378Z",
     "iopub.status.idle": "2024-03-04T19:50:29.435999Z",
     "shell.execute_reply": "2024-03-04T19:50:29.435801Z"
    },
    "papermill": {
     "duration": 0.007418,
     "end_time": "2024-03-04T19:50:29.436631",
     "exception": false,
     "start_time": "2024-03-04T19:50:29.429213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "model.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:29.446665Z",
     "iopub.status.busy": "2024-03-04T19:50:29.446496Z",
     "iopub.status.idle": "2024-03-04T19:50:29.449900Z",
     "shell.execute_reply": "2024-03-04T19:50:29.449689Z"
    },
    "papermill": {
     "duration": 0.009075,
     "end_time": "2024-03-04T19:50:29.450516",
     "exception": false,
     "start_time": "2024-03-04T19:50:29.441441",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 13120.73M, Trainable: 268.70M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c57893e-6e42-4cda-83c8-1e51a86d2da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:29.460443Z",
     "iopub.status.busy": "2024-03-04T19:50:29.460259Z",
     "iopub.status.idle": "2024-03-04T19:50:29.462849Z",
     "shell.execute_reply": "2024-03-04T19:50:29.462645Z"
    },
    "papermill": {
     "duration": 0.0082,
     "end_time": "2024-03-04T19:50:29.463481",
     "exception": false,
     "start_time": "2024-03-04T19:50:29.455281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5605\n",
      "changing total num size to 1000\n",
      "PubMedQA\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "total_num_steps = 11_210 // batch_size\n",
    "print(total_num_steps)\n",
    "\n",
    "if benchmark == \"MedQA-4\" or benchmark == \"MedQA-5\":\n",
    "    total_num_steps = 10178\n",
    "elif benchmark == \"MedMCQA\":\n",
    "    total_num_steps = 100000\n",
    "elif benchmark == \"PubMedQA\":\n",
    "    total_num_steps = 1000\n",
    "elif benchmark == \"bioASQ_with_snippet\":\n",
    "    total_num_steps = 486\n",
    "print(f\"changing total num size to {total_num_steps}\")\n",
    "print(benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c86339ca-27e5-496b-9559-3a65bb3c26a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:29.473980Z",
     "iopub.status.busy": "2024-03-04T19:50:29.473846Z",
     "iopub.status.idle": "2024-03-04T19:50:29.508158Z",
     "shell.execute_reply": "2024-03-04T19:50:29.507865Z"
    },
    "papermill": {
     "duration": 0.040449,
     "end_time": "2024-03-04T19:50:29.509066",
     "exception": false,
     "start_time": "2024-03-04T19:50:29.468617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\" + size  + \"/\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size//2,\n",
    "    bf16=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=total_num_steps // 10,\n",
    "    num_train_epochs=10,\n",
    "    max_steps=total_num_steps,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=5000,\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=8,\n",
    "    save_strategy=\"epoch\", #changed to epoch so we save every epoch i guess?\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e35a9b70-f36b-4bfd-857f-c80d3450e111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:29.519111Z",
     "iopub.status.busy": "2024-03-04T19:50:29.519033Z",
     "iopub.status.idle": "2024-03-04T19:50:29.566286Z",
     "shell.execute_reply": "2024-03-04T19:50:29.565978Z"
    },
    "papermill": {
     "duration": 0.053178,
     "end_time": "2024-03-04T19:50:29.567059",
     "exception": false,
     "start_time": "2024-03-04T19:50:29.513881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from utils import LLMSampleCB, token_accuracy\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset_with_texts,\n",
    "    dataset_text_field=\"text\",\n",
    "    # eval_dataset=test_dataset,\n",
    "    packing=True,\n",
    "    max_seq_length=1024,\n",
    "    args=training_args,\n",
    "    formatting_func=create_prompt,\n",
    "    # compute_metrics=token_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5754789-2e15-4bc9-800c-01f8ffc625e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:50:29.577338Z",
     "iopub.status.busy": "2024-03-04T19:50:29.577230Z",
     "iopub.status.idle": "2024-03-04T19:58:40.509147Z",
     "shell.execute_reply": "2024-03-04T19:58:40.508650Z"
    },
    "papermill": {
     "duration": 490.938383,
     "end_time": "2024-03-04T19:58:40.510338",
     "exception": false,
     "start_time": "2024-03-04T19:50:29.571955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/utils.py:268: UserWarning: The dataset reached end and the iterator is reset to the start.\n",
      "  warnings.warn(\"The dataset reached end and the iterator is reset to the start.\")\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 07:56, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.724100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.725400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.666200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.549900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.467900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.445400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.357400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.364500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.288300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.291700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.230300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.238400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.307800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>1.321100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>1.197600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>1.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>1.209500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>1.226900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.281700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>1.135400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>1.265600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>1.229900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>1.164900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>1.141200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>1.110200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>1.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>1.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>1.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>1.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>1.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>1.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>1.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>1.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>1.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>1.108700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.159600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>1.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>1.155100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>1.058700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>1.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.995600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>1.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>1.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>1.090300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>1.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>1.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>1.090100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>1.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>0.929700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>0.881600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.892700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>0.882600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.900700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.863800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>0.900600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>0.877600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.886500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>0.882500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>0.891800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>0.888600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>0.887100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.833600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>0.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>0.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>0.888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>0.886500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.882400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>0.886300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>0.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>0.847100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>0.883900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.863500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>0.910800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>0.966300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>0.845800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>0.915100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.884300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>0.883200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>0.842400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>0.901800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>0.859400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>0.855600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>0.868900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>0.875700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.905700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>848</td>\n",
       "      <td>0.867400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>0.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>0.909200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>0.866900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>0.923100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>0.892600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>904</td>\n",
       "      <td>0.868400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912</td>\n",
       "      <td>0.873800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>0.867600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>0.896500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>0.899400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.934200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>968</td>\n",
       "      <td>0.879900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>976</td>\n",
       "      <td>0.925100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>984</td>\n",
       "      <td>0.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>992</td>\n",
       "      <td>0.896900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.895000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/data/dataloader.py:642: UserWarning: Length of IterableDataset <trl.trainer.utils.ConstantLengthDataset object at 0x7fc813119e50> was reported to be 1000 (when accessing len(dataloader)), but 1001 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.004 MB of 0.004 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.004 MB of 0.047 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.056 MB of 0.056 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.056 MB of 0.056 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.056 MB of 0.056 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 1.06582\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 484.3064\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 4.13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mdeep-brook-174\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft/runs/icmdu63q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240304_195011-icmdu63q/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#very hacky but maybe this will work:\n",
    "tokenizer.model_input_names = ['labels', 'input_ids', 'attention_mask']\n",
    "# trainer.args.train_batch_size = 1\n",
    "# self.args.train_batch_size\n",
    "\n",
    "#also hacky, but could work:\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Starting training\")\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48ca3011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:58:40.523071Z",
     "iopub.status.busy": "2024-03-04T19:58:40.522926Z",
     "iopub.status.idle": "2024-03-04T19:58:40.525661Z",
     "shell.execute_reply": "2024-03-04T19:58:40.525338Z"
    },
    "papermill": {
     "duration": 0.00999,
     "end_time": "2024-03-04T19:58:40.526479",
     "exception": false,
     "start_time": "2024-03-04T19:58:40.516489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PubMedQA'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64e06dfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:58:40.538708Z",
     "iopub.status.busy": "2024-03-04T19:58:40.538465Z",
     "iopub.status.idle": "2024-03-04T19:58:40.540870Z",
     "shell.execute_reply": "2024-03-04T19:58:40.540518Z"
    },
    "papermill": {
     "duration": 0.00958,
     "end_time": "2024-03-04T19:58:40.541733",
     "exception": false,
     "start_time": "2024-03-04T19:58:40.532153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "[19]\n"
     ]
    }
   ],
   "source": [
    "print(size)\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\" + benchmark + \"/\" + size  + \"/\"\n",
    "print(RETRO_layer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "684ca6fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:58:40.554027Z",
     "iopub.status.busy": "2024-03-04T19:58:40.553793Z",
     "iopub.status.idle": "2024-03-04T19:59:03.139193Z",
     "shell.execute_reply": "2024-03-04T19:59:03.138766Z"
    },
    "papermill": {
     "duration": 22.592975,
     "end_time": "2024-03-04T19:59:03.140460",
     "exception": false,
     "start_time": "2024-03-04T19:58:40.547485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/service/BioLlama/utilities/finetuning/biollama_training_output/PubMedQA/13\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath(output_dir))\n",
    "trainer.save_model(output_dir)\n",
    "# !ls -l $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16ff2529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:59:03.162733Z",
     "iopub.status.busy": "2024-03-04T19:59:03.162616Z",
     "iopub.status.idle": "2024-03-04T19:59:03.164660Z",
     "shell.execute_reply": "2024-03-04T19:59:03.164379Z"
    },
    "papermill": {
     "duration": 0.012179,
     "end_time": "2024-03-04T19:59:03.165508",
     "exception": false,
     "start_time": "2024-03-04T19:59:03.153329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/service/BioLlama/utilities/finetuning/biollama_training_output/PubMedQA/13/\n"
     ]
    }
   ],
   "source": [
    "#load this local model here and use it to generate some text\n",
    "print(output_dir)\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import time\n",
    "# import torch\n",
    "# from utilities.biollama import BioLlama\n",
    "\n",
    "# chunk_length = 32\n",
    "\n",
    "# BioLlama = BioLlama(model_id=output_dir, \n",
    "#     chunk_length=chunk_length, \n",
    "#     RETRO_layer_ids = RETRO_layer_ids, \n",
    "#     training=False, \n",
    "#     torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f20f493d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:59:03.185264Z",
     "iopub.status.busy": "2024-03-04T19:59:03.184997Z",
     "iopub.status.idle": "2024-03-04T19:59:09.660732Z",
     "shell.execute_reply": "2024-03-04T19:59:09.660450Z"
    },
    "papermill": {
     "duration": 6.486299,
     "end_time": "2024-03-04T19:59:09.661295",
     "exception": false,
     "start_time": "2024-03-04T19:59:03.174996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \n",
      " (A) Ampicillin\n",
      " (B) Ceftriaxone\n",
      " (C) Ciprofloxacin\n",
      " (D) Doxycycline\n",
      " (E) Nitrofurantoin</QUESTION>\n",
      "<ANSWER> \n",
      "(D) Doxycycline</ANSWER>\n",
      "<RATIONALE> Burning upon urination in a pregnant woman at 22 weeks gestation is a symptom of urinary tract infection (\n",
      "Time taken for generation: 6.473160028457642\n",
      "Tokens per second: 7.724202673838961\n"
     ]
    }
   ],
   "source": [
    "BioLlama.training = False\n",
    "import time\n",
    "prompt  = '<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \\n (A) Ampicillin\\n (B) Ceftriaxone\\n (C) Ciprofloxacin\\n (D) Doxycycline\\n (E) Nitrofurantoin</QUESTION>\\n<ANSWER> '\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99856974",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:59:09.669578Z",
     "iopub.status.busy": "2024-03-04T19:59:09.669488Z",
     "iopub.status.idle": "2024-03-04T19:59:13.445576Z",
     "shell.execute_reply": "2024-03-04T19:59:13.445225Z"
    },
    "papermill": {
     "duration": 3.780803,
     "end_time": "2024-03-04T19:59:13.446139",
     "exception": false,
     "start_time": "2024-03-04T19:59:09.665336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby? \n",
      " (A) Placing the infant in a supine position on a firm mattress while sleeping\n",
      " (B) Routine postnatal electrocardiogram (ECG)\n",
      " (C) Keeping the infant covered and maintaining a high room temperature\n",
      " (D) Application of a device to maintain the sleeping position\n",
      " (E) Avoiding pacifier use during sleep</QUESTION>\n",
      "<ANSWER>  A) Placing the infant in a supine position on a firm mattress while sleeping</ANSWER>\n",
      "<Rationale> The death of a 3-month-old baby while sleeping could have been prevented by\n",
      "Time taken for generation: 3.7737021446228027\n",
      "Tokens per second: 13.249588357482228\n"
     ]
    }
   ],
   "source": [
    "prompt2 = '<QUESTION>A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby? \\n (A) Placing the infant in a supine position on a firm mattress while sleeping\\n (B) Routine postnatal electrocardiogram (ECG)\\n (C) Keeping the infant covered and maintaining a high room temperature\\n (D) Application of a device to maintain the sleeping position\\n (E) Avoiding pacifier use during sleep</QUESTION>\\n<ANSWER> '\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt2, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16ad4c40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:59:13.454350Z",
     "iopub.status.busy": "2024-03-04T19:59:13.454232Z",
     "iopub.status.idle": "2024-03-04T19:59:19.239296Z",
     "shell.execute_reply": "2024-03-04T19:59:19.238952Z"
    },
    "papermill": {
     "duration": 5.789761,
     "end_time": "2024-03-04T19:59:19.239862",
     "exception": false,
     "start_time": "2024-03-04T19:59:13.450101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation? \n",
      " (A) Abnormal migration of ventral pancreatic bud\n",
      " (B) Complete failure of proximal duodenum to recanalize\n",
      " (C) Error in neural crest cell migration\n",
      " (D) Abnormal hypertrophy of the pylorus\n",
      " (E) Failure of lateral body folds to move ventrally and fuse in the midline</QUESTION>\n",
      "<ANSWER> \n",
      "C) Error in neural crest cell migration.\n",
      "</ANSWER>\n",
      "<Rationale>\n",
      "The symptoms described in the question, such as fussiness, regurgitation, and yellow vomit, suggest that there may\n",
      "Time taken for generation: 5.782537460327148\n",
      "Tokens per second: 8.646723059390476\n"
     ]
    }
   ],
   "source": [
    "prompt3 = \"<QUESTION>A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation? \\n (A) Abnormal migration of ventral pancreatic bud\\n (B) Complete failure of proximal duodenum to recanalize\\n (C) Error in neural crest cell migration\\n (D) Abnormal hypertrophy of the pylorus\\n (E) Failure of lateral body folds to move ventrally and fuse in the midline</QUESTION>\\n<ANSWER> \"\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt3, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f24725d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T19:59:19.248317Z",
     "iopub.status.busy": "2024-03-04T19:59:19.248194Z",
     "iopub.status.idle": "2024-03-04T19:59:25.865690Z",
     "shell.execute_reply": "2024-03-04T19:59:25.865325Z"
    },
    "papermill": {
     "duration": 6.622552,
     "end_time": "2024-03-04T19:59:25.866521",
     "exception": false,
     "start_time": "2024-03-04T19:59:19.243969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A 20-year-old woman presents with menorrhagia for the past several years. She says that her menses ‚Äúhave always been heavy‚Äù, and she has experienced easy bruising for as long as she can remember. Family history is significant for her mother, who had similar problems with bruising easily. The patient's vital signs include: heart rate 98/min, respiratory rate 14/min, temperature 36.1¬∞C (96.9¬∞F), and blood pressure 110/87 mm Hg. Physical examination is unremarkable. Laboratory tests show the following: platelet count 200,000/mm3, PT 12 seconds, and PTT 43 seconds. Which of the following is the most likely cause of this patient‚Äôs symptoms? \n",
      " (A) Factor V Leiden\n",
      " (B) Hemophilia A\n",
      " (C) Lupus anticoagulant\n",
      " (D) Protein C deficiency\n",
      " (E) Von Willebrand disease</QUESTION>\n",
      "<ANSWER> \n",
      "The most likely cause of this patient's symptoms is (D) Protein C deficiency.\n",
      "\n",
      "The patient's history of heavy menstrual bleeding and easy bruising suggest a bleeding disorder. The fact\n",
      "Time taken for generation: 6.614832639694214\n",
      "Tokens per second: 7.558770224957856\n"
     ]
    }
   ],
   "source": [
    "prompt4 = \"<QUESTION>A 20-year-old woman presents with menorrhagia for the past several years. She says that her menses ‚Äúhave always been heavy‚Äù, and she has experienced easy bruising for as long as she can remember. Family history is significant for her mother, who had similar problems with bruising easily. The patient's vital signs include: heart rate 98/min, respiratory rate 14/min, temperature 36.1¬∞C (96.9¬∞F), and blood pressure 110/87 mm Hg. Physical examination is unremarkable. Laboratory tests show the following: platelet count 200,000/mm3, PT 12 seconds, and PTT 43 seconds. Which of the following is the most likely cause of this patient‚Äôs symptoms? \\n (A) Factor V Leiden\\n (B) Hemophilia A\\n (C) Lupus anticoagulant\\n (D) Protein C deficiency\\n (E) Von Willebrand disease</QUESTION>\\n<ANSWER> \"\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt4, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883afe6b",
   "metadata": {
    "papermill": {
     "duration": 0.008606,
     "end_time": "2024-03-04T19:59:25.882062",
     "exception": false,
     "start_time": "2024-03-04T19:59:25.873456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1afa7",
   "metadata": {
    "papermill": {
     "duration": 0.006274,
     "end_time": "2024-03-04T19:59:25.894713",
     "exception": false,
     "start_time": "2024-03-04T19:59:25.888439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 557.596291,
   "end_time": "2024-03-04T19:59:27.619979",
   "environment_variables": {},
   "exception": null,
   "input_path": "BioLlama_finetuning_WandB_HF.ipynb",
   "output_path": "output_biollama.ipynb",
   "parameters": {},
   "start_time": "2024-03-04T19:50:10.023688",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}