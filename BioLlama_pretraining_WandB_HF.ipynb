{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb384650",
   "metadata": {
    "papermill": {
     "duration": 0.002541,
     "end_time": "2024-01-17T00:20:59.083690",
     "exception": false,
     "start_time": "2024-01-17T00:20:59.081149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is taken directly from https://github.com/tcapelle/llm_recipes/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
   "metadata": {
    "papermill": {
     "duration": 0.002618,
     "end_time": "2024-01-17T00:20:59.094857",
     "exception": false,
     "start_time": "2024-01-17T00:20:59.092239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# From Llama to Alpaca: Finetunning and LLM with Weights & Biases\n",
    "In this notebooks you will learn how to finetune a pretrained LLama model on an Instruction dataset. We will use an updated version of the Alpaca dataset that, instead of davinci-003 (GPT3) generations uses GPT4 to get an even better instruction dataset! More details on the [official repo page](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data)\n",
    "\n",
    "> This notebook requires a A100/A10 GPU with at least 24GB of memory. You could tweak the params down and run on a T4 but it would take very long time\n",
    "\n",
    "This notebooks has a companion project and [report](wandb.me/alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7744abe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:20:59.099735Z",
     "iopub.status.busy": "2024-01-17T00:20:59.099584Z",
     "iopub.status.idle": "2024-01-17T00:21:00.872449Z",
     "shell.execute_reply": "2024-01-17T00:21:00.871963Z"
    },
    "papermill": {
     "duration": 1.776896,
     "end_time": "2024-01-17T00:21:00.873674",
     "exception": false,
     "start_time": "2024-01-17T00:20:59.096778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (0.16.1)\n",
      "Requirement already satisfied: transformers in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (4.37.2)\n",
      "Requirement already satisfied: trl in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (0.4.7)\n",
      "Requirement already satisfied: datasets in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (2.14.4)\n",
      "Requirement already satisfied: protobuf==3.20.3 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (3.20.3)\n",
      "Requirement already satisfied: evaluate in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (0.4.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from wandb) (8.1.6)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from wandb) (3.1.40)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from wandb) (1.39.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: filelock in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from trl) (2.1.2)\n",
      "Requirement already satisfied: accelerate in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from trl) (0.26.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from torch>=1.4.0->trl) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl) (12.3.101)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb transformers trl datasets \"protobuf==3.20.3\" evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693a36b-5cdc-4582-844b-8dd52fa522c5",
   "metadata": {
    "papermill": {
     "duration": 0.008392,
     "end_time": "2024-01-17T00:21:00.884644",
     "exception": false,
     "start_time": "2024-01-17T00:21:00.876252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## With Huggingface TRL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04c0a5-f481-4364-880d-10c254388987",
   "metadata": {
    "papermill": {
     "duration": 0.002148,
     "end_time": "2024-01-17T00:21:00.889189",
     "exception": false,
     "start_time": "2024-01-17T00:21:00.887041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's grab the Alpaca (GPT-4 curated instructions and outputs) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:00.894314Z",
     "iopub.status.busy": "2024-01-17T00:21:00.894173Z",
     "iopub.status.idle": "2024-01-17T00:21:00.896381Z",
     "shell.execute_reply": "2024-01-17T00:21:00.896113Z"
    },
    "papermill": {
     "duration": 0.005637,
     "end_time": "2024-01-17T00:21:00.896990",
     "exception": false,
     "start_time": "2024-01-17T00:21:00.891353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json\n",
    "# from uparse_benchmark import parse_benchmark\n",
    "# from ..utilities.parse_benchmark import parse_benchmark\n",
    "# from utilities.parse_benchmark import parse_benchmark\n",
    "\n",
    "# benchmark = \"MedQA\"\n",
    "# benchmark_questions, benchmark_answers = parse_benchmark(benchmark)\n",
    "# # print(benchmark_questions[0])\n",
    "# # print(benchmark_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a154f968-da0f-4bdc-bf45-e499d95f0606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:01.024789Z",
     "iopub.status.busy": "2024-01-17T00:21:01.024684Z",
     "iopub.status.idle": "2024-01-17T00:21:07.425408Z",
     "shell.execute_reply": "2024-01-17T00:21:07.425094Z"
    },
    "papermill": {
     "duration": 6.410539,
     "end_time": "2024-01-17T00:21:07.426311",
     "exception": false,
     "start_time": "2024-01-17T00:21:01.015772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnelectric\u001b[0m (\u001b[33mneelectric\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/service/BioLlama/wandb/run-20240301_004154-r090aqvr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/neelectric/biollama_ft/runs/r090aqvr' target=\"_blank\">silver-snow-144</a></strong> to <a href='https://wandb.ai/neelectric/biollama_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/neelectric/biollama_ft' target=\"_blank\">https://wandb.ai/neelectric/biollama_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/neelectric/biollama_ft/runs/r090aqvr' target=\"_blank\">https://wandb.ai/neelectric/biollama_ft/runs/r090aqvr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/neelectric/biollama_ft/runs/r090aqvr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f390a61f210>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"biollama_ft\", # the project I am working on\n",
    "           tags=[\"hf_sft\", \"BioLlama\"]) # the Hyperparameters I want to keep track of\n",
    "# artifact = wandb.use_artifact('Neelectric/MedQA-USMLE', type='dataset')\n",
    "# artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811291cc-b7ce-422a-8971-3cbf9fe10a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:07.440755Z",
     "iopub.status.busy": "2024-01-17T00:21:07.440567Z",
     "iopub.status.idle": "2024-01-17T00:21:08.281875Z",
     "shell.execute_reply": "2024-01-17T00:21:08.281591Z"
    },
    "papermill": {
     "duration": 0.852358,
     "end_time": "2024-01-17T00:21:08.282858",
     "exception": false,
     "start_time": "2024-01-17T00:21:07.430500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24cf6197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0  \\\n",
      "0              NaN   \n",
      "1              NaN   \n",
      "2              NaN   \n",
      "3              NaN   \n",
      "4              NaN   \n",
      "...            ...   \n",
      "473153         NaN   \n",
      "473154         NaN   \n",
      "473155         NaN   \n",
      "473156         NaN   \n",
      "473157         NaN   \n",
      "\n",
      "       LAUGHING GAS is the newest thing for kids seeking kicks, the Stanford Daily reports. \"They sniff it.\"So begins a news story in the Los Angeles Times of 26 January 1967. The story continues:\"It's the latest way to travel, or so say a growing group of devotees on the campus,\" the university student paper said. \"It can produce much the same effects as psychedelic drugs, they claim, and it's cheaper to obtain.\"\"One student said he buys the gas, nitrous oxide, from a medical supply house. ;They think I am anesthetizing rats,' he explained.\"Campus medical authorities said the gas, sniffed ;in sufficient amounts... could produce all the states of anesthesia, including the final stage-death.'\"  \\\n",
      "0       The role of angiography in the diagnosis of ca...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "1       Glutamine synthetase in the developing retina ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "2       Horizontal borders were deleted progressively ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "3       1. The lipid compositions of the low-density l...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "4       Experiments were conducted with aged nuclear-f...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "...                                                   ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "473153                                                NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "473154                                                NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "473155                                                NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "473156                                                NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "473157                                                NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "\n",
      "       To determine the extent to which the Brunnstrom recovery stages of upper limb in hemiparetic stroke patients are correlated to neurophysiological measures and the spasticity measure of Modified Modified Ashworth Scale (MMAS).  \n",
      "0                                                     NaN                                                                                                                                                                                 \n",
      "1                                                     NaN                                                                                                                                                                                 \n",
      "2                                                     NaN                                                                                                                                                                                 \n",
      "3                                                     NaN                                                                                                                                                                                 \n",
      "4                                                     NaN                                                                                                                                                                                 \n",
      "...                                                   ...                                                                                                                                                                                 \n",
      "473153  We previously reported that Lactobacillus case...                                                                                                                                                                                 \n",
      "473154  Experimental OA was induced by intra-articular...                                                                                                                                                                                 \n",
      "473155  Oral administration of L. casei together with ...                                                                                                                                                                                 \n",
      "473156  Our study provides evidence that L. casei coul...                                                                                                                                                                                 \n",
      "473157  NMDA glutamate receptors (NMDARs) and nicotini...                                                                                                                                                                                 \n",
      "\n",
      "[982205 rows x 3 columns]\n",
      "Dataset({\n",
      "    features: ['Unnamed: 0', 'LAUGHING GAS is the newest thing for kids seeking kicks, the Stanford Daily reports. \"They sniff it.\"So begins a news story in the Los Angeles Times of 26 January 1967. The story continues:\"It\\'s the latest way to travel, or so say a growing group of devotees on the campus,\" the university student paper said. \"It can produce much the same effects as psychedelic drugs, they claim, and it\\'s cheaper to obtain.\"\"One student said he buys the gas, nitrous oxide, from a medical supply house. ;They think I am anesthetizing rats,\\' he explained.\"Campus medical authorities said the gas, sniffed ;in sufficient amounts... could produce all the states of anesthesia, including the final stage-death.\\'\"', 'To determine the extent to which the Brunnstrom recovery stages of upper limb in hemiparetic stroke patients are correlated to neurophysiological measures and the spasticity measure of Modified Modified Ashworth Scale (MMAS).', '__index_level_0__'],\n",
      "    num_rows: 982205\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#trying gsutil for SciFive pretraining corpus\n",
    "# !pip install gsutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "abs_1_16 = pd.read_csv(\"abs_1_16.tsv\", sep='\\t')\n",
    "abs_1_30 = pd.read_csv(\"abs_1_30.tsv\", sep='\\t')\n",
    "# combine these two and then convert to a dataset\n",
    "abs_combined = pd.concat([abs_1_16, abs_1_30])\n",
    "print(abs_combined)\n",
    "# use load_dataset to convert to a dataset\n",
    "from datasets import Dataset\n",
    "abs_combined = Dataset.from_pandas(abs_combined)\n",
    "print(abs_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3f4c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_combined\n",
    "# use load_dataset()\n",
    "abs = load_dataset(\"text\", data_files=\"abs_1_16.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb96e0-b2f2-4a79-ba65-e1c8d6395d54",
   "metadata": {
    "papermill": {
     "duration": 0.003798,
     "end_time": "2024-01-17T00:21:08.308503",
     "exception": false,
     "start_time": "2024-01-17T00:21:08.304705",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's log the dataset also as a table so we can inspect it on the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a744395c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 509063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c7dda87-d70a-470b-a0f9-040af434dc42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:08.340344Z",
     "iopub.status.busy": "2024-01-17T00:21:08.340115Z",
     "iopub.status.idle": "2024-01-17T00:21:08.341747Z",
     "shell.execute_reply": "2024-01-17T00:21:08.341575Z"
    },
    "papermill": {
     "duration": 0.005068,
     "end_time": "2024-01-17T00:21:08.342162",
     "exception": false,
     "start_time": "2024-01-17T00:21:08.337094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509063\n"
     ]
    }
   ],
   "source": [
    "train_dataset = abs[\"train\"]\n",
    "# eval_dataset = abs[\"validation\"]\n",
    "#print sizes\n",
    "print(len(train_dataset))\n",
    "# print(len(eval_dataset))\n",
    "# turn both of these into only half their size\n",
    "# train_dataset = train_dataset.select(range(0, len(train_dataset)//2))\n",
    "# eval_dataset = eval_dataset.select(range(0, len(eval_dataset)//2))\n",
    "\n",
    "# print(len(train_dataset))\n",
    "# print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95e23774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a038eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_prompt(row):\n",
    "#     option_string = \"\"\n",
    "#     for option in row[\"options\"].keys():\n",
    "#         option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "#     row[\"option_string\"] = option_string\n",
    "#     return (\"<QUESTION>{question} {option_string}</QUESTION><ANSWER> ({answer_idx}) {answer}</ANSWER>\").format_map(row)\n",
    "# create_prompt(train_dataset[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8f1a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_prompt_no_answer(row):\n",
    "#     option_string = \"\"\n",
    "#     for option in row[\"options\"].keys():\n",
    "#         option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "#     row[\"option_string\"] = option_string\n",
    "#     return (\"<QUESTION>{question} {option_string}</QUESTION><ANSWER> \").format_map(row)\n",
    "\n",
    "# def return_prompt_no_answer(row):\n",
    "#     return {\"text\": create_prompt_no_answer(row)}\n",
    "\n",
    "# def return_prompt(row):\n",
    "#     return {\"text\": create_prompt(row)}\n",
    "    \n",
    "# test_dataset = eval_dataset.map(return_prompt_no_answer)\n",
    "# # print(test_dataset[0][\"text\"])\n",
    "# train_dataset_with_texts = train_dataset.map(return_prompt)\n",
    "# # print(train_dataset_with_texts[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79dbbb0-6dac-4f78-a862-8201088c9d57",
   "metadata": {
    "papermill": {
     "duration": 0.008656,
     "end_time": "2024-01-17T00:21:12.560165",
     "exception": false,
     "start_time": "2024-01-17T00:21:12.551509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Training the full models is expensive, but if you have a GPU that can fit the full model, you can skip this part. Let's just train the last 8 layers of the model (Llama2-7B has 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5be57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = \"7\"\n",
    "if size == \"7\":\n",
    "    RETRO_layer_ids = [15]\n",
    "elif size == \"13\":\n",
    "    RETRO_layer_ids = [19]\n",
    "elif size == \"70\":\n",
    "    RETRO_layer_ids = [39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37d2f1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from utilities.biollama import BioLlama\n",
    "import torch\n",
    "\n",
    "# questions = [\"Which is the main calcium pump of the sarcoplasmic reticulum? Answer:\"]\n",
    "amended_questions = [\"The main calcium pump of the sarcoplasmic reticulum is \"]\n",
    "questions = amended_questions\n",
    "# answers = [\"Sarcoplasmic reticulum Ca(2+)-ATPase\"] # or \"SERCA\",\"serca2\"\n",
    "\n",
    "prompt = questions[0]\n",
    "# model_id = \"TheBloke/Llama-2-7b-chat-GPTQ\"\n",
    "model_id = \"meta-llama/Llama-2-\" + size +\"b-chat-hf\"\n",
    "chunk_length = 32\n",
    "\n",
    "BioLlama = BioLlama(\n",
    "    model_id=model_id,\n",
    "    chunk_length=chunk_length,\n",
    "    RETRO_layer_ids=RETRO_layer_ids,\n",
    "    training=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76e11ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(BioLlama.model)\n",
    "model = BioLlama.model\n",
    "tokenizer = BioLlama.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:12.566820Z",
     "iopub.status.busy": "2024-01-17T00:21:12.566652Z",
     "iopub.status.idle": "2024-01-17T00:21:12.569626Z",
     "shell.execute_reply": "2024-01-17T00:21:12.569433Z"
    },
    "papermill": {
     "duration": 0.007278,
     "end_time": "2024-01-17T00:21:12.570433",
     "exception": false,
     "start_time": "2024-01-17T00:21:12.563155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freezing layers, currently only works for single unfrozen retro layer\n",
      "printing layer 14 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "\n",
      "printing layer 15 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "cca_attn.q_proj.weight, requires_grad = False\n",
      "cca_attn.k_proj.weight, requires_grad = False\n",
      "cca_attn.v_proj.weight, requires_grad = False\n",
      "cca_attn.o_proj.weight, requires_grad = False\n",
      "pre_cca_layernorm.weight, requires_grad = False\n",
      "\n",
      "printing layer 15 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "cca_attn.q_proj.weight, requires_grad = True\n",
      "cca_attn.k_proj.weight, requires_grad = True\n",
      "cca_attn.v_proj.weight, requires_grad = True\n",
      "cca_attn.o_proj.weight, requires_grad = True\n",
      "pre_cca_layernorm.weight, requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "print(\"freezing layers, currently only works for single unfrozen retro layer\")\n",
    "n_freeze = BioLlama.RETRO_layer_ids[0]\n",
    "# n_freeze = 15\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): \n",
    "    param.requires_grad = True\n",
    "#for every parameter in retro_layer_params, print where in the model it comes from (ie is it from self attention, layer norm, etc)\n",
    "print(\"printing layer 14 params\")\n",
    "for name, param in model.model.layers[14].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\") \n",
    "print(f\"\\nprinting layer {n_freeze} params\")\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")   \n",
    "\n",
    "list_of_params_to_unfreeze = [\n",
    "    \"cca_attn.q_proj.weight\",\n",
    "    \"cca_attn.k_proj.weight\",\n",
    "    \"cca_attn.v_proj.weight\",\n",
    "    \"cca_attn.o_proj.weight\",\n",
    "    \"pre_cca_layernorm.weight\",\n",
    "]\n",
    "\n",
    "for name, param in model.model.layers[n_freeze].named_parameters(): \n",
    "    if name in list_of_params_to_unfreeze:\n",
    "        param.requires_grad = True\n",
    "print(\"\\nprinting layer 15 params\")\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15a29820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-14): 15 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "        (cca_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (pre_cca_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (16-31): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for name, param in model.model.named_parameters(): \n",
    "#     param.requires_grad = True\n",
    "\n",
    "BioLlama.model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:12.579436Z",
     "iopub.status.busy": "2024-01-17T00:21:12.579356Z",
     "iopub.status.idle": "2024-01-17T00:21:12.580922Z",
     "shell.execute_reply": "2024-01-17T00:21:12.580748Z"
    },
    "papermill": {
     "duration": 0.006503,
     "end_time": "2024-01-17T00:21:12.581422",
     "exception": false,
     "start_time": "2024-01-17T00:21:12.574919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "model.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:12.587401Z",
     "iopub.status.busy": "2024-01-17T00:21:12.587279Z",
     "iopub.status.idle": "2024-01-17T00:21:12.591541Z",
     "shell.execute_reply": "2024-01-17T00:21:12.591295Z"
    },
    "papermill": {
     "duration": 0.007943,
     "end_time": "2024-01-17T00:21:12.592091",
     "exception": false,
     "start_time": "2024-01-17T00:21:12.584148",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6805.53M, Trainable: 198.18M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c57893e-6e42-4cda-83c8-1e51a86d2da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:17.982179Z",
     "iopub.status.busy": "2024-01-17T00:21:17.981899Z",
     "iopub.status.idle": "2024-01-17T00:21:17.984608Z",
     "shell.execute_reply": "2024-01-17T00:21:17.984325Z"
    },
    "papermill": {
     "duration": 0.012515,
     "end_time": "2024-01-17T00:21:17.985359",
     "exception": false,
     "start_time": "2024-01-17T00:21:17.972844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5605\n",
      "changing total num size to 1000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "total_num_steps = 11_210 // batch_size\n",
    "print(total_num_steps)\n",
    "\n",
    "\n",
    "total_num_steps = 1000\n",
    "print(f\"changing total num size to {total_num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c86339ca-27e5-496b-9559-3a65bb3c26a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:18.067794Z",
     "iopub.status.busy": "2024-01-17T00:21:18.067685Z",
     "iopub.status.idle": "2024-01-17T00:21:18.072736Z",
     "shell.execute_reply": "2024-01-17T00:21:18.072410Z"
    },
    "papermill": {
     "duration": 0.012575,
     "end_time": "2024-01-17T00:21:18.073701",
     "exception": false,
     "start_time": "2024-01-17T00:21:18.061126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\" + size  + \"/\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size//2,\n",
    "    bf16=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=total_num_steps // 10,\n",
    "    num_train_epochs=10,\n",
    "    max_steps=145320,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=145320,\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=8,\n",
    "    save_strategy=\"epoch\", #changed to epoch so we save every epoch i guess?\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e35a9b70-f36b-4bfd-857f-c80d3450e111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:18.094173Z",
     "iopub.status.busy": "2024-01-17T00:21:18.094032Z",
     "iopub.status.idle": "2024-01-17T00:21:18.336158Z",
     "shell.execute_reply": "2024-01-17T00:21:18.335751Z"
    },
    "papermill": {
     "duration": 0.247046,
     "end_time": "2024-01-17T00:21:18.336774",
     "exception": false,
     "start_time": "2024-01-17T00:21:18.089728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from utils import LLMSampleCB, token_accuracy\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    # eval_dataset=test_dataset,\n",
    "    packing=True,\n",
    "    max_seq_length=1024,\n",
    "    args=training_args,\n",
    "    # formatting_func=create_prompt,\n",
    "    # compute_metrics=token_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5754789-2e15-4bc9-800c-01f8ffc625e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:18.383781Z",
     "iopub.status.busy": "2024-01-17T00:21:18.383531Z",
     "iopub.status.idle": "2024-01-17T00:52:49.183854Z",
     "shell.execute_reply": "2024-01-17T00:52:49.183476Z"
    },
    "papermill": {
     "duration": 1890.805672,
     "end_time": "2024-01-17T00:52:49.184963",
     "exception": false,
     "start_time": "2024-01-17T00:21:18.379291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='145320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    18/145320 00:04 < 12:18:37, 3.28 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     10\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1541\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1543\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/trainer.py:1881\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1879\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss_step\n\u001b[0;32m-> 1881\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_flos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloating_point_ops(inputs))\n\u001b[1;32m   1883\u001b[0m is_last_step_and_steps_less_than_grad_acc \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1884\u001b[0m     steps_in_epoch \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;129;01mand\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m steps_in_epoch\n\u001b[1;32m   1885\u001b[0m )\n\u001b[1;32m   1887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1888\u001b[0m     total_batched_samples \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1889\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1893\u001b[0m     \u001b[38;5;66;03m# the `or` condition of `is_last_step_and_steps_less_than_grad_acc` is not covered\u001b[39;00m\n\u001b[1;32m   1894\u001b[0m     \u001b[38;5;66;03m# in accelerate. So, explicitly enable sync gradients to True in that case.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/trainer.py:3529\u001b[0m, in \u001b[0;36mTrainer.floating_point_ops\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3525\u001b[0m         logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   3527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (loss, logits, labels)\n\u001b[0;32m-> 3529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfloating_point_ops\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Union[torch\u001b[38;5;241m.\u001b[39mTensor, Any]]):\n\u001b[1;32m   3530\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3531\u001b[0m \u001b[38;5;124;03m    For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\u001b[39;00m\n\u001b[1;32m   3532\u001b[0m \u001b[38;5;124;03m    operations for every backward + forward pass. If using another model, either implement such a method in the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3540\u001b[0m \u001b[38;5;124;03m        `int`: The number of floating-point operations.\u001b[39;00m\n\u001b[1;32m   3541\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloating_point_ops\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#very hacky but maybe this will work:\n",
    "tokenizer.model_input_names = ['labels', 'input_ids', 'attention_mask']\n",
    "# trainer.args.train_batch_size = 1\n",
    "# self.args.train_batch_size\n",
    "\n",
    "#also hacky, but could work:\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Starting training\")\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e06dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = \"7\"\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\" + size  + \"/\"\n",
    "RETRO_layer_ids = [15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "684ca6fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:52:49.215531Z",
     "iopub.status.busy": "2024-01-17T00:52:49.215430Z",
     "iopub.status.idle": "2024-01-17T00:53:05.370183Z",
     "shell.execute_reply": "2024-01-17T00:53:05.369679Z"
    },
    "papermill": {
     "duration": 16.162767,
     "end_time": "2024-01-17T00:53:05.371206",
     "exception": false,
     "start_time": "2024-01-17T00:52:49.208439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/service/BioLlama/utilities/finetuning/biollama_training_output/7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath(output_dir))\n",
    "trainer.save_model(output_dir)\n",
    "# !ls -l $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e7619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = \"7\"\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\" + size  + \"/\"\n",
    "RETRO_layer_ids = [15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ff2529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/service/BioLlama/utilities/finetuning/biollama_training_output/7/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.58it/s]\n",
      "Some weights of the model checkpoint at /home/service/BioLlama/utilities/finetuning/biollama_training_output/7/ were not used when initializing LlamaForCausalLM: ['model.layers.15.cca_attn.k_proj.weight', 'model.layers.15.cca_attn.o_proj.weight', 'model.layers.15.cca_attn.q_proj.weight', 'model.layers.15.cca_attn.v_proj.weight', 'model.layers.15.pre_cca_layernorm.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING THE 7B BIOLLAMA WEIGHTS FOR CCA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "#load this local model here and use it to generate some text\n",
    "print(output_dir)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import torch\n",
    "from utilities.biollama import BioLlama\n",
    "\n",
    "chunk_length = 32\n",
    "\n",
    "BioLlama = BioLlama(model_id=output_dir, chunk_length=chunk_length, RETRO_layer_ids = RETRO_layer_ids, training=False, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f20f493d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[38;5;124mn room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m (A) Ampicillin\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m (B) Ceftriaxone\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m (C) Ciprofloxacin\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m (D) Doxycycline\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m (E) Nitrofurantoin</QUESTION>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m<ANSWER> \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m time_before_generation \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m num_tokens, text \u001b[38;5;241m=\u001b[39m BioLlama\u001b[38;5;241m.\u001b[39mgenerate(prompt\u001b[38;5;241m=\u001b[39mprompt, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      4\u001b[0m time_after \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m***Generating***\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/BioLlama/utilities/biollama.py:401\u001b[0m, in \u001b[0;36mBioLlama.generate\u001b[0;34m(self, prompt, max_new_tokens)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minput_ids_biollama \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mprompt_biollama \u001b[38;5;241m=\u001b[39m prompt\n\u001b[0;32m--> 401\u001b[0m generate_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(inputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens, use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# print(f\"generate_ids is {generate_ids}\")\u001b[39;00m\n\u001b[1;32m    403\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(generate_ids[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/generation/utils.py:1525\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1524\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m   1526\u001b[0m         input_ids,\n\u001b[1;32m   1527\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   1528\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[1;32m   1529\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   1530\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m   1531\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m   1532\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[1;32m   1533\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1534\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   1535\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   1536\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1537\u001b[0m     )\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1542\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1543\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1548\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1549\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/generation/utils.py:2658\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2657\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2658\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2660\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "prompt  = '<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \\n (A) Ampicillin\\n (B) Ceftriaxone\\n (C) Ciprofloxacin\\n (D) Doxycycline\\n (E) Nitrofurantoin</QUESTION>\\n<ANSWER> '\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99856974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby? \n",
      " (A) Placing the infant in a supine position on a firm mattress while sleeping\n",
      " (B) Routine postnatal electrocardiogram (ECG)\n",
      " (C) Keeping the infant covered and maintaining a high room temperature\n",
      " (D) Application of a device to maintain the sleeping position\n",
      " (E) Avoiding pacifier use during sleep</QUESTION>\n",
      "<ANSWER> \n",
      " (A) Placing the infant in a supine position on a firm mattress while sleeping\n",
      "</ANSWER>\n",
      "Time taken for generation: 9.476145029067993\n",
      "Tokens per second: 0.10552814429628384\n"
     ]
    }
   ],
   "source": [
    "prompt2 = '<QUESTION>A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby? \\n (A) Placing the infant in a supine position on a firm mattress while sleeping\\n (B) Routine postnatal electrocardiogram (ECG)\\n (C) Keeping the infant covered and maintaining a high room temperature\\n (D) Application of a device to maintain the sleeping position\\n (E) Avoiding pacifier use during sleep</QUESTION>\\n<ANSWER> '\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt2, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad4c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation? \n",
      " (A) Abnormal migration of ventral pancreatic bud\n",
      " (B) Complete failure of proximal duodenum to recanalize\n",
      " (C) Error in neural crest cell migration\n",
      " (D) Abnormal hypertrophy of the pylorus\n",
      " (E) Failure of lateral body folds to move ventrally and fuse in the midline</QUESTION>\n",
      "<ANSWER> \n",
      " (E) Failure of lateral body folds to move ventrally and fuse in the midline</ANSWER>\n",
      "<EXPLANATION>\n",
      "This embryologic error refers to the failure of the later\n",
      "Time taken for generation: 17.52500629425049\n",
      "Tokens per second: 0.05706132044746111\n"
     ]
    }
   ],
   "source": [
    "prompt3 = \"<QUESTION>A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation? \\n (A) Abnormal migration of ventral pancreatic bud\\n (B) Complete failure of proximal duodenum to recanalize\\n (C) Error in neural crest cell migration\\n (D) Abnormal hypertrophy of the pylorus\\n (E) Failure of lateral body folds to move ventrally and fuse in the midline</QUESTION>\\n<ANSWER> \"\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt3, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24725d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A 20-year-old woman presents with menorrhagia for the past several years. She says that her menses “have always been heavy”, and she has experienced easy bruising for as long as she can remember. Family history is significant for her mother, who had similar problems with bruising easily. The patient's vital signs include: heart rate 98/min, respiratory rate 14/min, temperature 36.1°C (96.9°F), and blood pressure 110/87 mm Hg. Physical examination is unremarkable. Laboratory tests show the following: platelet count 200,000/mm3, PT 12 seconds, and PTT 43 seconds. Which of the following is the most likely cause of this patient’s symptoms? \n",
      " (A) Factor V Leiden\n",
      " (B) Hemophilia A\n",
      " (C) Lupus anticoagulant\n",
      " (D) Protein C deficiency\n",
      " (E) Von Willebrand disease</QUESTION>\n",
      "<ANSWER> \n",
      " (E) Von Willebrand disease</ANSWER>\n",
      "Time taken for generation: 5.000464677810669\n",
      "Tokens per second: 0.19998141461481647\n"
     ]
    }
   ],
   "source": [
    "prompt4 = \"<QUESTION>A 20-year-old woman presents with menorrhagia for the past several years. She says that her menses “have always been heavy”, and she has experienced easy bruising for as long as she can remember. Family history is significant for her mother, who had similar problems with bruising easily. The patient's vital signs include: heart rate 98/min, respiratory rate 14/min, temperature 36.1°C (96.9°F), and blood pressure 110/87 mm Hg. Physical examination is unremarkable. Laboratory tests show the following: platelet count 200,000/mm3, PT 12 seconds, and PTT 43 seconds. Which of the following is the most likely cause of this patient’s symptoms? \\n (A) Factor V Leiden\\n (B) Hemophilia A\\n (C) Lupus anticoagulant\\n (D) Protein C deficiency\\n (E) Von Willebrand disease</QUESTION>\\n<ANSWER> \"\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt4, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883afe6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1afa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1928.222791,
   "end_time": "2024-01-17T00:53:06.599361",
   "environment_variables": {},
   "exception": null,
   "input_path": "Alpaca_finetunning_with_WandB_and_HF.ipynb",
   "output_path": "Alpaca_finetunning_with_WandB_and_HF.ipynb",
   "parameters": {},
   "start_time": "2024-01-17T00:20:58.376570",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
