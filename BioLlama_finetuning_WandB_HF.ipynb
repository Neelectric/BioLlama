{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb384650",
   "metadata": {
    "papermill": {
     "duration": 0.002541,
     "end_time": "2024-01-17T00:20:59.083690",
     "exception": false,
     "start_time": "2024-01-17T00:20:59.081149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is taken directly from https://github.com/tcapelle/llm_recipes/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
   "metadata": {
    "papermill": {
     "duration": 0.002618,
     "end_time": "2024-01-17T00:20:59.094857",
     "exception": false,
     "start_time": "2024-01-17T00:20:59.092239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Finetuning Llama-2 to produce BioLlama using HF and WanB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:00.894314Z",
     "iopub.status.busy": "2024-01-17T00:21:00.894173Z",
     "iopub.status.idle": "2024-01-17T00:21:00.896381Z",
     "shell.execute_reply": "2024-01-17T00:21:00.896113Z"
    },
    "papermill": {
     "duration": 0.005637,
     "end_time": "2024-01-17T00:21:00.896990",
     "exception": false,
     "start_time": "2024-01-17T00:21:00.891353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Benchmark from MedQA-USMLE/US/train.jsonl\n",
      "Benchmark contains 10178 questions, made up of 10178 with 5 options and 0 with non-5 options\n"
     ]
    }
   ],
   "source": [
    "# !pip install wandb transformers trl datasets \"protobuf==3.20.3\" evaluate\n",
    "# !wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json\n",
    "from utilities.parse_benchmark import parse_benchmark\n",
    "\n",
    "benchmark = \"MedQA-5\"\n",
    "# benchmark = \"MedMCQA\"\n",
    "benchmark_questions, benchmark_answers = parse_benchmark(benchmark, \"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a154f968-da0f-4bdc-bf45-e499d95f0606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:01.024789Z",
     "iopub.status.busy": "2024-01-17T00:21:01.024684Z",
     "iopub.status.idle": "2024-01-17T00:21:07.425408Z",
     "shell.execute_reply": "2024-01-17T00:21:07.425094Z"
    },
    "papermill": {
     "duration": 6.410539,
     "end_time": "2024-01-17T00:21:07.426311",
     "exception": false,
     "start_time": "2024-01-17T00:21:01.015772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnelectric\u001b[0m (\u001b[33mneelectric\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/service/BioLlama/wandb/run-20240303_110812-fih3i5fs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/neelectric/biollama_ft/runs/fih3i5fs' target=\"_blank\">deep-leaf-158</a></strong> to <a href='https://wandb.ai/neelectric/biollama_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/neelectric/biollama_ft' target=\"_blank\">https://wandb.ai/neelectric/biollama_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/neelectric/biollama_ft/runs/fih3i5fs' target=\"_blank\">https://wandb.ai/neelectric/biollama_ft/runs/fih3i5fs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/neelectric/biollama_ft/runs/fih3i5fs?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4aeae9a2d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"biollama_ft\", # the project I am working on\n",
    "           tags=[\"hf_sft\", \"BioLlama\"]) # the Hyperparameters I want to keep track of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811291cc-b7ce-422a-8971-3cbf9fe10a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:07.440755Z",
     "iopub.status.busy": "2024-01-17T00:21:07.440567Z",
     "iopub.status.idle": "2024-01-17T00:21:08.281875Z",
     "shell.execute_reply": "2024-01-17T00:21:08.281591Z"
    },
    "papermill": {
     "duration": 0.852358,
     "end_time": "2024-01-17T00:21:08.282858",
     "exception": false,
     "start_time": "2024-01-17T00:21:07.430500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if benchmark == \"MedQA-4\":\n",
    "    artifact_dir = os.getcwd() + \"/benchmarks/MedQA-4-option/\"\n",
    "elif benchmark == \"MedQA-5\":\n",
    "    artifact_dir = os.getcwd() + \"/benchmarks/MedQA-USMLE/\"\n",
    "elif benchmark == \"MedMCQA\":\n",
    "    artifact_dir = os.getcwd() + \"/benchmarks/MedMCQA/\"\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_dir=artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3f4c49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 10178\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 1272\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 1273\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c7dda87-d70a-470b-a0f9-040af434dc42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:08.340344Z",
     "iopub.status.busy": "2024-01-17T00:21:08.340115Z",
     "iopub.status.idle": "2024-01-17T00:21:08.341747Z",
     "shell.execute_reply": "2024-01-17T00:21:08.341575Z"
    },
    "papermill": {
     "duration": 0.005068,
     "end_time": "2024-01-17T00:21:08.342162",
     "exception": false,
     "start_time": "2024-01-17T00:21:08.337094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10178\n",
      "1272\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a038eb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<QUESTION>A pulmonary autopsy specimen from a 58-year-old woman who died of acute hypoxic respiratory failure was examined. She had recently undergone surgery for a fractured femur 3 months ago. Initial hospital course was uncomplicated, and she was discharged to a rehab facility in good health. Shortly after discharge home from rehab, she developed sudden shortness of breath and had cardiac arrest. Resuscitation was unsuccessful. On histological examination of lung tissue, fibrous connective tissue around the lumen of the pulmonary artery is observed. Which of the following is the most likely pathogenesis for the present findings? \\n (A) Thromboembolism\\n (B) Pulmonary ischemia\\n (C) Pulmonary hypertension\\n (D) Pulmonary passive congestion\\n (E) Pulmonary hemorrhage</QUESTION><ANSWER> (A) Thromboembolism</ANSWER>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_prompt(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"<QUESTION>{question} {option_string}</QUESTION><ANSWER> ({answer_idx}) {answer}</ANSWER>\").format_map(row)\n",
    "\n",
    "if benchmark == \"MedMCQA\":\n",
    "    def create_prompt(row):\n",
    "        option_string = \"\\n(1) \" + row['opa']\n",
    "        option_string += \"\\n(2) \" + row['opb']\n",
    "        option_string += \"\\n(3) \" + row['opc']\n",
    "        option_string += \"\\n(4) \" + row['opd']\n",
    "        row[\"option_string\"] = option_string\n",
    "        if row['cop'] == 1:\n",
    "            row['answer'] = row['opa']\n",
    "        elif row['cop'] == 2:\n",
    "            row['answer'] = row['opb']\n",
    "        elif row['cop'] == 3:\n",
    "            row['answer'] = row['opc']\n",
    "        elif row['cop'] == 4:\n",
    "            row['answer'] = row['opd']\n",
    "        return (\"<QUESTION>{question} {option_string}</QUESTION><ANSWER> ({cop}) {answer}</ANSWER>\").format_map(row)\n",
    "\n",
    "create_prompt(train_dataset[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f1a9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \n",
      " (A) Ampicillin\n",
      " (B) Ceftriaxone\n",
      " (C) Ciprofloxacin\n",
      " (D) Doxycycline\n",
      " (E) Nitrofurantoin</QUESTION><ANSWER> (E) Nitrofurantoin</ANSWER>\n"
     ]
    }
   ],
   "source": [
    "def create_prompt_no_answer(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"<QUESTION>{question} {option_string}</QUESTION><ANSWER> \").format_map(row)\n",
    "\n",
    "def return_prompt_no_answer(row):\n",
    "    return {\"text\": create_prompt_no_answer(row)}\n",
    "\n",
    "def return_prompt(row):\n",
    "    return {\"text\": create_prompt(row)}\n",
    "    \n",
    "if benchmark == \"MedQA\":\n",
    "    test_dataset = eval_dataset.map(return_prompt_no_answer)\n",
    "train_dataset_with_texts = train_dataset.map(return_prompt)\n",
    "print(train_dataset_with_texts[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5be57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = \"7\"\n",
    "if size == \"7\":\n",
    "    RETRO_layer_ids = [15]\n",
    "elif size == \"13\":\n",
    "    RETRO_layer_ids = [19]\n",
    "elif size == \"70\":\n",
    "    RETRO_layer_ids = [39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d2f1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.09s/it]\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from utilities.biollama import BioLlama\n",
    "import torch\n",
    "\n",
    "amended_questions = [\"The main calcium pump of the sarcoplasmic reticulum is \"]\n",
    "# answers = [\"Sarcoplasmic reticulum Ca(2+)-ATPase\"] # or \"SERCA\",\"serca2\"\n",
    "prompt = amended_questions[0]\n",
    "model_id = \"meta-llama/Llama-2-\" + size +\"b-chat-hf\"\n",
    "chunk_length = 32\n",
    "\n",
    "BioLlama = BioLlama(\n",
    "    model_id=model_id,\n",
    "    chunk_length=chunk_length,\n",
    "    RETRO_layer_ids=RETRO_layer_ids,\n",
    "    training=True,\n",
    "    torch_dtype=torch.float32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e11ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BioLlama.model\n",
    "tokenizer = BioLlama.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:12.566820Z",
     "iopub.status.busy": "2024-01-17T00:21:12.566652Z",
     "iopub.status.idle": "2024-01-17T00:21:12.569626Z",
     "shell.execute_reply": "2024-01-17T00:21:12.569433Z"
    },
    "papermill": {
     "duration": 0.007278,
     "end_time": "2024-01-17T00:21:12.570433",
     "exception": false,
     "start_time": "2024-01-17T00:21:12.563155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freezing layers, currently only works for single unfrozen retro layer\n",
      "\n",
      "printing layer 15 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "cca_attn.q_proj.weight, requires_grad = False\n",
      "cca_attn.k_proj.weight, requires_grad = False\n",
      "cca_attn.v_proj.weight, requires_grad = False\n",
      "cca_attn.o_proj.weight, requires_grad = False\n",
      "pre_cca_layernorm.weight, requires_grad = False\n",
      "\n",
      "printing layer 15 params\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "cca_attn.q_proj.weight, requires_grad = True\n",
      "cca_attn.k_proj.weight, requires_grad = True\n",
      "cca_attn.v_proj.weight, requires_grad = True\n",
      "cca_attn.o_proj.weight, requires_grad = True\n",
      "pre_cca_layernorm.weight, requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "print(\"freezing layers, currently only works for single unfrozen retro layer\")\n",
    "n_freeze = BioLlama.RETRO_layer_ids[0]\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): \n",
    "    param.requires_grad = True\n",
    "#for every parameter in retro_layer_params, print where in the model it comes from (ie is it from self attention, layer norm, etc)\n",
    "print(f\"\\nprinting layer {n_freeze} params\")\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")   \n",
    "\n",
    "list_of_params_to_unfreeze = [\n",
    "    \"cca_attn.q_proj.weight\",\n",
    "    \"cca_attn.k_proj.weight\",\n",
    "    \"cca_attn.v_proj.weight\",\n",
    "    \"cca_attn.o_proj.weight\",\n",
    "    \"pre_cca_layernorm.weight\",\n",
    "]\n",
    "\n",
    "for name, param in model.model.layers[n_freeze].named_parameters(): \n",
    "    if name in list_of_params_to_unfreeze:\n",
    "        param.requires_grad = True\n",
    "print(\"\\nprinting layer 15 params\")\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15a29820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight, requires_grad = False\n",
      "layers.0.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.0.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.0.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.0.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.0.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.0.mlp.up_proj.weight, requires_grad = False\n",
      "layers.0.mlp.down_proj.weight, requires_grad = False\n",
      "layers.0.input_layernorm.weight, requires_grad = False\n",
      "layers.0.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.1.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.1.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.1.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.1.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.1.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.1.mlp.up_proj.weight, requires_grad = False\n",
      "layers.1.mlp.down_proj.weight, requires_grad = False\n",
      "layers.1.input_layernorm.weight, requires_grad = False\n",
      "layers.1.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.2.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.2.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.2.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.2.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.2.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.2.mlp.up_proj.weight, requires_grad = False\n",
      "layers.2.mlp.down_proj.weight, requires_grad = False\n",
      "layers.2.input_layernorm.weight, requires_grad = False\n",
      "layers.2.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.3.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.3.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.3.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.3.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.3.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.3.mlp.up_proj.weight, requires_grad = False\n",
      "layers.3.mlp.down_proj.weight, requires_grad = False\n",
      "layers.3.input_layernorm.weight, requires_grad = False\n",
      "layers.3.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.4.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.4.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.4.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.4.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.4.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.4.mlp.up_proj.weight, requires_grad = False\n",
      "layers.4.mlp.down_proj.weight, requires_grad = False\n",
      "layers.4.input_layernorm.weight, requires_grad = False\n",
      "layers.4.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.5.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.5.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.5.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.5.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.5.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.5.mlp.up_proj.weight, requires_grad = False\n",
      "layers.5.mlp.down_proj.weight, requires_grad = False\n",
      "layers.5.input_layernorm.weight, requires_grad = False\n",
      "layers.5.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.6.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.6.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.6.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.6.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.6.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.6.mlp.up_proj.weight, requires_grad = False\n",
      "layers.6.mlp.down_proj.weight, requires_grad = False\n",
      "layers.6.input_layernorm.weight, requires_grad = False\n",
      "layers.6.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.7.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.7.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.7.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.7.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.7.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.7.mlp.up_proj.weight, requires_grad = False\n",
      "layers.7.mlp.down_proj.weight, requires_grad = False\n",
      "layers.7.input_layernorm.weight, requires_grad = False\n",
      "layers.7.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.8.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.8.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.8.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.8.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.8.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.8.mlp.up_proj.weight, requires_grad = False\n",
      "layers.8.mlp.down_proj.weight, requires_grad = False\n",
      "layers.8.input_layernorm.weight, requires_grad = False\n",
      "layers.8.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.9.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.9.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.9.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.9.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.9.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.9.mlp.up_proj.weight, requires_grad = False\n",
      "layers.9.mlp.down_proj.weight, requires_grad = False\n",
      "layers.9.input_layernorm.weight, requires_grad = False\n",
      "layers.9.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.10.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.10.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.10.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.10.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.10.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.10.mlp.up_proj.weight, requires_grad = False\n",
      "layers.10.mlp.down_proj.weight, requires_grad = False\n",
      "layers.10.input_layernorm.weight, requires_grad = False\n",
      "layers.10.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.11.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.11.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.11.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.11.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.11.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.11.mlp.up_proj.weight, requires_grad = False\n",
      "layers.11.mlp.down_proj.weight, requires_grad = False\n",
      "layers.11.input_layernorm.weight, requires_grad = False\n",
      "layers.11.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.12.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.12.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.12.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.12.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.12.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.12.mlp.up_proj.weight, requires_grad = False\n",
      "layers.12.mlp.down_proj.weight, requires_grad = False\n",
      "layers.12.input_layernorm.weight, requires_grad = False\n",
      "layers.12.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.13.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.13.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.13.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.13.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.13.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.13.mlp.up_proj.weight, requires_grad = False\n",
      "layers.13.mlp.down_proj.weight, requires_grad = False\n",
      "layers.13.input_layernorm.weight, requires_grad = False\n",
      "layers.13.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.14.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.14.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.14.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.14.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.14.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.14.mlp.up_proj.weight, requires_grad = False\n",
      "layers.14.mlp.down_proj.weight, requires_grad = False\n",
      "layers.14.input_layernorm.weight, requires_grad = False\n",
      "layers.14.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.15.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.15.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.15.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.15.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.15.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.15.mlp.up_proj.weight, requires_grad = False\n",
      "layers.15.mlp.down_proj.weight, requires_grad = False\n",
      "layers.15.input_layernorm.weight, requires_grad = False\n",
      "layers.15.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.15.cca_attn.q_proj.weight, requires_grad = True\n",
      "layers.15.cca_attn.k_proj.weight, requires_grad = True\n",
      "layers.15.cca_attn.v_proj.weight, requires_grad = True\n",
      "layers.15.cca_attn.o_proj.weight, requires_grad = True\n",
      "layers.15.pre_cca_layernorm.weight, requires_grad = True\n",
      "layers.16.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.16.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.16.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.16.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.16.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.16.mlp.up_proj.weight, requires_grad = False\n",
      "layers.16.mlp.down_proj.weight, requires_grad = False\n",
      "layers.16.input_layernorm.weight, requires_grad = False\n",
      "layers.16.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.17.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.17.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.17.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.17.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.17.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.17.mlp.up_proj.weight, requires_grad = False\n",
      "layers.17.mlp.down_proj.weight, requires_grad = False\n",
      "layers.17.input_layernorm.weight, requires_grad = False\n",
      "layers.17.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.18.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.18.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.18.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.18.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.18.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.18.mlp.up_proj.weight, requires_grad = False\n",
      "layers.18.mlp.down_proj.weight, requires_grad = False\n",
      "layers.18.input_layernorm.weight, requires_grad = False\n",
      "layers.18.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.19.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.19.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.19.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.19.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.19.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.19.mlp.up_proj.weight, requires_grad = False\n",
      "layers.19.mlp.down_proj.weight, requires_grad = False\n",
      "layers.19.input_layernorm.weight, requires_grad = False\n",
      "layers.19.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.20.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.20.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.20.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.20.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.20.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.20.mlp.up_proj.weight, requires_grad = False\n",
      "layers.20.mlp.down_proj.weight, requires_grad = False\n",
      "layers.20.input_layernorm.weight, requires_grad = False\n",
      "layers.20.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.21.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.21.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.21.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.21.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.21.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.21.mlp.up_proj.weight, requires_grad = False\n",
      "layers.21.mlp.down_proj.weight, requires_grad = False\n",
      "layers.21.input_layernorm.weight, requires_grad = False\n",
      "layers.21.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.22.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.22.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.22.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.22.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.22.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.22.mlp.up_proj.weight, requires_grad = False\n",
      "layers.22.mlp.down_proj.weight, requires_grad = False\n",
      "layers.22.input_layernorm.weight, requires_grad = False\n",
      "layers.22.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.23.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.23.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.23.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.23.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.23.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.23.mlp.up_proj.weight, requires_grad = False\n",
      "layers.23.mlp.down_proj.weight, requires_grad = False\n",
      "layers.23.input_layernorm.weight, requires_grad = False\n",
      "layers.23.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.24.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.24.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.24.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.24.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.24.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.24.mlp.up_proj.weight, requires_grad = False\n",
      "layers.24.mlp.down_proj.weight, requires_grad = False\n",
      "layers.24.input_layernorm.weight, requires_grad = False\n",
      "layers.24.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.25.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.25.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.25.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.25.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.25.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.25.mlp.up_proj.weight, requires_grad = False\n",
      "layers.25.mlp.down_proj.weight, requires_grad = False\n",
      "layers.25.input_layernorm.weight, requires_grad = False\n",
      "layers.25.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.26.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.26.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.26.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.26.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.26.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.26.mlp.up_proj.weight, requires_grad = False\n",
      "layers.26.mlp.down_proj.weight, requires_grad = False\n",
      "layers.26.input_layernorm.weight, requires_grad = False\n",
      "layers.26.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.27.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.27.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.27.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.27.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.27.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.27.mlp.up_proj.weight, requires_grad = False\n",
      "layers.27.mlp.down_proj.weight, requires_grad = False\n",
      "layers.27.input_layernorm.weight, requires_grad = False\n",
      "layers.27.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.28.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.28.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.28.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.28.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.28.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.28.mlp.up_proj.weight, requires_grad = False\n",
      "layers.28.mlp.down_proj.weight, requires_grad = False\n",
      "layers.28.input_layernorm.weight, requires_grad = False\n",
      "layers.28.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.29.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.29.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.29.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.29.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.29.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.29.mlp.up_proj.weight, requires_grad = False\n",
      "layers.29.mlp.down_proj.weight, requires_grad = False\n",
      "layers.29.input_layernorm.weight, requires_grad = False\n",
      "layers.29.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.30.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.30.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.30.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.30.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.30.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.30.mlp.up_proj.weight, requires_grad = False\n",
      "layers.30.mlp.down_proj.weight, requires_grad = False\n",
      "layers.30.input_layernorm.weight, requires_grad = False\n",
      "layers.30.post_attention_layernorm.weight, requires_grad = False\n",
      "layers.31.self_attn.q_proj.weight, requires_grad = False\n",
      "layers.31.self_attn.k_proj.weight, requires_grad = False\n",
      "layers.31.self_attn.v_proj.weight, requires_grad = False\n",
      "layers.31.self_attn.o_proj.weight, requires_grad = False\n",
      "layers.31.mlp.gate_proj.weight, requires_grad = False\n",
      "layers.31.mlp.up_proj.weight, requires_grad = False\n",
      "layers.31.mlp.down_proj.weight, requires_grad = False\n",
      "layers.31.input_layernorm.weight, requires_grad = False\n",
      "layers.31.post_attention_layernorm.weight, requires_grad = False\n",
      "norm.weight, requires_grad = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-14): 15 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "        (cca_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (pre_cca_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (16-31): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, param in model.model.named_parameters(): \n",
    "    # param.requires_grad = True\n",
    "    #print if needs grad\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")\n",
    "\n",
    "BioLlama.model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:12.579436Z",
     "iopub.status.busy": "2024-01-17T00:21:12.579356Z",
     "iopub.status.idle": "2024-01-17T00:21:12.580922Z",
     "shell.execute_reply": "2024-01-17T00:21:12.580748Z"
    },
    "papermill": {
     "duration": 0.006503,
     "end_time": "2024-01-17T00:21:12.581422",
     "exception": false,
     "start_time": "2024-01-17T00:21:12.574919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "model.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:12.587401Z",
     "iopub.status.busy": "2024-01-17T00:21:12.587279Z",
     "iopub.status.idle": "2024-01-17T00:21:12.591541Z",
     "shell.execute_reply": "2024-01-17T00:21:12.591295Z"
    },
    "papermill": {
     "duration": 0.007943,
     "end_time": "2024-01-17T00:21:12.592091",
     "exception": false,
     "start_time": "2024-01-17T00:21:12.584148",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6805.53M, Trainable: 198.18M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c57893e-6e42-4cda-83c8-1e51a86d2da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:17.982179Z",
     "iopub.status.busy": "2024-01-17T00:21:17.981899Z",
     "iopub.status.idle": "2024-01-17T00:21:17.984608Z",
     "shell.execute_reply": "2024-01-17T00:21:17.984325Z"
    },
    "papermill": {
     "duration": 0.012515,
     "end_time": "2024-01-17T00:21:17.985359",
     "exception": false,
     "start_time": "2024-01-17T00:21:17.972844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5605\n",
      "changing total num size to 10178\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "total_num_steps = 11_210 // batch_size\n",
    "print(total_num_steps)\n",
    "\n",
    "if benchmark == \"MedQA-4\" or \"MedQA-5\":\n",
    "    total_num_steps = 10178\n",
    "elif benchmark == \"MedMCQA\":\n",
    "    total_num_steps = 100000\n",
    "print(f\"changing total num size to {total_num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c86339ca-27e5-496b-9559-3a65bb3c26a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:18.067794Z",
     "iopub.status.busy": "2024-01-17T00:21:18.067685Z",
     "iopub.status.idle": "2024-01-17T00:21:18.072736Z",
     "shell.execute_reply": "2024-01-17T00:21:18.072410Z"
    },
    "papermill": {
     "duration": 0.012575,
     "end_time": "2024-01-17T00:21:18.073701",
     "exception": false,
     "start_time": "2024-01-17T00:21:18.061126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\" + size  + \"/\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size//2,\n",
    "    bf16=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=total_num_steps // 10,\n",
    "    num_train_epochs=10,\n",
    "    max_steps=total_num_steps,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=5000,\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=8,\n",
    "    save_strategy=\"epoch\", #changed to epoch so we save every epoch i guess?\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e35a9b70-f36b-4bfd-857f-c80d3450e111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:18.094173Z",
     "iopub.status.busy": "2024-01-17T00:21:18.094032Z",
     "iopub.status.idle": "2024-01-17T00:21:18.336158Z",
     "shell.execute_reply": "2024-01-17T00:21:18.335751Z"
    },
    "papermill": {
     "duration": 0.247046,
     "end_time": "2024-01-17T00:21:18.336774",
     "exception": false,
     "start_time": "2024-01-17T00:21:18.089728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from utils import LLMSampleCB, token_accuracy\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset_with_texts,\n",
    "    dataset_text_field=\"text\",\n",
    "    # eval_dataset=test_dataset,\n",
    "    packing=True,\n",
    "    max_seq_length=1024,\n",
    "    args=training_args,\n",
    "    formatting_func=create_prompt,\n",
    "    # compute_metrics=token_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5754789-2e15-4bc9-800c-01f8ffc625e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:21:18.383781Z",
     "iopub.status.busy": "2024-01-17T00:21:18.383531Z",
     "iopub.status.idle": "2024-01-17T00:52:49.183854Z",
     "shell.execute_reply": "2024-01-17T00:52:49.183476Z"
    },
    "papermill": {
     "duration": 1890.805672,
     "end_time": "2024-01-17T00:52:49.184963",
     "exception": false,
     "start_time": "2024-01-17T00:21:18.379291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='10178' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  136/10178 00:42 < 53:04, 3.15 it/s, Epoch 0.03/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.655400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.720900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.690200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.577100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.623600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.539200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.622400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.540800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.557200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.494400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     10\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1541\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1543\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/trainer.py:2772\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2772\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   2774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2775\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/trainer.py:2795\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2794\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2795\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   2796\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2797\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/accelerate/utils/operations.py:687\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/accelerate/utils/operations.py:675\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/BioLlama/utilities/biollama.py:23\u001b[0m, in \u001b[0;36mmodel_new_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids_biollama \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids or labels not found in kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1183\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1180\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1184\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1185\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1186\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1187\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1188\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1189\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1190\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1191\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1192\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1193\u001b[0m )\n\u001b[1;32m   1195\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1060\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m-> 1060\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1061\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1062\u001b[0m         hidden_states,\n\u001b[1;32m   1063\u001b[0m         attention_mask,\n\u001b[1;32m   1064\u001b[0m         position_ids,\n\u001b[1;32m   1065\u001b[0m         past_key_values,\n\u001b[1;32m   1066\u001b[0m         output_attentions,\n\u001b[1;32m   1067\u001b[0m         use_cache,\n\u001b[1;32m   1068\u001b[0m     )\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1070\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m   1071\u001b[0m         hidden_states,\n\u001b[1;32m   1072\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1076\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1077\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:451\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m         )\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CheckpointFunction\u001b[38;5;241m.\u001b[39mapply(function, preserve, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    454\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    455\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:230\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    227\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 230\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m run_function(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/BioLlama/utilities/biollama.py:206\u001b[0m, in \u001b[0;36mRETRO_layer_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;66;03m# if we are processing prompts in a batch (for eg during training), adapt CCA\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     first_prompts_states \u001b[38;5;241m=\u001b[39m cca_forward_true(\u001b[38;5;28mself\u001b[39m, input_ids, hidden_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    208\u001b[0m         next_prompt_states \u001b[38;5;241m=\u001b[39m cca_forward_true(\u001b[38;5;28mself\u001b[39m, input_ids, hidden_states[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/BioLlama/utilities/biollama.py:84\u001b[0m, in \u001b[0;36mcca_forward_true\u001b[0;34m(self, input_ids, hidden_states)\u001b[0m\n\u001b[1;32m     82\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_cca_layernorm\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     83\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_cca_layernorm(hidden_states)\n\u001b[0;32m---> 84\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(element) \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m input_ids[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     85\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_ids)\n\u001b[1;32m     86\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiollama\u001b[38;5;241m.\u001b[39mchunk_length\n",
      "File \u001b[0;32m~/BioLlama/utilities/biollama.py:84\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_cca_layernorm\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     83\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_cca_layernorm(hidden_states)\n\u001b[0;32m---> 84\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(element) \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m input_ids[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     85\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_ids)\n\u001b[1;32m     86\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiollama\u001b[38;5;241m.\u001b[39mchunk_length\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#very hacky but maybe this will work:\n",
    "tokenizer.model_input_names = ['labels', 'input_ids', 'attention_mask']\n",
    "# trainer.args.train_batch_size = 1\n",
    "# self.args.train_batch_size\n",
    "\n",
    "#also hacky, but could work:\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Starting training\")\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca3011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MedQA-5'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e06dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = \"7\"\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\" + benchmark + \"7\" + size  + \"/\"\n",
    "RETRO_layer_ids = [15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ca6fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:52:49.215531Z",
     "iopub.status.busy": "2024-01-17T00:52:49.215430Z",
     "iopub.status.idle": "2024-01-17T00:53:05.370183Z",
     "shell.execute_reply": "2024-01-17T00:53:05.369679Z"
    },
    "papermill": {
     "duration": 16.162767,
     "end_time": "2024-01-17T00:53:05.371206",
     "exception": false,
     "start_time": "2024-01-17T00:52:49.208439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/service/BioLlama/utilities/finetuning/biollama_training_output/7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath(output_dir))\n",
    "trainer.save_model(output_dir)\n",
    "# !ls -l $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = \"7\"\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/biollama_training_output/\" + size  + \"/\"\n",
    "RETRO_layer_ids = [15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff2529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/service/BioLlama/utilities/finetuning/biollama_training_output/7/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:03<00:00,  1.83it/s]\n",
      "Some weights of the model checkpoint at /home/service/BioLlama/utilities/finetuning/biollama_training_output/7/ were not used when initializing LlamaForCausalLM: ['model.layers.15.cca_attn.k_proj.weight', 'model.layers.15.cca_attn.o_proj.weight', 'model.layers.15.cca_attn.q_proj.weight', 'model.layers.15.cca_attn.v_proj.weight', 'model.layers.15.pre_cca_layernorm.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING THE 7B BIOLLAMA WEIGHTS FOR CCA IN FLOAT32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "#load this local model here and use it to generate some text\n",
    "print(output_dir)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import torch\n",
    "from utilities.biollama import BioLlama\n",
    "\n",
    "chunk_length = 32\n",
    "\n",
    "BioLlama = BioLlama(model_id=output_dir, \n",
    "    chunk_length=chunk_length, \n",
    "    RETRO_layer_ids = RETRO_layer_ids, \n",
    "    training=False, \n",
    "    torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f493d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \n",
      " (A) Ampicillin\n",
      " (B) Ceftriaxone\n",
      " (C) Ciprofloxacin\n",
      " (D) Doxycycline\n",
      " (E) Nitrofurantoin</QUESTION>\n",
      "<ANSWER> \n",
      " (C) Ciprofloxacin</ANSWER>\n",
      "Time taken for generation: 5.607628583908081\n",
      "Tokens per second: 3.20991301949877\n"
     ]
    }
   ],
   "source": [
    "prompt  = '<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \\n (A) Ampicillin\\n (B) Ceftriaxone\\n (C) Ciprofloxacin\\n (D) Doxycycline\\n (E) Nitrofurantoin</QUESTION>\\n<ANSWER> '\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99856974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby? \n",
      " (A) Placing the infant in a supine position on a firm mattress while sleeping\n",
      " (B) Routine postnatal electrocardiogram (ECG)\n",
      " (C) Keeping the infant covered and maintaining a high room temperature\n",
      " (D) Application of a device to maintain the sleeping position\n",
      " (E) Avoiding pacifier use during sleep</QUESTION>\n",
      "<ANSWER> \n",
      " (C) Keeping the infant covered and maintaining a high room temperature\n",
      "</ANSWER>\n",
      "Time taken for generation: 1.8624725341796875\n",
      "Tokens per second: 12.349175398782556\n"
     ]
    }
   ],
   "source": [
    "prompt2 = '<QUESTION>A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby? \\n (A) Placing the infant in a supine position on a firm mattress while sleeping\\n (B) Routine postnatal electrocardiogram (ECG)\\n (C) Keeping the infant covered and maintaining a high room temperature\\n (D) Application of a device to maintain the sleeping position\\n (E) Avoiding pacifier use during sleep</QUESTION>\\n<ANSWER> '\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt2, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad4c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation? \n",
      " (A) Abnormal migration of ventral pancreatic bud\n",
      " (B) Complete failure of proximal duodenum to recanalize\n",
      " (C) Error in neural crest cell migration\n",
      " (D) Abnormal hypertrophy of the pylorus\n",
      " (E) Failure of lateral body folds to move ventrally and fuse in the midline</QUESTION>\n",
      "<ANSWER> \n",
      " (C) Error in neural crest cell migration</ANSWER>\n",
      "Time taken for generation: 3.063973903656006\n",
      "Tokens per second: 5.548350127824261\n"
     ]
    }
   ],
   "source": [
    "prompt3 = \"<QUESTION>A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation? \\n (A) Abnormal migration of ventral pancreatic bud\\n (B) Complete failure of proximal duodenum to recanalize\\n (C) Error in neural crest cell migration\\n (D) Abnormal hypertrophy of the pylorus\\n (E) Failure of lateral body folds to move ventrally and fuse in the midline</QUESTION>\\n<ANSWER> \"\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt3, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24725d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generating***\n",
      "<QUESTION>A 20-year-old woman presents with menorrhagia for the past several years. She says that her menses “have always been heavy”, and she has experienced easy bruising for as long as she can remember. Family history is significant for her mother, who had similar problems with bruising easily. The patient's vital signs include: heart rate 98/min, respiratory rate 14/min, temperature 36.1°C (96.9°F), and blood pressure 110/87 mm Hg. Physical examination is unremarkable. Laboratory tests show the following: platelet count 200,000/mm3, PT 12 seconds, and PTT 43 seconds. Which of the following is the most likely cause of this patient’s symptoms? \n",
      " (A) Factor V Leiden\n",
      " (B) Hemophilia A\n",
      " (C) Lupus anticoagulant\n",
      " (D) Protein C deficiency\n",
      " (E) Von Willebrand disease</QUESTION>\n",
      "<ANSWER> \n",
      " (E) Von Willebrand disease</ANSWER>\n",
      "Time taken for generation: 2.9569287300109863\n",
      "Tokens per second: 4.734642353029585\n"
     ]
    }
   ],
   "source": [
    "prompt4 = \"<QUESTION>A 20-year-old woman presents with menorrhagia for the past several years. She says that her menses “have always been heavy”, and she has experienced easy bruising for as long as she can remember. Family history is significant for her mother, who had similar problems with bruising easily. The patient's vital signs include: heart rate 98/min, respiratory rate 14/min, temperature 36.1°C (96.9°F), and blood pressure 110/87 mm Hg. Physical examination is unremarkable. Laboratory tests show the following: platelet count 200,000/mm3, PT 12 seconds, and PTT 43 seconds. Which of the following is the most likely cause of this patient’s symptoms? \\n (A) Factor V Leiden\\n (B) Hemophilia A\\n (C) Lupus anticoagulant\\n (D) Protein C deficiency\\n (E) Von Willebrand disease</QUESTION>\\n<ANSWER> \"\n",
    "time_before_generation = time.time()\n",
    "num_tokens, text = BioLlama.generate(prompt=prompt4, max_new_tokens=50)\n",
    "time_after = time.time()\n",
    "\n",
    "print(\"***Generating***\")\n",
    "print(text)\n",
    "print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883afe6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1afa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1928.222791,
   "end_time": "2024-01-17T00:53:06.599361",
   "environment_variables": {},
   "exception": null,
   "input_path": "Alpaca_finetunning_with_WandB_and_HF.ipynb",
   "output_path": "Alpaca_finetunning_with_WandB_and_HF.ipynb",
   "parameters": {},
   "start_time": "2024-01-17T00:20:58.376570",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
