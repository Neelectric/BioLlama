{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb384650",
   "metadata": {
    "papermill": {
     "duration": 0.002616,
     "end_time": "2024-02-07T18:56:24.775678",
     "exception": false,
     "start_time": "2024-02-07T18:56:24.773062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is taken directly from https://github.com/tcapelle/llm_recipes/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
   "metadata": {
    "papermill": {
     "duration": 0.002732,
     "end_time": "2024-02-07T18:56:24.787197",
     "exception": false,
     "start_time": "2024-02-07T18:56:24.784465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# From Llama to Alpaca: Finetunning and LLM with Weights & Biases\n",
    "In this notebooks you will learn how to finetune a pretrained LLama model on an Instruction dataset. We will use an updated version of the Alpaca dataset that, instead of davinci-003 (GPT3) generations uses GPT4 to get an even better instruction dataset! More details on the [official repo page](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data)\n",
    "\n",
    "> This notebook requires a A100/A10 GPU with at least 24GB of memory. You could tweak the params down and run on a T4 but it would take very long time\n",
    "\n",
    "This notebooks has a companion project and [report](wandb.me/alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7744abe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:24.792053Z",
     "iopub.status.busy": "2024-02-07T18:56:24.791891Z",
     "iopub.status.idle": "2024-02-07T18:56:24.794411Z",
     "shell.execute_reply": "2024-02-07T18:56:24.794038Z"
    },
    "papermill": {
     "duration": 0.005831,
     "end_time": "2024-02-07T18:56:24.795019",
     "exception": false,
     "start_time": "2024-02-07T18:56:24.789188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install wandb transformers trl datasets \"protobuf==3.20.3\" evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693a36b-5cdc-4582-844b-8dd52fa522c5",
   "metadata": {
    "papermill": {
     "duration": 0.001918,
     "end_time": "2024-02-07T18:56:24.798841",
     "exception": false,
     "start_time": "2024-02-07T18:56:24.796923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## With Huggingface TRL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04c0a5-f481-4364-880d-10c254388987",
   "metadata": {
    "papermill": {
     "duration": 0.00191,
     "end_time": "2024-02-07T18:56:24.802650",
     "exception": false,
     "start_time": "2024-02-07T18:56:24.800740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's grab the Alpaca (GPT-4 curated instructions and outputs) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:24.807406Z",
     "iopub.status.busy": "2024-02-07T18:56:24.807025Z",
     "iopub.status.idle": "2024-02-07T18:56:24.822032Z",
     "shell.execute_reply": "2024-02-07T18:56:24.821685Z"
    },
    "papermill": {
     "duration": 0.018067,
     "end_time": "2024-02-07T18:56:24.822616",
     "exception": false,
     "start_time": "2024-02-07T18:56:24.804549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Benchmark from MedQA-USMLE/US/test.jsonl\n",
      "Benchmark contains 1273 questions, made up of 1273 with 5 options and 0 with non-5 options\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json\n",
    "# from uparse_benchmark import parse_benchmark\n",
    "# from ..utilities.parse_benchmark import parse_benchmark\n",
    "from utilities.parse_benchmark import parse_benchmark\n",
    "\n",
    "benchmark = \"MedQA\"\n",
    "benchmark_questions, benchmark_answers = parse_benchmark(benchmark)\n",
    "# print(benchmark_questions[0])\n",
    "# print(benchmark_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a154f968-da0f-4bdc-bf45-e499d95f0606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:24.827141Z",
     "iopub.status.busy": "2024-02-07T18:56:24.827025Z",
     "iopub.status.idle": "2024-02-07T18:56:29.655636Z",
     "shell.execute_reply": "2024-02-07T18:56:29.655365Z"
    },
    "papermill": {
     "duration": 4.831627,
     "end_time": "2024-02-07T18:56:29.656175",
     "exception": false,
     "start_time": "2024-02-07T18:56:24.824548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnelectric\u001b[0m (\u001b[33mneelectric\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/service/BioLlama/wandb/run-20240207_185625-xw7mij40\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdauntless-star-107\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft/runs/xw7mij40\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/neelectric/biollama_ft/runs/xw7mij40?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa1ef66eb10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"biollama_ft\", # the project I am working on\n",
    "           tags=[\"hf_sft\", \"Llama2\"]) # the Hyperparameters I want to keep track of\n",
    "# artifact = wandb.use_artifact('Neelectric/MedQA-USMLE', type='dataset')\n",
    "# artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811291cc-b7ce-422a-8971-3cbf9fe10a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:29.667804Z",
     "iopub.status.busy": "2024-02-07T18:56:29.667661Z",
     "iopub.status.idle": "2024-02-07T18:56:30.482007Z",
     "shell.execute_reply": "2024-02-07T18:56:30.481723Z"
    },
    "papermill": {
     "duration": 0.824223,
     "end_time": "2024-02-07T18:56:30.482910",
     "exception": false,
     "start_time": "2024-02-07T18:56:29.658687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# print(artifact_dir)\n",
    "artifact_dir = os.getcwd() + \"/benchmarks/MedQA-USMLE/\"\n",
    "from datasets import load_dataset\n",
    "#dataset = load_dataset(\"Neelectric/MedQA-USMLE\")\n",
    "medqa = load_dataset(\"json\", data_dir=artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9e0ed7-375f-431a-9420-0022cf5566f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:30.495840Z",
     "iopub.status.busy": "2024-02-07T18:56:30.495606Z",
     "iopub.status.idle": "2024-02-07T18:56:30.498025Z",
     "shell.execute_reply": "2024-02-07T18:56:30.497799Z"
    },
    "papermill": {
     "duration": 0.011976,
     "end_time": "2024-02-07T18:56:30.498598",
     "exception": false,
     "start_time": "2024-02-07T18:56:30.486622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 10178\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 1272\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 1273\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb96e0-b2f2-4a79-ba65-e1c8d6395d54",
   "metadata": {
    "papermill": {
     "duration": 0.003211,
     "end_time": "2024-02-07T18:56:30.505419",
     "exception": false,
     "start_time": "2024-02-07T18:56:30.502208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's log the dataset also as a table so we can inspect it on the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7dda87-d70a-470b-a0f9-040af434dc42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:30.511770Z",
     "iopub.status.busy": "2024-02-07T18:56:30.511686Z",
     "iopub.status.idle": "2024-02-07T18:56:30.513976Z",
     "shell.execute_reply": "2024-02-07T18:56:30.513800Z"
    },
    "papermill": {
     "duration": 0.005739,
     "end_time": "2024-02-07T18:56:30.514435",
     "exception": false,
     "start_time": "2024-02-07T18:56:30.508696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10178\n",
      "1272\n"
     ]
    }
   ],
   "source": [
    "train_dataset = medqa[\"train\"]\n",
    "eval_dataset = medqa[\"validation\"]\n",
    "#print sizes\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "# turn both of these into only half their size\n",
    "# train_dataset = train_dataset.select(range(0, len(train_dataset)//2))\n",
    "# eval_dataset = eval_dataset.select(range(0, len(eval_dataset)//2))\n",
    "\n",
    "# print(len(train_dataset))\n",
    "# print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a038eb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:30.519574Z",
     "iopub.status.busy": "2024-02-07T18:56:30.519331Z",
     "iopub.status.idle": "2024-02-07T18:56:30.521919Z",
     "shell.execute_reply": "2024-02-07T18:56:30.521745Z"
    },
    "papermill": {
     "duration": 0.005671,
     "end_time": "2024-02-07T18:56:30.522383",
     "exception": false,
     "start_time": "2024-02-07T18:56:30.516712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \\n (A) Ampicillin\\n (B) Ceftriaxone\\n (C) Ciprofloxacin\\n (D) Doxycycline\\n (E) Nitrofurantoin</QUESTION>\\n<ANSWER> (E) Nitrofurantoin</ANSWER>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_prompt(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"<QUESTION>{question} {option_string}</QUESTION>\\n<ANSWER> ({answer_idx}) {answer}</ANSWER>\").format_map(row)\n",
    "create_prompt(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8f1a9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:30.527436Z",
     "iopub.status.busy": "2024-02-07T18:56:30.527355Z",
     "iopub.status.idle": "2024-02-07T18:56:30.531880Z",
     "shell.execute_reply": "2024-02-07T18:56:30.531720Z"
    },
    "papermill": {
     "duration": 0.007627,
     "end_time": "2024-02-07T18:56:30.532354",
     "exception": false,
     "start_time": "2024-02-07T18:56:30.524727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt_no_answer(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"<QUESTION>{question} {option_string}</QUESTION>\\n<ANSWER> \").format_map(row)\n",
    "\n",
    "def return_prompt_no_answer(row):\n",
    "    return {\"text\": create_prompt_no_answer(row)}\n",
    "\n",
    "def return_prompt(row):\n",
    "    return {\"text\": create_prompt(row)}\n",
    "    \n",
    "test_dataset = eval_dataset.map(return_prompt_no_answer)\n",
    "# print(test_dataset[0][\"text\"])\n",
    "train_dataset_with_texts = train_dataset.map(return_prompt)\n",
    "# print(train_dataset_with_texts[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "720c707b-3bce-4164-b8c1-3c3122200c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:30.537446Z",
     "iopub.status.busy": "2024-02-07T18:56:30.537369Z",
     "iopub.status.idle": "2024-02-07T18:56:31.401406Z",
     "shell.execute_reply": "2024-02-07T18:56:31.401041Z"
    },
    "papermill": {
     "duration": 0.867754,
     "end_time": "2024-02-07T18:56:31.402482",
     "exception": false,
     "start_time": "2024-02-07T18:56:30.534728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# from cti.transformers.transformers.src.transformers.models.auto import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4162aec8-f2ba-45db-9633-817b416d4e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:31.414257Z",
     "iopub.status.busy": "2024-02-07T18:56:31.414066Z",
     "iopub.status.idle": "2024-02-07T18:56:31.416370Z",
     "shell.execute_reply": "2024-02-07T18:56:31.416158Z"
    },
    "papermill": {
     "duration": 0.011731,
     "end_time": "2024-02-07T18:56:31.416890",
     "exception": false,
     "start_time": "2024-02-07T18:56:31.405159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type = \"nf4\", bnb_4bit_compute_dtype=\"float16\", use_nested_quant = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51c10f7f-2551-4aa4-aef2-29c888b57a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:31.421872Z",
     "iopub.status.busy": "2024-02-07T18:56:31.421795Z",
     "iopub.status.idle": "2024-02-07T18:56:35.091364Z",
     "shell.execute_reply": "2024-02-07T18:56:35.091119Z"
    },
    "papermill": {
     "duration": 3.672934,
     "end_time": "2024-02-07T18:56:35.092116",
     "exception": false,
     "start_time": "2024-02-07T18:56:31.419182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                       | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                       | 1/2 [00:01<00:01,  1.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bf16 = False\n",
    "fp16 = True\n",
    "# Load base model\n",
    "if bf16:\n",
    "    torch_dtype=torch.bfloat16\n",
    "else:\n",
    "    torch_dtype=torch.float16\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    # torch_dtype=torch_dtype,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # quantization_config=bnb_config,\n",
    "    # load_in_4bit = True, # this causes \"RuntimeError: only Tensors of floating point and complex dtype can require gradients\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79dbbb0-6dac-4f78-a862-8201088c9d57",
   "metadata": {
    "papermill": {
     "duration": 0.008491,
     "end_time": "2024-02-07T18:56:35.103432",
     "exception": false,
     "start_time": "2024-02-07T18:56:35.094941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Training the full models is expensive, but if you have a GPU that can fit the full model, you can skip this part. Let's just train the last 8 layers of the model (Llama2-7B has 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:35.112319Z",
     "iopub.status.busy": "2024-02-07T18:56:35.112195Z",
     "iopub.status.idle": "2024-02-07T18:56:35.118061Z",
     "shell.execute_reply": "2024-02-07T18:56:35.117824Z"
    },
    "papermill": {
     "duration": 0.011345,
     "end_time": "2024-02-07T18:56:35.118848",
     "exception": false,
     "start_time": "2024-02-07T18:56:35.107503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = True\n",
      "mlp.up_proj.weight, requires_grad = True\n",
      "mlp.down_proj.weight, requires_grad = True\n",
      "input_layernorm.weight, requires_grad = True\n",
      "post_attention_layernorm.weight, requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "n_freeze = 15\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): \n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")  \n",
    "\n",
    "list_of_params_to_unfreeze = [\n",
    "    \"mlp.gate_proj.weight\",\n",
    "    \"mlp.up_proj.weight\",\n",
    "    \"mlp.down_proj.weight\",\n",
    "    \"input_layernorm.weight\",\n",
    "    \"post_attention_layernorm.weight\",\n",
    "]\n",
    "# for param in model.model.layers[n_freeze].parameters(): \n",
    "#     param.requires_grad = True\n",
    "\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    if name in list_of_params_to_unfreeze:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:35.127586Z",
     "iopub.status.busy": "2024-02-07T18:56:35.127465Z",
     "iopub.status.idle": "2024-02-07T18:56:35.129342Z",
     "shell.execute_reply": "2024-02-07T18:56:35.129112Z"
    },
    "papermill": {
     "duration": 0.00716,
     "end_time": "2024-02-07T18:56:35.130108",
     "exception": false,
     "start_time": "2024-02-07T18:56:35.122948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "model.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:35.138974Z",
     "iopub.status.busy": "2024-02-07T18:56:35.138857Z",
     "iopub.status.idle": "2024-02-07T18:56:35.143030Z",
     "shell.execute_reply": "2024-02-07T18:56:35.142794Z"
    },
    "papermill": {
     "duration": 0.009461,
     "end_time": "2024-02-07T18:56:35.143818",
     "exception": false,
     "start_time": "2024-02-07T18:56:35.134357",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 266.35M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f9c99da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:35.152690Z",
     "iopub.status.busy": "2024-02-07T18:56:35.152575Z",
     "iopub.status.idle": "2024-02-07T18:56:35.154794Z",
     "shell.execute_reply": "2024-02-07T18:56:35.154547Z"
    },
    "papermill": {
     "duration": 0.007546,
     "end_time": "2024-02-07T18:56:35.155557",
     "exception": false,
     "start_time": "2024-02-07T18:56:35.148011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac3525d7-3028-499e-8749-c6fbc21a26d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:35.164519Z",
     "iopub.status.busy": "2024-02-07T18:56:35.164406Z",
     "iopub.status.idle": "2024-02-07T18:56:35.342847Z",
     "shell.execute_reply": "2024-02-07T18:56:35.342511Z"
    },
    "papermill": {
     "duration": 0.18418,
     "end_time": "2024-02-07T18:56:35.343975",
     "exception": false,
     "start_time": "2024-02-07T18:56:35.159795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c57893e-6e42-4cda-83c8-1e51a86d2da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:35.358322Z",
     "iopub.status.busy": "2024-02-07T18:56:35.358050Z",
     "iopub.status.idle": "2024-02-07T18:56:35.360836Z",
     "shell.execute_reply": "2024-02-07T18:56:35.360582Z"
    },
    "papermill": {
     "duration": 0.01302,
     "end_time": "2024-02-07T18:56:35.361656",
     "exception": false,
     "start_time": "2024-02-07T18:56:35.348636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5605\n",
      "changing total num size to 6000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "total_num_steps = 11_210 // batch_size\n",
    "print(total_num_steps)\n",
    "\n",
    "\n",
    "total_num_steps = 6000\n",
    "print(f\"changing total num size to {total_num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c86339ca-27e5-496b-9559-3a65bb3c26a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:35.370714Z",
     "iopub.status.busy": "2024-02-07T18:56:35.370633Z",
     "iopub.status.idle": "2024-02-07T18:56:35.373388Z",
     "shell.execute_reply": "2024-02-07T18:56:35.373167Z"
    },
    "papermill": {
     "duration": 0.008124,
     "end_time": "2024-02-07T18:56:35.374166",
     "exception": false,
     "start_time": "2024-02-07T18:56:35.366042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/llama2_training_output/\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size//2,\n",
    "    bf16=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=total_num_steps // 10,\n",
    "    num_train_epochs=20,\n",
    "    # max_steps = -1\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=total_num_steps // 6,\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=8,\n",
    "    save_strategy=\"epoch\", #changed to epoch so we save every epoch i guess?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13c056",
   "metadata": {
    "papermill": {
     "duration": 0.004182,
     "end_time": "2024-02-07T18:56:35.382573",
     "exception": false,
     "start_time": "2024-02-07T18:56:35.378391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0b6e3",
   "metadata": {
    "papermill": {
     "duration": 0.004149,
     "end_time": "2024-02-07T18:56:35.391056",
     "exception": false,
     "start_time": "2024-02-07T18:56:35.386907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e35a9b70-f36b-4bfd-857f-c80d3450e111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:35.400311Z",
     "iopub.status.busy": "2024-02-07T18:56:35.399978Z",
     "iopub.status.idle": "2024-02-07T18:56:35.751332Z",
     "shell.execute_reply": "2024-02-07T18:56:35.750918Z"
    },
    "papermill": {
     "duration": 0.356982,
     "end_time": "2024-02-07T18:56:35.752204",
     "exception": false,
     "start_time": "2024-02-07T18:56:35.395222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|                                                                                                                                                                                                   | 0/1272 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1272/1272 [00:00<00:00, 17619.11 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from utils import LLMSampleCB, token_accuracy\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset_with_texts,\n",
    "    dataset_text_field=\"text\",\n",
    "    eval_dataset=test_dataset,\n",
    "    packing=False,\n",
    "    max_seq_length=None,\n",
    "    args=training_args,\n",
    "    formatting_func=create_prompt,\n",
    "    # compute_metrics=token_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5754789-2e15-4bc9-800c-01f8ffc625e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T18:56:35.761488Z",
     "iopub.status.busy": "2024-02-07T18:56:35.761384Z",
     "iopub.status.idle": "2024-02-07T23:14:38.778734Z",
     "shell.execute_reply": "2024-02-07T23:14:38.778310Z"
    },
    "papermill": {
     "duration": 15483.0231,
     "end_time": "2024-02-07T23:14:38.779821",
     "exception": false,
     "start_time": "2024-02-07T18:56:35.756721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101780' max='101780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [101780/101780 4:17:57, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.285600</td>\n",
       "      <td>1.595086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.616500</td>\n",
       "      <td>1.542105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.284000</td>\n",
       "      <td>1.493757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.247900</td>\n",
       "      <td>1.459633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.223000</td>\n",
       "      <td>1.429920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.936900</td>\n",
       "      <td>1.425044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.976600</td>\n",
       "      <td>1.417215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.935400</td>\n",
       "      <td>1.412201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.001000</td>\n",
       "      <td>1.407521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.971000</td>\n",
       "      <td>1.404186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>1.408820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.882500</td>\n",
       "      <td>1.408723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.933500</td>\n",
       "      <td>1.406792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>1.407880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.844800</td>\n",
       "      <td>1.405816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.898100</td>\n",
       "      <td>1.411564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.829100</td>\n",
       "      <td>1.411978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.896200</td>\n",
       "      <td>1.417412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.905600</td>\n",
       "      <td>1.414419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.842400</td>\n",
       "      <td>1.413820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.884500</td>\n",
       "      <td>1.417397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.781100</td>\n",
       "      <td>1.420396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.818800</td>\n",
       "      <td>1.422149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.797400</td>\n",
       "      <td>1.421781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>1.421388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.854600</td>\n",
       "      <td>1.425452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.813200</td>\n",
       "      <td>1.426919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.798800</td>\n",
       "      <td>1.429231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.757200</td>\n",
       "      <td>1.428577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.806100</td>\n",
       "      <td>1.428677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.800600</td>\n",
       "      <td>1.430921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.754200</td>\n",
       "      <td>1.433733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.779100</td>\n",
       "      <td>1.434319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.787200</td>\n",
       "      <td>1.435527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.883700</td>\n",
       "      <td>1.435029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.771100</td>\n",
       "      <td>1.436996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.772200</td>\n",
       "      <td>1.438595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.901300</td>\n",
       "      <td>1.439235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>1.440446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.757600</td>\n",
       "      <td>1.440636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.707900</td>\n",
       "      <td>1.441782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.760400</td>\n",
       "      <td>1.444005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.764900</td>\n",
       "      <td>1.444252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.758100</td>\n",
       "      <td>1.445298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.803900</td>\n",
       "      <td>1.445885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.824300</td>\n",
       "      <td>1.447209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.774200</td>\n",
       "      <td>1.448628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.826900</td>\n",
       "      <td>1.449853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>1.450919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.820100</td>\n",
       "      <td>1.451147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.771800</td>\n",
       "      <td>1.451377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.800800</td>\n",
       "      <td>1.452499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.768300</td>\n",
       "      <td>1.453611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.781300</td>\n",
       "      <td>1.454820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.814500</td>\n",
       "      <td>1.455409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.876700</td>\n",
       "      <td>1.456288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.790700</td>\n",
       "      <td>1.457464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.753900</td>\n",
       "      <td>1.458640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>1.459191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.747400</td>\n",
       "      <td>1.459962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.762200</td>\n",
       "      <td>1.460987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.782600</td>\n",
       "      <td>1.461938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.773700</td>\n",
       "      <td>1.463030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.731300</td>\n",
       "      <td>1.463582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.735300</td>\n",
       "      <td>1.464356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>1.465191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.776400</td>\n",
       "      <td>1.466333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.770600</td>\n",
       "      <td>1.467227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.781700</td>\n",
       "      <td>1.468381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.789700</td>\n",
       "      <td>1.468899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>1.469686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.814700</td>\n",
       "      <td>1.470561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>1.471448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.843400</td>\n",
       "      <td>1.472153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.815500</td>\n",
       "      <td>1.472998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>1.473840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.809100</td>\n",
       "      <td>1.474450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.818700</td>\n",
       "      <td>1.475202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.830200</td>\n",
       "      <td>1.475833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.837700</td>\n",
       "      <td>1.476295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>0.762600</td>\n",
       "      <td>1.476743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.838600</td>\n",
       "      <td>1.477397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>0.793100</td>\n",
       "      <td>1.477881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.793800</td>\n",
       "      <td>1.478228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>1.478627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.875300</td>\n",
       "      <td>1.478954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>0.849600</td>\n",
       "      <td>1.479343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.722600</td>\n",
       "      <td>1.479601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>1.479756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.828400</td>\n",
       "      <td>1.479956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>0.806200</td>\n",
       "      <td>1.480105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.899800</td>\n",
       "      <td>1.480229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>0.785500</td>\n",
       "      <td>1.480308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>1.480380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.775700</td>\n",
       "      <td>1.480422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>1.480446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>0.723500</td>\n",
       "      <td>1.480466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.762600</td>\n",
       "      <td>1.480468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>0.793300</td>\n",
       "      <td>1.480467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>1.480461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>0.786300</td>\n",
       "      <td>1.480461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.004 MB of 0.004 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.009 MB of 0.044 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.009 MB of 0.044 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.044 MB of 0.044 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñá‚ñà‚ñÑ‚ñÑ‚ñÜ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ‚ñà‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 1.48046\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 53.2684\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 23.879\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 23.879\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 20.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 101780\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.738\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 2.798261347221209e+18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.84301\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 15477.4885\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 13.152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 6.576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mdauntless-star-107\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft/runs/xw7mij40\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240207_185625-xw7mij40/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#set llama model config use_cache to false!!!\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a81beabc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T23:14:38.792521Z",
     "iopub.status.busy": "2024-02-07T23:14:38.792219Z",
     "iopub.status.idle": "2024-02-07T23:14:38.794401Z",
     "shell.execute_reply": "2024-02-07T23:14:38.794141Z"
    },
    "papermill": {
     "duration": 0.009175,
     "end_time": "2024-02-07T23:14:38.795205",
     "exception": false,
     "start_time": "2024-02-07T23:14:38.786030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/service/BioLlama/utilities/finetuning/llama2_training_output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "684ca6fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T23:14:38.807162Z",
     "iopub.status.busy": "2024-02-07T23:14:38.806939Z",
     "iopub.status.idle": "2024-02-07T23:14:56.866283Z",
     "shell.execute_reply": "2024-02-07T23:14:56.865866Z"
    },
    "papermill": {
     "duration": 18.066043,
     "end_time": "2024-02-07T23:14:56.866901",
     "exception": false,
     "start_time": "2024-02-07T23:14:38.800858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 13163468\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  1 01:37 checkpoint-10000\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 19:22 checkpoint-10178\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 23:14 checkpoint-101780\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 00:17 checkpoint-1468\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 19:35 checkpoint-15267\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 19:47 checkpoint-20356\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 20:00 checkpoint-25445\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 00:23 checkpoint-2936\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 20:13 checkpoint-30534\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 20:26 checkpoint-35623\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 20:39 checkpoint-40712\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 00:30 checkpoint-4404\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 20:51 checkpoint-45801\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 19:09 checkpoint-5089\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 21:04 checkpoint-50890\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 21:17 checkpoint-55979\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 00:36 checkpoint-5872\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 11:51 checkpoint-6000\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 21:31 checkpoint-61068\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 21:44 checkpoint-66157\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 21:56 checkpoint-71246\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 22:09 checkpoint-76335\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 22:22 checkpoint-81424\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 22:35 checkpoint-86513\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 22:48 checkpoint-91602\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 23:01 checkpoint-96691\r\n",
      "-rw-rw-r-- 1 service service        690 Feb  7 23:14 config.json\r\n",
      "-rw-rw-r-- 1 service service        183 Feb  7 23:14 generation_config.json\r\n",
      "drwxrwxr-x 2 service service       4096 Feb  7 18:56 logs\r\n",
      "-rw-rw-r-- 1 service service 4938985352 Feb  7 23:14 model-00001-of-00003.safetensors\r\n",
      "-rw-rw-r-- 1 service service 4947390880 Feb  7 23:14 model-00002-of-00003.safetensors\r\n",
      "-rw-rw-r-- 1 service service 3590488816 Feb  7 23:14 model-00003-of-00003.safetensors\r\n",
      "-rw-rw-r-- 1 service service      23950 Feb  7 23:14 model.safetensors.index.json\r\n",
      "-rw-rw-r-- 1 service service        437 Feb  7 23:14 special_tokens_map.json\r\n",
      "-rw-rw-r-- 1 service service       1762 Feb  7 23:14 tokenizer_config.json\r\n",
      "-rw-rw-r-- 1 service service    1842866 Feb  7 23:14 tokenizer.json\r\n",
      "-rw-rw-r-- 1 service service     499723 Feb  7 23:14 tokenizer.model\r\n",
      "-rw-rw-r-- 1 service service       4792 Feb  7 23:14 training_args.bin\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(output_dir)\n",
    "#print contents of output_dir\n",
    "!ls -l $output_dir\n",
    "#print full path of output_dir\n",
    "# !pwd $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16ff2529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T23:14:56.875621Z",
     "iopub.status.busy": "2024-02-07T23:14:56.875494Z",
     "iopub.status.idle": "2024-02-07T23:15:03.041703Z",
     "shell.execute_reply": "2024-02-07T23:15:03.041465Z"
    },
    "papermill": {
     "duration": 6.171157,
     "end_time": "2024-02-07T23:15:03.042241",
     "exception": false,
     "start_time": "2024-02-07T23:14:56.871084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/service/BioLlama/utilities/finetuning/llama2_training_output/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                       | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                                                    | 1/3 [00:01<00:03,  1.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                          | 2/3 [00:03<00:01,  1.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/transformers/generation/utils.py:1413: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an excellently helpful AI assistant that answers biomedical questions. <QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \n",
      " (A) Ampicillin\n",
      " (B) Ceftriaxone\n",
      " (C) Ciprofloxacin\n",
      " (D) Doxycycline\n",
      " (E) Nitrofurantoin</QUESTION>\n",
      "<ANSWER> \n",
      " (D) Doxycycline\n",
      "\n",
      " (A) Ampicillin may be effective against bacteria that cause urinary tract infections,\n"
     ]
    }
   ],
   "source": [
    "#there is a finetune of llama 2 7b hf in the foler finetuned_models\n",
    "#load this local model here and use it to generate some text\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/llama2_training_output/\"\n",
    "print(output_dir)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "new_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
    "prompt = 'You are an excellently helpful AI assistant that answers biomedical questions. <QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \\n (A) Ampicillin\\n (B) Ceftriaxone\\n (C) Ciprofloxacin\\n (D) Doxycycline\\n (E) Nitrofurantoin</QUESTION>\\n<ANSWER> '\n",
    "\n",
    "input_ids = new_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "# input_ids = new_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# print(input_ids)\n",
    "# print(input_ids.shape)\n",
    "\n",
    "output = new_model.generate(input_ids, max_new_tokens=35, do_sample=True, top_p=0.95, top_k=60)\n",
    "print(new_tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15520.186541,
   "end_time": "2024-02-07T23:15:04.275870",
   "environment_variables": {},
   "exception": null,
   "input_path": "Llama2_finetuning_WandB_HF.ipynb",
   "output_path": "output_llama2.ipynb",
   "parameters": {},
   "start_time": "2024-02-07T18:56:24.089329",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}