{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb384650",
   "metadata": {
    "papermill": {
     "duration": 0.002756,
     "end_time": "2024-02-07T11:25:00.743492",
     "exception": false,
     "start_time": "2024-02-07T11:25:00.740736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is taken directly from https://github.com/tcapelle/llm_recipes/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
   "metadata": {
    "papermill": {
     "duration": 0.00439,
     "end_time": "2024-02-07T11:25:00.756613",
     "exception": false,
     "start_time": "2024-02-07T11:25:00.752223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# From Llama to Alpaca: Finetunning and LLM with Weights & Biases\n",
    "In this notebooks you will learn how to finetune a pretrained LLama model on an Instruction dataset. We will use an updated version of the Alpaca dataset that, instead of davinci-003 (GPT3) generations uses GPT4 to get an even better instruction dataset! More details on the [official repo page](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data)\n",
    "\n",
    "> This notebook requires a A100/A10 GPU with at least 24GB of memory. You could tweak the params down and run on a T4 but it would take very long time\n",
    "\n",
    "This notebooks has a companion project and [report](wandb.me/alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7744abe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:00.764746Z",
     "iopub.status.busy": "2024-02-07T11:25:00.764565Z",
     "iopub.status.idle": "2024-02-07T11:25:00.766781Z",
     "shell.execute_reply": "2024-02-07T11:25:00.766555Z"
    },
    "papermill": {
     "duration": 0.00744,
     "end_time": "2024-02-07T11:25:00.767630",
     "exception": false,
     "start_time": "2024-02-07T11:25:00.760190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install wandb transformers trl datasets \"protobuf==3.20.3\" evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693a36b-5cdc-4582-844b-8dd52fa522c5",
   "metadata": {
    "papermill": {
     "duration": 0.003543,
     "end_time": "2024-02-07T11:25:00.774732",
     "exception": false,
     "start_time": "2024-02-07T11:25:00.771189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## With Huggingface TRL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04c0a5-f481-4364-880d-10c254388987",
   "metadata": {
    "papermill": {
     "duration": 0.003493,
     "end_time": "2024-02-07T11:25:00.781719",
     "exception": false,
     "start_time": "2024-02-07T11:25:00.778226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's grab the Alpaca (GPT-4 curated instructions and outputs) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:00.789360Z",
     "iopub.status.busy": "2024-02-07T11:25:00.789256Z",
     "iopub.status.idle": "2024-02-07T11:25:00.844754Z",
     "shell.execute_reply": "2024-02-07T11:25:00.844455Z"
    },
    "papermill": {
     "duration": 0.060405,
     "end_time": "2024-02-07T11:25:00.845648",
     "exception": false,
     "start_time": "2024-02-07T11:25:00.785243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Benchmark from MedQA-USMLE/US/train.jsonl\n",
      "Benchmark contains 10178 questions, made up of 10178 with 5 options and 0 with non-5 options\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json\n",
    "# from uparse_benchmark import parse_benchmark\n",
    "# from ..utilities.parse_benchmark import parse_benchmark\n",
    "from utilities.parse_benchmark import parse_benchmark\n",
    "\n",
    "benchmark = \"MedQA\"\n",
    "benchmark_questions, benchmark_answers = parse_benchmark(benchmark)\n",
    "# print(benchmark_questions[0])\n",
    "# print(benchmark_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a154f968-da0f-4bdc-bf45-e499d95f0606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:00.853679Z",
     "iopub.status.busy": "2024-02-07T11:25:00.853574Z",
     "iopub.status.idle": "2024-02-07T11:25:05.821475Z",
     "shell.execute_reply": "2024-02-07T11:25:05.821117Z"
    },
    "papermill": {
     "duration": 4.972713,
     "end_time": "2024-02-07T11:25:05.822127",
     "exception": false,
     "start_time": "2024-02-07T11:25:00.849414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnelectric\u001b[0m (\u001b[33mneelectric\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/service/BioLlama/wandb/run-20240207_112501-5xstxjiq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdistinctive-microwave-92\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_ft/runs/5xstxjiq\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/neelectric/biollama_ft/runs/5xstxjiq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f2174d7b810>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"biollama_ft\", # the project I am working on\n",
    "           tags=[\"hf_sft\"]) # the Hyperparameters I want to keep track of\n",
    "# artifact = wandb.use_artifact('Neelectric/MedQA-USMLE', type='dataset')\n",
    "# artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811291cc-b7ce-422a-8971-3cbf9fe10a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:05.833872Z",
     "iopub.status.busy": "2024-02-07T11:25:05.833736Z",
     "iopub.status.idle": "2024-02-07T11:25:06.693019Z",
     "shell.execute_reply": "2024-02-07T11:25:06.692636Z"
    },
    "papermill": {
     "duration": 0.869285,
     "end_time": "2024-02-07T11:25:06.693983",
     "exception": false,
     "start_time": "2024-02-07T11:25:05.824698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# print(artifact_dir)\n",
    "artifact_dir = os.getcwd() + \"/benchmarks/MedQA-USMLE/\"\n",
    "from datasets import load_dataset\n",
    "#dataset = load_dataset(\"Neelectric/MedQA-USMLE\")\n",
    "medqa = load_dataset(\"json\", data_dir=artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9e0ed7-375f-431a-9420-0022cf5566f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:06.701869Z",
     "iopub.status.busy": "2024-02-07T11:25:06.701618Z",
     "iopub.status.idle": "2024-02-07T11:25:06.704221Z",
     "shell.execute_reply": "2024-02-07T11:25:06.703935Z"
    },
    "papermill": {
     "duration": 0.007022,
     "end_time": "2024-02-07T11:25:06.704800",
     "exception": false,
     "start_time": "2024-02-07T11:25:06.697778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 10178\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 1272\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
       "        num_rows: 1273\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb96e0-b2f2-4a79-ba65-e1c8d6395d54",
   "metadata": {
    "papermill": {
     "duration": 0.00343,
     "end_time": "2024-02-07T11:25:06.711738",
     "exception": false,
     "start_time": "2024-02-07T11:25:06.708308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's log the dataset also as a table so we can inspect it on the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7dda87-d70a-470b-a0f9-040af434dc42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:06.718153Z",
     "iopub.status.busy": "2024-02-07T11:25:06.718026Z",
     "iopub.status.idle": "2024-02-07T11:25:06.720589Z",
     "shell.execute_reply": "2024-02-07T11:25:06.720315Z"
    },
    "papermill": {
     "duration": 0.006064,
     "end_time": "2024-02-07T11:25:06.721135",
     "exception": false,
     "start_time": "2024-02-07T11:25:06.715071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10178\n",
      "1272\n"
     ]
    }
   ],
   "source": [
    "train_dataset = medqa[\"train\"]\n",
    "eval_dataset = medqa[\"validation\"]\n",
    "#print sizes\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "# turn both of these into only half their size\n",
    "# train_dataset = train_dataset.select(range(0, len(train_dataset)//2))\n",
    "# eval_dataset = eval_dataset.select(range(0, len(eval_dataset)//2))\n",
    "\n",
    "# print(len(train_dataset))\n",
    "# print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a038eb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:06.726864Z",
     "iopub.status.busy": "2024-02-07T11:25:06.726749Z",
     "iopub.status.idle": "2024-02-07T11:25:06.729808Z",
     "shell.execute_reply": "2024-02-07T11:25:06.729548Z"
    },
    "papermill": {
     "duration": 0.006712,
     "end_time": "2024-02-07T11:25:06.730356",
     "exception": false,
     "start_time": "2024-02-07T11:25:06.723644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \\n (A) Ampicillin\\n (B) Ceftriaxone\\n (C) Ciprofloxacin\\n (D) Doxycycline\\n (E) Nitrofurantoin</QUESTION>\\n<ANSWER> (E) Nitrofurantoin</ANSWER>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_prompt(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"<QUESTION>{question} {option_string}</QUESTION>\\n<ANSWER> ({answer_idx}) {answer}</ANSWER>\").format_map(row)\n",
    "create_prompt(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8f1a9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:06.735809Z",
     "iopub.status.busy": "2024-02-07T11:25:06.735695Z",
     "iopub.status.idle": "2024-02-07T11:25:06.740726Z",
     "shell.execute_reply": "2024-02-07T11:25:06.740503Z"
    },
    "papermill": {
     "duration": 0.008545,
     "end_time": "2024-02-07T11:25:06.741400",
     "exception": false,
     "start_time": "2024-02-07T11:25:06.732855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt_no_answer(row):\n",
    "    option_string = \"\"\n",
    "    for option in row[\"options\"].keys():\n",
    "        option_string += \"\\n (\" + option + \") \" + row[\"options\"][option]\n",
    "    row[\"option_string\"] = option_string\n",
    "    return (\"<QUESTION>{question} {option_string}</QUESTION>\\n<ANSWER> \").format_map(row)\n",
    "\n",
    "def return_prompt_no_answer(row):\n",
    "    return {\"text\": create_prompt_no_answer(row)}\n",
    "\n",
    "def return_prompt(row):\n",
    "    return {\"text\": create_prompt(row)}\n",
    "    \n",
    "test_dataset = eval_dataset.map(return_prompt_no_answer)\n",
    "# print(test_dataset[0][\"text\"])\n",
    "train_dataset_with_texts = train_dataset.map(return_prompt)\n",
    "# print(train_dataset_with_texts[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "720c707b-3bce-4164-b8c1-3c3122200c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:06.747034Z",
     "iopub.status.busy": "2024-02-07T11:25:06.746917Z",
     "iopub.status.idle": "2024-02-07T11:25:07.579733Z",
     "shell.execute_reply": "2024-02-07T11:25:07.579362Z"
    },
    "papermill": {
     "duration": 0.836717,
     "end_time": "2024-02-07T11:25:07.580687",
     "exception": false,
     "start_time": "2024-02-07T11:25:06.743970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# from cti.transformers.transformers.src.transformers.models.auto import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4162aec8-f2ba-45db-9633-817b416d4e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:07.592687Z",
     "iopub.status.busy": "2024-02-07T11:25:07.592482Z",
     "iopub.status.idle": "2024-02-07T11:25:07.594537Z",
     "shell.execute_reply": "2024-02-07T11:25:07.594218Z"
    },
    "papermill": {
     "duration": 0.011662,
     "end_time": "2024-02-07T11:25:07.595137",
     "exception": false,
     "start_time": "2024-02-07T11:25:07.583475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51c10f7f-2551-4aa4-aef2-29c888b57a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:07.600600Z",
     "iopub.status.busy": "2024-02-07T11:25:07.600446Z",
     "iopub.status.idle": "2024-02-07T11:25:11.280183Z",
     "shell.execute_reply": "2024-02-07T11:25:11.279935Z"
    },
    "papermill": {
     "duration": 3.683197,
     "end_time": "2024-02-07T11:25:11.280822",
     "exception": false,
     "start_time": "2024-02-07T11:25:07.597625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                       | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                       | 1/2 [00:01<00:01,  1.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # quantization_config=bnb_config,\n",
    "    # load_in_4bit = True, # this causes \"RuntimeError: only Tensors of floating point and complex dtype can require gradients\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79dbbb0-6dac-4f78-a862-8201088c9d57",
   "metadata": {
    "papermill": {
     "duration": 0.008834,
     "end_time": "2024-02-07T11:25:11.292616",
     "exception": false,
     "start_time": "2024-02-07T11:25:11.283782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Training the full models is expensive, but if you have a GPU that can fit the full model, you can skip this part. Let's just train the last 8 layers of the model (Llama2-7B has 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:11.298875Z",
     "iopub.status.busy": "2024-02-07T11:25:11.298741Z",
     "iopub.status.idle": "2024-02-07T11:25:11.305549Z",
     "shell.execute_reply": "2024-02-07T11:25:11.305282Z"
    },
    "papermill": {
     "duration": 0.010667,
     "end_time": "2024-02-07T11:25:11.306119",
     "exception": false,
     "start_time": "2024-02-07T11:25:11.295452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = False\n",
      "mlp.up_proj.weight, requires_grad = False\n",
      "mlp.down_proj.weight, requires_grad = False\n",
      "input_layernorm.weight, requires_grad = False\n",
      "post_attention_layernorm.weight, requires_grad = False\n",
      "self_attn.q_proj.weight, requires_grad = False\n",
      "self_attn.k_proj.weight, requires_grad = False\n",
      "self_attn.v_proj.weight, requires_grad = False\n",
      "self_attn.o_proj.weight, requires_grad = False\n",
      "mlp.gate_proj.weight, requires_grad = True\n",
      "mlp.up_proj.weight, requires_grad = True\n",
      "mlp.down_proj.weight, requires_grad = True\n",
      "input_layernorm.weight, requires_grad = True\n",
      "post_attention_layernorm.weight, requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "n_freeze = 15\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): \n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")  \n",
    "\n",
    "list_of_params_to_unfreeze = [\n",
    "    \"mlp.gate_proj.weight\",\n",
    "    \"mlp.up_proj.weight\",\n",
    "    \"mlp.down_proj.weight\",\n",
    "    \"input_layernorm.weight\",\n",
    "    \"post_attention_layernorm.weight\",\n",
    "]\n",
    "# for param in model.model.layers[n_freeze].parameters(): \n",
    "#     param.requires_grad = True\n",
    "\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    if name in list_of_params_to_unfreeze:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for name, param in model.model.layers[n_freeze].named_parameters():\n",
    "    print(f\"{name}, requires_grad = {param.requires_grad}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:11.311779Z",
     "iopub.status.busy": "2024-02-07T11:25:11.311654Z",
     "iopub.status.idle": "2024-02-07T11:25:11.313513Z",
     "shell.execute_reply": "2024-02-07T11:25:11.313290Z"
    },
    "papermill": {
     "duration": 0.005353,
     "end_time": "2024-02-07T11:25:11.314055",
     "exception": false,
     "start_time": "2024-02-07T11:25:11.308702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "model.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:11.319737Z",
     "iopub.status.busy": "2024-02-07T11:25:11.319620Z",
     "iopub.status.idle": "2024-02-07T11:25:11.323900Z",
     "shell.execute_reply": "2024-02-07T11:25:11.323663Z"
    },
    "papermill": {
     "duration": 0.007807,
     "end_time": "2024-02-07T11:25:11.324454",
     "exception": false,
     "start_time": "2024-02-07T11:25:11.316647",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 266.35M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f9c99da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:11.330158Z",
     "iopub.status.busy": "2024-02-07T11:25:11.330036Z",
     "iopub.status.idle": "2024-02-07T11:25:11.332226Z",
     "shell.execute_reply": "2024-02-07T11:25:11.332000Z"
    },
    "papermill": {
     "duration": 0.005733,
     "end_time": "2024-02-07T11:25:11.332768",
     "exception": false,
     "start_time": "2024-02-07T11:25:11.327035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac3525d7-3028-499e-8749-c6fbc21a26d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:11.338747Z",
     "iopub.status.busy": "2024-02-07T11:25:11.338426Z",
     "iopub.status.idle": "2024-02-07T11:25:11.523740Z",
     "shell.execute_reply": "2024-02-07T11:25:11.523413Z"
    },
    "papermill": {
     "duration": 0.189241,
     "end_time": "2024-02-07T11:25:11.524634",
     "exception": false,
     "start_time": "2024-02-07T11:25:11.335393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c57893e-6e42-4cda-83c8-1e51a86d2da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:11.530924Z",
     "iopub.status.busy": "2024-02-07T11:25:11.530708Z",
     "iopub.status.idle": "2024-02-07T11:25:11.533355Z",
     "shell.execute_reply": "2024-02-07T11:25:11.533056Z"
    },
    "papermill": {
     "duration": 0.006289,
     "end_time": "2024-02-07T11:25:11.533946",
     "exception": false,
     "start_time": "2024-02-07T11:25:11.527657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5605\n",
      "changing total num size to 6000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "total_num_steps = 11_210 // batch_size\n",
    "print(total_num_steps)\n",
    "\n",
    "\n",
    "total_num_steps = 6000\n",
    "print(f\"changing total num size to {total_num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "154326d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:11.539681Z",
     "iopub.status.busy": "2024-02-07T11:25:11.539585Z",
     "iopub.status.idle": "2024-02-07T11:25:11.541377Z",
     "shell.execute_reply": "2024-02-07T11:25:11.541035Z"
    },
    "papermill": {
     "duration": 0.005328,
     "end_time": "2024-02-07T11:25:11.541903",
     "exception": false,
     "start_time": "2024-02-07T11:25:11.536575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from cti.transformers.transformers.src.transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "098dea68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:11.547760Z",
     "iopub.status.busy": "2024-02-07T11:25:11.547525Z",
     "iopub.status.idle": "2024-02-07T11:25:11.549186Z",
     "shell.execute_reply": "2024-02-07T11:25:11.548919Z"
    },
    "papermill": {
     "duration": 0.005262,
     "end_time": "2024-02-07T11:25:11.549784",
     "exception": false,
     "start_time": "2024-02-07T11:25:11.544522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip uninstall torch_xla -y\n",
    "# !pip install -i https://pip.repos.neuron.amazonaws.com torch-xla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c86339ca-27e5-496b-9559-3a65bb3c26a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:11.555535Z",
     "iopub.status.busy": "2024-02-07T11:25:11.555448Z",
     "iopub.status.idle": "2024-02-07T11:25:11.558288Z",
     "shell.execute_reply": "2024-02-07T11:25:11.558023Z"
    },
    "papermill": {
     "duration": 0.006398,
     "end_time": "2024-02-07T11:25:11.558819",
     "exception": false,
     "start_time": "2024-02-07T11:25:11.552421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/llama2_training_output/\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size//2,\n",
    "    bf16=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=total_num_steps // 10,\n",
    "    num_train_epochs=4,\n",
    "    max_steps=total_num_steps,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=total_num_steps // 6,\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\", #changed to epoch so we save every epoch i guess?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13c056",
   "metadata": {
    "papermill": {
     "duration": 0.002722,
     "end_time": "2024-02-07T11:25:11.564190",
     "exception": false,
     "start_time": "2024-02-07T11:25:11.561468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e35a9b70-f36b-4bfd-857f-c80d3450e111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:11.569990Z",
     "iopub.status.busy": "2024-02-07T11:25:11.569881Z",
     "iopub.status.idle": "2024-02-07T11:25:11.803903Z",
     "shell.execute_reply": "2024-02-07T11:25:11.803496Z"
    },
    "papermill": {
     "duration": 0.237849,
     "end_time": "2024-02-07T11:25:11.804679",
     "exception": false,
     "start_time": "2024-02-07T11:25:11.566830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from utils import LLMSampleCB, token_accuracy\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    eval_dataset=eval_dataset,\n",
    "    packing=True,\n",
    "    max_seq_length=1024,\n",
    "    args=training_args,\n",
    "    formatting_func=create_prompt,\n",
    "    # compute_metrics=token_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b129b",
   "metadata": {
    "papermill": {
     "duration": 0.004473,
     "end_time": "2024-02-07T11:25:11.814209",
     "exception": false,
     "start_time": "2024-02-07T11:25:11.809736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df6d1457-3c7a-47e6-994b-334dbe9c3d5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:11.823801Z",
     "iopub.status.busy": "2024-02-07T11:25:11.823687Z",
     "iopub.status.idle": "2024-02-07T11:25:11.825655Z",
     "shell.execute_reply": "2024-02-07T11:25:11.825367Z"
    },
    "papermill": {
     "duration": 0.007794,
     "end_time": "2024-02-07T11:25:11.826494",
     "exception": false,
     "start_time": "2024-02-07T11:25:11.818700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.add_callback(wandb_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5754789-2e15-4bc9-800c-01f8ffc625e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T11:25:11.835864Z",
     "iopub.status.busy": "2024-02-07T11:25:11.835767Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-02-07T11:25:11.830940",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3001' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3001/6000 12:33 < 12:33, 3.98 it/s, Epoch 0.59/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.676600</td>\n",
       "      <td>1.289494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.079300</td>\n",
       "      <td>1.215242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/service/miniconda3/envs/llm_research/lib/python3.11/site-packages/trl/trainer/utils.py:268: UserWarning: The dataset reached end and the iterator is reset to the start.\n",
      "  warnings.warn(\"The dataset reached end and the iterator is reset to the start.\")\n"
     ]
    }
   ],
   "source": [
    "#set llama model config use_cache to false!!!\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81beabc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:52:49.199125Z",
     "iopub.status.busy": "2024-01-17T00:52:49.199038Z",
     "iopub.status.idle": "2024-01-17T00:52:49.201025Z",
     "shell.execute_reply": "2024-01-17T00:52:49.200809Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.abspath(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ca6fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T00:52:49.215531Z",
     "iopub.status.busy": "2024-01-17T00:52:49.215430Z",
     "iopub.status.idle": "2024-01-17T00:53:05.370183Z",
     "shell.execute_reply": "2024-01-17T00:53:05.369679Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)\n",
    "#print contents of output_dir\n",
    "!ls -l $output_dir\n",
    "#print full path of output_dir\n",
    "# !pwd $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff2529",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#there is a finetune of llama 2 7b hf in the foler finetuned_models\n",
    "#load this local model here and use it to generate some text\n",
    "output_dir = \"/home/service/BioLlama/utilities/finetuning/llama2_training_output/\"\n",
    "print(output_dir)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "new_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
    "prompt = 'You are an excellently helpful AI assistant that answers biomedical questions. <QUESTION>A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? \\n (A) Ampicillin\\n (B) Ceftriaxone\\n (C) Ciprofloxacin\\n (D) Doxycycline\\n (E) Nitrofurantoin</QUESTION>\\n<ANSWER> '\n",
    "\n",
    "input_ids = new_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "# input_ids = new_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# print(input_ids)\n",
    "# print(input_ids.shape)\n",
    "\n",
    "output = new_model.generate(input_ids, max_new_tokens=35, do_sample=True, top_p=0.95, top_k=60)\n",
    "print(new_tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "Llama2_finetuning_WandB_HF.ipynb",
   "output_path": "output_llama2.ipynb",
   "parameters": {},
   "start_time": "2024-02-07T11:25:00.040039",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}