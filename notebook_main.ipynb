{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_count(m):\n",
    "    m = m.model\n",
    "    temp = m.parameters()\n",
    "    temp_list = [elt for elt in temp]\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is in config mapping\n",
      "<class 'utilities.finetuning.cti.transformers.transformers.src.transformers.models.auto.configuration_auto._LazyConfigMapping'>\n",
      "that worked!\n",
      "Wrapping layer 15 with retro\n",
      "is in config mapping\n",
      "<class 'utilities.finetuning.cti.transformers.transformers.src.transformers.models.auto.configuration_auto._LazyConfigMapping'>\n",
      "that worked!\n",
      "is in config mapping\n",
      "<class 'utilities.finetuning.cti.transformers.transformers.src.transformers.models.auto.configuration_auto._LazyConfigMapping'>\n",
      "that worked!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m time_before_setup \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     15\u001b[0m BioLlama \u001b[38;5;241m=\u001b[39m BioLlama(model_id\u001b[38;5;241m=\u001b[39mmodel_id, chunk_length\u001b[38;5;241m=\u001b[39mchunk_length)\n\u001b[0;32m---> 18\u001b[0m params, trainable_params \u001b[38;5;241m=\u001b[39m param_count(BioLlama)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# freeze layers (disable gradients)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m BioLlama\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(): param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m, in \u001b[0;36mparam_count\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m      3\u001b[0m temp \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mparameters()\n\u001b[1;32m      4\u001b[0m temp_list \u001b[38;5;241m=\u001b[39m [elt \u001b[38;5;28;01mfor\u001b[39;00m elt \u001b[38;5;129;01min\u001b[39;00m temp]\n\u001b[0;32m----> 5\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mparameters()])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1_000_000\u001b[39m\n\u001b[1;32m      6\u001b[0m trainable_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1_000_000\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mM, Trainable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainable_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m, in \u001b[0;36mparam_count\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m      3\u001b[0m temp \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mparameters()\n\u001b[1;32m      4\u001b[0m temp_list \u001b[38;5;241m=\u001b[39m [elt \u001b[38;5;28;01mfor\u001b[39;00m elt \u001b[38;5;129;01min\u001b[39;00m temp]\n\u001b[0;32m----> 5\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mparameters()])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1_000_000\u001b[39m\n\u001b[1;32m      6\u001b[0m trainable_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1_000_000\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mM, Trainable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainable_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_research/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utilities.biollama import BioLlama\n",
    "import time\n",
    "\n",
    "questions = [\"Which is the main calcium pump of the sarcoplasmic reticulum? Answer:\"]\n",
    "#answers = [\"Sarcoplasmic reticulum Ca(2+)-ATPase\"] # or \"SERCA\",\"serca2\"\n",
    "\n",
    "db_name = \"RCT200ktrain\"\n",
    "retrieval_text_mode = \"input_segmentation\"\n",
    "\n",
    "prompt = questions[0]\n",
    "model_id = \"TheBloke/Llama-2-7b-chat-GPTQ\"\n",
    "chunk_length = 32\n",
    "\n",
    "time_before_setup = time.time()\n",
    "BioLlama = BioLlama(model_id=model_id, chunk_length=chunk_length)\n",
    "\n",
    "\n",
    "params, trainable_params = param_count(BioLlama)\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in BioLlama.model.parameters(): param.requires_grad = False\n",
    "for param in BioLlama.model.lm_head.parameters(): param.requires_grad = True\n",
    "for param in BioLlama.model.model.layers[0:14].parameters(): param.requires_grad = False\n",
    "for param in BioLlama.model.model.layers[16:].parameters(): param.requires_grad = False\n",
    "BioLlama.model.model.embed_tokens.weight.requires_grad_(False)\n",
    "BioLlama.model.gradient_checkpointing_enable()\n",
    "params, trainable_params = param_count(BioLlama)\n",
    "\n",
    "\n",
    "# time_before_generation = time.time()\n",
    "# print(\"***Generating***\")\n",
    "# num_tokens, text = BioLlama.generate(prompt=prompt, max_length=33)\n",
    "\n",
    "# time_after = time.time()\n",
    "\n",
    "\n",
    "# print(text)\n",
    "# # actual_response = text[len(prompt):]\n",
    "# # print(actual_response)\n",
    "# # print(f\"Actual response length: {len(actual_response)}\")\n",
    "# print(f\"Time taken for setup: {time_before_generation - time_before_setup}\")\n",
    "# print(f\"Time taken for generation: {time_after - time_before_generation}\")\n",
    "# print(f\"Tokens per second: {num_tokens/(time_after - time_before_generation)}\")\n",
    "# print(f\"Time total: {time_after - time_before_setup}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
